<?xml version="1.0"?>

<!-- This template is for creating an Internet Draft using xml2rfc,
     which is available here: http://xml.resource.org. -->

<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
  <!-- One method to get references from the online citation libraries.
       There has to be one entity for each item to be referenced.
       An alternate method (rfc include) is described in the references. -->

  <!ENTITY RFC1034 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1034.xml">
  <!ENTITY RFC1035 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1035.xml">
  <!ENTITY RFC1995 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1995.xml">
  <!ENTITY RFC1996 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1996.xml">
  <!ENTITY RFC2826 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.2826.xml">
  <!ENTITY RFC2845 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.2845.xml">
  <!ENTITY RFC5011 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5011.xml">
  <!ENTITY RFC5936 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5936.xml">
  <!ENTITY RFC6219 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.6219.xml">
  <!ENTITY RFC6891 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.6891.xml">
  <!ENTITY RFC7720 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7720.xml">
  <!ENTITY RFC7872 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7872.xml">
  <!ENTITY RFC5890 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5890.xml">
  <!ENTITY RFC8109 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.8109.xml">

  <!ENTITY I-D.muks-dns-message-fragments SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-muks-dns-message-fragments-00.xml">
  <!ENTITY I-D.wkumari-dnsop-trust-management SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-wkumari-dnsop-trust-management-01.xml">
  <!ENTITY I-D.wessels-edns-key-tag SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-wessels-edns-key-tag-00.xml">
  <!ENTITY I-D.andrews-tcp-and-ipv6-use-minmtu
    SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-andrews-tcp-and-ipv6-use-minmtu-04.xml">
  <!ENTITY I-D.bortzmeyer-dname-root SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-bortzmeyer-dname-root-00.xml">
    <!ENTITY I-D.taylor-v6ops-fragdrop SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-taylor-v6ops-fragdrop-02.xml">
    <!ENTITY I-D.song-atr-large-resp SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-song-atr-large-resp-00.xml">
    
]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<!-- used by XSLT processors -->
<!-- For a complete list and description of processing instructions (PIs),
     please see http://xml.resource.org/authoring/README.html. -->

<!-- Below are generally applicable Processing Instructions (PIs) that most
     I-Ds might want to use. (Here they are set differently than their
     defaults in xml2rfc v1.32) -->

<!-- give errors regarding ID-nits and DTD validation -->
<?rfc strict="yes" ?>

<!-- generate a ToC -->
<?rfc toc="yes"?>

<!-- control the table of contents (ToC) -->
<?rfc tocappendix="yes"?>

<!-- the number of levels of subsections in ToC. default: 3 -->
<?rfc tocdepth="3"?>

<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc symrefs="yes"?>

<!-- sort the reference entries alphabetically -->
<?rfc sortrefs="yes" ?>

<!-- control vertical white space
     (using these PIs as follows is recommended by the RFC Editor) -->
<!-- do not start each main section on a new page -->
<?rfc compact="yes" ?>

<!-- keep one blank line between list items -->
<?rfc subcompact="no" ?>

<!-- end of list of popular I-D processing instructions -->

<?rfc comments="no" ?>
<?rfc inline="yes" ?>

<rfc category="info" docName="draft-song-yeti-testbed-experience-06"
    ipr="trust200902">

  <front>
    <title>Yeti DNS Testbed</title>

    <author fullname="Linjian Song" initials="L." surname="Song" role="editor">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>songlinjian@gmail.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <author fullname="Dong Liu" initials="D." surname="Liu">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>dliu@biigroup.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>


    <author fullname="Paul Vixie" initials="P." surname="Vixie" >
      <organization>TISF</organization>
      <address>
        <postal>
        <street>11400 La Honda Road</street>
        <city>Woodside</city>
        <region>California</region>
        <code>94062</code>
        <country>US</country>
      </postal>
      <email>vixie@tisf.net</email>
      <uri>http://www.redbarn.org/</uri>
      </address>
    </author>

    <author fullname="Akira Kato" initials="" surname="Kato" >
      <organization>Keio University/WIDE Project</organization>
      <address>
        <postal>
          <street>Graduate School of Media Design, 4-1-1 Hiyoshi, Kohoku</street>
          <city>Yokohama</city>
          <region></region>
          <code>223-8526</code>
          <country>JAPAN</country>
        </postal>
        <email>kato@wide.ad.jp</email>
        <uri>http://www.kmd.keio.ac.jp/</uri>
      </address>
    </author>

    <author fullname="Shane Kerr" initials="S." surname="Kerr">
      <address>
        <postal>
          <street>Antoon Coolenlaan 41</street>
          <city>Uithoorn</city>
          <code>1422 GN</code>
          <country>NL</country>
        </postal>
        <email>shane@time-travellers.org</email>
      </address>
    </author>
    <date/>

    <!-- Meta-data Declarations -->

    <area>Internet Area</area>
    <workgroup>Internet Engineering Task Force (IETF)</workgroup>

    <!-- <keyword>dns</keyword> -->

    <abstract>
      <t>The Internet's Domain Name System (DNS) is built upon the
        foundation provided by the Root Server System -- that is,
        the critical infrastructure that serves the DNS root zone.</t>

      <t>Yeti DNS is an experimental, non-production testbed that
        provides an environment where technical and operational
        experiments can safely be performed without risk to production
        infrastructure.  This testbed has been used by a broad
        community of participants to perform experiments that aim
        to inform operations and future development of the production
        DNS. Yeti DNS is an independently-coordinated project and
        is not affiliated with ICANN, IANA or any Root Server
        Operator.</t>

      <t>The Yeti DNS testbed implementation includes various novel
        and experimental components including IPv6-only transport,
        independent, autonomous Zone Signing Key management, large
        cryptographic keys and a large number of Yeti-Root
        Servers. These differences from the Root Server System have
        operational consequences such as large responses to priming
        queries and the coordination of a large pool of independent
        operators; by deploying such a system globally but outside
        the production DNS system, the Yeti DNS project provides
        an opportunity to gain insight into those consequences
        without threatening the stability of the DNS.</t>

      <t>This document neither addresses the relevant policies under
        which the Root Server System is operated nor makes any
        proposal for changing any aspect of its implementation or
        operation. This document aims solely to document the technical
        and operational experience of deploying a system which is
        similar to but different from the Root Server System.</t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction">
      <t>The Domain Name System (DNS), as originally specified in
        <xref target="RFC1034"/> and <xref target="RFC1035"/>, has
        proved to be an enduring and important platform upon which
        almost every end-user of the Internet relies. Despite its
        longevity, extensions to the protocol, new implementations
        and refinements to DNS operations continue to emerge both
        inside and outside the IETF.</t>

      <t>The Root Server System in particular has seen technical
        innovation and development, for example in the form of
        wide-scale anycast deployment, the mitigation of unwanted
        traffic on a global scale, the widespread deployment of
        Response Rate Limiting <xref target="RRL"/>, the introduction
        of IPv6 transport, the deployment of DNSSEC, changes in
        DNSSEC key sizes and preparations to roll the root zone's
        Key Signing Key (KSK) and corresponding trust anchor.
        Together, even the projects listed in this brief summary
        imply tremendous operational change, all the more impressive
        when considered the necessary caution when managing Internet
        critical infrastructure, and the context of the adjacent
        administrative changes involved in root zone management and
        the (relatively speaking) massive increase in the the number
        of delegations in the root zone itself <eref
        target="https://newgtlds.icann.org/"/>.</t>

      <t>Aspects of the operational structure of the Root Server
        System have been described in such documents as <xref
        target="TNO2009"/>, <xref target="ISC-TN-2003-1"/>, <xref
        target="RSSAC001"/> and <xref target="RFC7720"/>. Such
        references, considered together, provide sufficient insight
        into the operations of the system as a whole that it is
        straightforward to imagine structural changes to the root
        server system's infrastructure and to wonder what the
        operational implications of such changes might be.</t>

      <t>The Yeti DNS Project was conceived in May 2015 to provide
        a non-production testbed upon which the technical
        community could propose and run experiments designed to
        answer these kinds of questions. Coordination for the project
        was provided by TISF, the WIDE Project and the
        Beijing Internet Institute. Many volunteers collaborated
        to build a distributed testbed that at the time of writing
        includes 25 Yeti root servers with 16 operators and
        handles experimental traffic from individual volunteers,
        universities, DNS vendors and distributed measurement
        networks.</t>

      <t>By design, the Yeti testbed system serves the root zone
        published by the IANA with only those structural modifications
        necessary to ensure that it is able to function usefully
        in the Yeti testbed system instead of the production Root
        Server system.  In particular, no delegation for any top-level
        zone is changed, added or removed from the IANA-published
        root zone to construct the root zone served by the Yeti
        testbed system and changes in the root zone are reflected
        in the testbed in near real-time. In this document, for
        clarity, we refer to the zone derived from the IANA-published
        root zone as the Yeti-Root zone.</t>

      <t>The Yeti DNS testbed serves a similar function to the Root
        Server System in the sense that they both serve similar
        zones: the Yeti-Root zone and the IANA-published root zone.
        However, the Yeti DNS testbed only serves clients that are
        explicitly configured to participate in the experiment,
        whereas the Root Server System serves the whole Internet.
        Since the dependent end-users and systems of the Yeti DNS
        testbed are known and their operations well-coordinated
        with those of the Yeti project, it has been possible to
        deploy structural changes in the Yeti DNS testbed with
        effective measurement and analysis, something that is
        difficult or simply impractical in the production Root
        Server System.</t>
    </section>

    <section title="Areas of Study">
     <t>Examples of topics that the Yeti DNS Testbed was built to
       address are included below, each illustrated with indicative
       questions.</t>

     <section title="Implementation of a Root Server System-like Testbed">
       <t>
         <list style="symbols">
           <t>How can a testbed be constructed and deployed
             on the Internet, allowing useful public participation
             without any risk of disruption of the Root Server System?</t>

           <t>How can representative traffic be introduced into
             such a testbed such that insights into the
             impact of specific differences between the testbed and
             the Root Server System can be observed?</t>
         </list>
       </t>
     </section>

      <section title="Yeti-Root Zone Distribution">
        <t>
          <list style="symbols">
            <t>What are the scaling properties of Yeti-Root zone
              distribution as the number of Yeti-Root servers,
              Yeti-Root server instances or intermediate distribution
              points increase?</t>
          </list>
        </t>
      </section>

      <section title="Yeti-Root Server Names and Addressing">
        <t>
          <list style="symbols">
            <t>What naming schemes other than those closely analogous
              to the use of ROOT-SERVERS.NET in the production root
              zone are practical, and what are their respective
              advantages and disadvantages?</t>

            <t>What are the risks and benefits of signing the zone that
              contains the names of the Yeti-Root servers?</t>

            <t>What automatic mechanisms might be useful to improve
              the rate at which clients of Yeti-Root servers are
              able to react to a Yeti-Root server renumbering
              event?</t>
          </list>
        </t>
      </section>

      <section title="IPv6-Only Yeti-Root Servers">
        <t>
          <list style="symbols">
            <t>Are there negative operational effects in the use of
              IPv6-only Yeti-Root servers, compared to the use of
              servers that are dual-stack?</t>

            <t>What effect does the IPv6 fragmentation model have on the
              operation of Yeti-Root servers, compared with that of IPv4?</t>
          </list>
        </t>
      </section>

      <section title="DNSSEC in the Yeti-Root Zone">
        <t>
          <list style="symbols">
            <t>Is it practical to sign the Yeti-Root zone using multiple,
              independently-operated DNSSEC signers and multiple
              corresponding ZSKs?</t>

            <t>To what extent is <xref target="RFC5011"/> supported by
              resolvers?</t>

            <t>Does the KSK Rollover plan designed and in the process
              of being implemented by ICANN work as expected on the
              Yeti testbed?</t>

            <t>What is the operational impact of using much larger RSA
              key sizes in the ZSKs used in the Yeti-Root?</t>

            <t>What are the operational consequences of choosing
              DNSSEC algorithms other than RSA to sign the Yeti-Root
              zone?</t>
          </list>
        </t>
      </section>
    </section>

    <section title="Yeti DNS Testbed Infrastructure" anchor="testbed">
      <t>The purpose of the testbed is to allow DNS queries from
        stub resolvers, mediated by recursive resolvers, to be
        delivered to Yeti-Root servers, and for corresponding
        responses generated on the Yeti-Root servers to be returned,
        as illustrated in <xref target="components"/>.</t>

      <figure anchor="components" title="High-Level Testbed Components">
        <artwork>
          <![CDATA[
    ,----------.        ,-----------.        ,------------.
    |   stub   +------> | recursive +------> | Yeti-Root  |
    | resolver | <------+ resolver  | <------+ nameserver |
    `----------'        `-----------'        `------------'
       ^                   ^                    ^
       |  appropriate      |  Yeti-Root hints;  |  Yeti-Root zone
       `- resolver         `- Yeti-Root trust   `- with DNSKEY RRSet
          configured          anchor               signed by Yeti-KSK
]]>
        </artwork>
      </figure>

      <t>To use the Yeti DNS testbed, a recursive resolver must be
        configured to use the Yeti-Root servers. That configuration
        consists of a list of names and addresses for the Yeti-Root
        servers (often referred to as a "hints file") that replaces
        the corresponding hints used for the production Root Server
        System <xref target="yeti-hints"/>.  Resolvers also need
        to be configured with a DNSSEC trust anchor that corresponds
        to a KSK used in the Yeti DNS Project, in place of the
        normal trust anchor set used for the root zone.</t>

      <t>The need for a Yeti-specific trust anchor in the resolver
        stems from the need to make minimal changes to the root
        zone, as retrieved from the IANA, to transform it into the
        Yeti-Root zone that can be used in the testbed. Corresponding
        changes are required in the Yeti-Root hints file <xref
        target="yeti-hints"/>. Those changes would be properly
        rejected by any validator using the production Root Server
        System's root zone trust anchor set as bogus.</t>

      <t>Stub resolvers become part of the Yeti DNS Testbed by
        their use of recursive resolvers that are configured as
        described above.</t>

      <t>The data flow from IANA to stub resolvers through the
        Yeti testbed is illustrated in <xref target="yeti-root"/>
        and are described in more detail in the sections that
        follow.</t>

      <figure anchor="yeti-root" title="Testbed Data Flow">
        <artwork>
          <![CDATA[
                               ,----------------.
                          ,-- / IANA Root Zone / ---.
                          |  `----------------'     |
                          |            |            |
                          |            |            |       Root Zone
  ,--------------.    ,---V---.    ,---V---.    ,---V---.
  | Yeti Traffic |    | BII   |    | WIDE  |    | TISF  |
  | Collection   |    |  DM   |    |  DM   |    |  DM   |
  `----+----+----'    `---+---'    `---+---'    `---+---'
       |    |       ,-----'    ,-------'            `----.
       |    |       |          |                         |  Yeti-Root
       ^    ^       |          |                         |     Zone
       |    |   ,---V---.  ,---V---.                 ,---V---.
       |    `---+ Yeti  |  | Yeti  |  . . . . . . .  | Yeti  |
       |        | Root  |  | Root  |                 | Root  |
       |        `---+---'  `---+---'                 `---+---'
       |            |          |                         |    DNS
       |            |          |                         |  Response
       |         ,--V----------V-------------------------V--.
       `---------+              Yeti Resolvers              |
                 `--------------------+---------------------'
                                      |                       DNS
                                      |                     Response
                 ,--------------------V---------------------.
                 |            Yeti Stub Resolvers           |
                 `------------------------------------------'
]]>
        </artwork>
      </figure>

      <section title="Root Zone Retrieval" anchor="retrieval">
        <t>The Yeti-Root Zone is distributed within the
          Yeti DNS testbed through a set of internal master
          servers that are referred to as Distribution
          Masters (DMs). These server elements distribute the
          Yeti-Root zone to all Yeti-Root servers. The means
          by which the Yeti DMs construct the Yeti-Root
          zone for distribution is described below.</t>
          
        <t>Since Yeti DNS DMs do not receive DNS NOTIFY <xref
          target="RFC1996"/> messages from the Root Server System,
          a polling approach is used to determine when new revisions
          of the root zone are available from the production Root
          Server System. Each Yeti DM requests the root zone SOA
          record from a root server that permits unauthenticated
          zone transfers of the root zone, and performs a zone
          transfer from that server if the retrieved value of
          SOA.SERIAL is greater than that of the last retrieved
          zone.</t>

         <t>At the time of writing, unauthenticated zone transfers
          of the root zone are available directly from B-Root,
          C-Root, F-Root, G-Root and K-Root, and from L-Root via
          the two servers XFR.CJR.DNS.ICANN.ORG and XFR.LAX.DNS.ICANN.ORG,
          as well as via FTP from sites maintained by the Root Zone
          Maintainer and the IANA Functions Operator. The Yeti DNS
          Testbed retrieves the root zone from using zone transfers
          from F-Root. The schedule on which F-Root is polled by
          each Yeti DM is as follows:</t>

        <texttable>
          <ttcol>DM Operator</ttcol><ttcol>Time</ttcol>
          <c>BII</c><c>UTC hour + 00 minutes</c>
          <c>WIDE</c><c>UTC hour + 20 minutes</c>
          <c>TISF</c><c>UTC hour + 40 minutes</c>
        </texttable>

        <t>The Yeti DNS testbed uses multiple DMs, each of which acts
          autonomously and equivalently to its siblings. Any single
          DM can act to distribute new revisions of the Yeti-Root
          zone, and is also responsible for signing the RRSets that
          are changed as part of the transformation of the Root
          Zone into the Yeti-Root zone described in <xref
          target="transformation"/>. This shared control over the
          processing and distribution of the Yeti-Root zone
          approximates some of the ideas around shared zone control
          explored in <xref target="ITI2014"/>.</t>
      </section>

      <section title="Transformation of Root Zone to Yeti-Root Zone"
        anchor="transformation">
        <t>Two distinct approaches have been deployed in the Yeti-DNS
          Testbed, separately, to transform the Root Zone into the
          Yeti-Root Zone. At a high level both approaches are
          equivalent in the sense that they replace a minimal set
          of information in the root zone with corresponding data
          for the Yeti DNS Testbed; the mechanisms by which the
          transforms are executed are different, however.  Each is
          discussed in turn in <xref target="trans1"/> and <xref
          target="trans2"/>, respectively.</t>

        <t>A third approach has also been proposed, but not yet
          implemented. The motivations and changes implied by that
          approach are described in <xref target="trans3"/>.</t>

        <section title="ZSK and KSK Key Sets Shared Between DMs"
          anchor="trans1">
          <t>The approach described here was the first to be
            implemented. It features entirely autonomous operation
            of each DM, but also requires secret key material (the
            private key in each of the Yeti-Root KSK and ZSK key-pairs)
            to be distributed and maintained on each DM in a
            coordinated way.</t>

          <t>The Root Zone is transformed as follows to produce the
            Yeti-Root Zone. This transformation is carried out
            autonomously on each Yeti DNS Project DM. Each DM carries
            an authentic copy of the current set of Yeti KSK and
            ZSK key pairs, synchronized between all DMs (see <xref
            target="synch"/>).</t>

          <t>
            <list style="numbers">
              <t>SOA.MNAME is set to www.yeti-dns.org.</t>

              <t>SOA.RNAME is set to &lt;dm-operator&gt;.yeti-dns.org.
                where &lt;dm-operator&gt; is currently one of "wide",
                "bii" or "tisf".</t>
  
              <t>All DNSKEY, RRSIG and NSEC records are removed.</t>
  
              <t>The apex NS RRSet is removed, with the corresponding
                root server glue (A and AAAA) RRSets.</t>

              <t>A Yeti DNSKEY RRSet is added to the apex, comprising
                the public parts of all Yeti KSK and ZSKs.</t>

              <t>A Yeti NS RRSet is added to the apex that includes
                all Yeti-Root servers.</t>

              <t>Glue records (AAAA only, since Yeti-Root servers
                are v6-only) for all Yeti-Root servers are added.</t>

              <t>The Yeti-Root Zone is signed: the NSEC chain is
                regenerated; the Yeti KSK is used to sign the DNSKEY
                RRSet, and the DM's local ZSK is used to sign every
                other RRSet.</t>
            </list>
          </t>

          <t>Note that the SOA.SERIAL value published in the Yeti-Root
            Zone is identical to that found in the root zone.</t>
        </section>

        <section title="Unique ZSK per DM; No Shared KSK"
          anchor="trans2">
          <t>The approach described here was the second to be
            implemented. Each DM is provisioned with its own,
            dedicated ZSK key pairs that are not shared with other
            DMs. A Yeti-Root DNSKEY RRSet is constructed and signed
            upstream of all DMs as the union of the set of active
            KSKs and the set of active ZSKs for every individual
            DM. Each DM now only requires the secret part of its
            own dedicated ZSK key pairs to be available locally,
            and no other secret key material is shared. The high-level
            approach is illustrated in <xref target="multi-ZSK"/>.</t>

          <figure anchor="multi-ZSK" title="Unique ZSK per DM">
            <artwork>
              <![CDATA[
                         ,----------.         ,-----------.
                .--------> BII ZSK  +---------> Yeti-Root |
                | signs  `----------'  signs  `-----------'
                |
  ,-----------. |        ,----------.         ,-----------.
  | Yeti KSK  +-+--------> TISF ZSK +---------> Yeti-Root |
  `-----------' | signs  `----------'  signs  `-----------'
                |
                |        ,----------.         ,-----------.
                `--------> WIDE ZSK +---------> Yeti-Root |
                  signs  `----------'  signs  `-----------'
]]>
            </artwork>
          </figure>

          <t>The process of retrieving the Root Zone from the Root
            Server System and replacing and signing the apex DNSKEY
            RRSet no longer takes place on the DMs, and instead
            takes place on a central Hidden Master. The production
            of signed DNSKEY RRSets is analogous to the use of
            Signed Key Responses (SKR) produced during ICANN KSK
            key ceremonies <xref target="ICANN2010"/>.</t>

          <t>Each DM now retrieves source data (with pre-modified
            and Yeti-signed DNSKEY RRset, but otherwise unchanged)
            from the Yeti DNS Hidden Master instead of from the
            Root Server System.</t>

          <t>Each DM carries out a similar transformation to that
            described in <xref target="trans1"/>, except that DMs
            no longer need to modify or sign the DNSKEY RRSet.</t>

          <t>The Yeti-Root Zone served by any particular Yeti-Root
            Server will include signatures generated using the ZSK
            from the DM that served the Yeti-Root Zone to that
            Yeti-Root Server. Signatures cached at resolvers might
            be retrieved from any Yeti-Root Server, and hence are
            expected to be a mixture of signatures generated by
            different ZSKs. Since all ZSKs can be trusted through
            the signature by the Yeti KSK over the DNSKEY RRSet,
            which includes all ZSKs, the mixture of signatures was
            predicted not to be a threat to reliable validation.
            Deployment and experimentation confirms this to be the
            case, even when individual ZSKs are rolled on different
            schedules.</t>

          <t>A consequence of this approach is that the apex DNSKEY
            RRSet in the Yeti-Root zone is much larger than the
            corresponding DNSKEY RRSet in the Root Zone.</t>
        </section>

        <section title="Preserving Root Zone NSEC Chain and ZSK RRSIGs"
          anchor="trans3">
          <t>A change to the transformation described in <xref
            target="trans2"/> has been proposed that would preserve
            the NSEC chain from the Root Zone and all RRSIG RRs
            generated using the Root Zone's ZSKs. The DNSKEY RRSet
            would continue to be modified to replace the Root Zone
            KSKs, and the Yeti KSK would be used to generate
            replacement signatures over the apex DNSKEY and NS
            RRSets. Source data would continue to flow from the
            Root Server System through the Hidden Master to the set
            of DMs, but no DNSSEC operations would be required on
            the DMs and the source NSEC and most RRSIGs would remain
            intact.</t>

          <t>This approach has been suggested in order to provide
            cryptographically-verifiable confidence that no owner
            name in the root zone had been changed in the process of
            producing the Yeti-Root zone from the Root Zone, addressing
            one of the concerns described in <xref target="controversy"/>
            in a way that can be verified automatically.</t>
        </section>
      </section>

      <section title="Yeti-Root Zone Distribution" anchor="distr">
        <t>Each Yeti DM is configured with a full list of Yeti-Root
          Server addresses to send NOTIFY <xref target="RFC1996"/>
          messages to, which also forms the basis for an address-based
          access-control list for zone transfers. Authentication
          by address could be replaced with more rigourous mechanisms
          (e.g. using Transaction Signatures (TSIG) <xref
          target="RFC2845"/>); this has not been done at the time
          of writing since the use of address-based controls avoids
          the need for the distribution of shared secrets amongst
          the Yeti-Root Server Operators.</t>

        <t>Individual Yeti-Root Servers are configured with a full
          set of Yeti DM addresses to which SOA and AXFR queries
          may be sent in the conventional manner.</t>
      </section>

      <section title="Synchronisation of Service Metadata" anchor="synch">
        <t>Changes in the Yeti-DNS Testbed infrastructure such as
          the addition or removal of Yeti-Root servers, renumbering
          Yeti-Root Servers or DNSSEC key rollovers require coordinated
          changes to take place on all DMs. The Yeti-DNS Testbed
          is subject to more frequent changes than are observed in
          the Root Server System and includes substantially more
          Yeti-Root Servers than there are IANA Root Servers, and hence
          a manual change process in the Yeti Testbed would be more
          likely to suffer from human error. An automated process
          was consequently implemented.</t>

        <t>A repository of all service metadata involved in the
          operation of each DM was implemented as a dedicated git
          repository hosted at github.com, a mechanism chosen since
          it was simple, transparent and familiar to participants.
          Requests to change the service metadata for a DM were
          submitted as pull requests from a fork of the corresponding
          repository; each DM operator reviewed pull requests and
          merged them to indicate approval. Once merged, changes
          were pulled automatically to individual DMs and promoted
          to production.</t>
      </section>

      <section title="Yeti-Root Server Naming Scheme" anchor="naming">
        <t>The current naming scheme for Root Servers was normalized
          to use single-character host names (A through M) under
          the domain ROOT-SERVERS.NET, as described in <xref
          target="RSSAC023"/>). The principal benefit of this naming
          scheme was that DNS label compression could be used to
          produce a priming response that would fit within 512 bytes
          at the time it was introduced, 512 bytes being the maximum
          DNS message size using UDP transport without EDNS(0) <xref
          target="RFC6891"/>.</t>

        <t>Yeti-Root Servers do not use this optimization, but rather
          use free-form nameserver names chosen by their respective
          operators -- in other words, no attempt is made to minimize
          the size of the priming response through the use of label
          compression. This approach aims to challenge the need for
          a minimally-sized priming response in a modern DNS ecosystem
          where EDNS(0) is prevalent.</t>

        <t>Priming responses from Yeti-Root Servers do not always
          include server addresses in the additional section, as
          is the case with priming responses from Root Servers. In
          particular, Yeti-Root Servers running BIND9 return an
          empty additional section if the configuration parameter
          minimum-responses is set, forcing resolvers to complete
          the priming process with a set of conventional recursive
          lookups in order to resolve addresses for each Yeti-Root
          server. The Yeti-Root Servers running NSD were observed
          to return a fully-populated additional section.</t>

        <t>Various approaches to normalize the composition of the
          priming response were considered, including:

          <list style="symbols">
            <t>Require use of DNS implementations that exhibit a
              desired behaviour in the priming response;</t>

            <t>Modify nameserver software or configuration as used
              by Yeti-Root Servers;</t>

            <t>Isolate the names of Yeti-Root Servers in one or
              more zones that could be slaved on each Yeti-Root
              Server, renaming servers as necessary, giving each a
              source of authoritative data with which the authority
              section of a priming response could be fully populated.
              This is the approach used in the Root Server System
              with the ROOT-SERVERS.NET zone.</t>
          </list>
        </t>

        <t>The potential mitigation of renaming all Yeti-Root Servers
          using a scheme that would allow their names to exist directly
          in the root zone was not considered, since that
          approach implies the invention of new top-level labels not
          present in the Root Zone.</t>

        <t>Given the relative infrequency of priming queries by
          individual resolvers and the additional complexity or
          other compromises implied by each of those mitigations,
          the decision was made to make no effort to ensure that
          the composition of priming responses was identical across
          servers. Even the empty additional sections generated by
          Yeti-Root Servers running BIND9 seem to be sufficient for
          all resolver software tested; resolvers simply perform a
          new recursive lookup for each authoritative server name
          they need to resolve.</t>
      </section>

      <section title="Yeti-Root Servers">
        <t>Various volunteers have donated authoritative servers
          to act as Yeti-Root servers. At the time of writing there
          are 25 Yeti-Root servers distributed globally, one of
          which is named using an IDNA2008 <xref target="RFC5890"/>
          label, shown in the following list in punycode.</t>

        <texttable>
          <ttcol>Name</ttcol><ttcol>Operator</ttcol><ttcol>Location</ttcol>

          <c>bii.dns-lab.net</c><c>BII</c><c>CHINA</c>

          <c>yeti-ns.tsif.net</c><c>TSIF</c><c>USA</c>

          <c>yeti-ns.wide.ad.jp</c><c>WIDE Project</c><c>Japan</c>

          <c>yeti-ns.as59715.net</c><c>as59715</c><c>Italy</c>

          <c>dahu1.yeti.eu.org</c><c>Dahu Group</c><c>France</c>

          <c>ns-yeti.bondis.org</c><c>Bond Internet Systems</c><c>Spain</c>

          <c>yeti-ns.ix.ru</c><c>Russia</c><c>MSK-IX</c>

          <c>yeti.bofh.priv.at</c><c>CERT Austria</c><c>Austria</c>

          <c>yeti.ipv6.ernet.in</c><c>ERNET India</c><c>India</c>

          <c>yeti-dns01.dnsworkshop.org</c><c>dnsworkshop
            /informnis</c><c>Germany</c>

          <c>dahu2.yeti.eu.org</c><c>Dahu Group</c><c>France</c>

          <c>yeti.aquaray.com</c><c>Aqua Ray SAS</c><c>France</c>

          <c>yeti-ns.switch.ch</c><c>SWITCH</c><c>Switzerland</c>

          <c>yeti-ns.lab.nic.cl</c><c>CHILE NIC</c><c>Chile</c>

          <c>yeti-ns1.dns-lab.net</c><c>BII</c><c>China</c>

          <c>yeti-ns2.dns-lab.net</c><c>BII</c><c>China</c>

          <c>yeti-ns3.dns-lab.net</c><c>BII</c><c>China</c>

          <c>ca...a23dc.yeti-dns.net</c><c>Yeti-ZA</c><c>South Africa</c>

          <c>3f...374cd.yeti-dns.net</c><c>Yeti-AU</c><c>Australia</c>

          <c>yeti1.ipv6.ernet.in</c><c>ERNET India</c><c>India</c>

          <c>xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c</c><c>ERNET
            India</c><c>India</c>

          <c>yeti-dns02.dnsworkshop.org</c><c>dnsworkshop
            /informnis</c><c>USA</c>

          <c>yeti.mind-dns.nl</c><c>Monshouwer Internet
            Diensten</c><c>Netherlands</c>

          <c>yeti-ns.datev.net</c><c>DATEV</c><c>Germany</c>

          <c>yeti.jhcloos.net.</c><c>jhcloos</c><c>USA</c>
        </texttable>

        <t>The current list of Yeti-Root server is made available
          to a participating resolver first using a substitute hints
          file <xref target="yeti-hints"/> and subsequently by the
          usual resolver priming process <xref target="RFC8109"/>.
          All Yeti-Root servers are IPv6-only, foreshadowing a
          future IPv6-only Internet, and hence the Yeti-Root hints
          file contains no IPv4 addresses and the Yeti-Root zone
          contains no IPv4 glue.</t>

        <t>At the time of writing, all root servers within the
          Root Server System serve the ROOT-SERVERS.NET zone in
          addition to the root zone, and all but one also serve the
          ARPA zone. Yeti-Root servers serve the Yeti-Root zone
          only.</t>

        <t>Significant software diversity exists across the set of
          Yeti-Root servers, as reported by their volunteer operators 
          at the time of writing:

          <list style="symbols">
            <t>Platform: 18 of 25 Yeti-Root servers are implemented
              on a VPS rather than bare metal.</t>

            <t>Operating System: 15 Yeti-Root servers run
              on Linux (Ubuntu, Debian, CentOS, Red Hat and ArchLinux); 4
              run on FreeBSD, 1 on NetBSD and 1 in Windows server 2016.</t>

            <t>DNS software: 18 of 25 Yeti-Root servers use BIND9
              (versions varying between 9.9.7 and 9.10.3); 4 use
              NSD (4.10 and 4.15); 2 use Knot (2.0.1 and 2.1.0), 1
              uses Bundy (1.2.0) and 1 uses MS DNS (10.0.14300.1000).</t>
          </list>
        </t>
      </section>

      <section title="Experimental Traffic">
        <t>For the Yeti DNS Testbed to be useful as a platform for
          experimentation, it needs to carry statistically
          representative traffic. Several approaches have been taken
          to load the system with traffic, including both real-world
          traffic triggered by end-users and synthetic traffic.</t>

        <t>Resolvers that have been explicitly configured to
          participate in the testbed, as described in <xref
          target="testbed"/>, are a source of real-world, end-user
          traffic. Sustained levels of traffic have been observed
          from a variety of sources, as summarised in
          <xref target="activev6"/>.</t>

        <t>Synthetic traffic has been introduced to the system from
          time to time in order to increase traffic loads. Approaches
          include the use of distributed measurement platforms such
          as RIPE ATLAS to send DNS queries to Yeti-Root servers,
          and the capture of traffic sent from non-Yeti resolvers
          to the Root Server System which was subsequently modified
          and replayed towards Yeti-Root servers.</t>
      </section>

      <section title="Traffic Capture and Analysis">
        <t>Query and response traffic capture is available in the
          testbed in both Yeti resolvers and Yeti-Root servers in
          anticipation of experiments that require packet-level
          visibility into DNS traffic.</t>

        <t>Traffic capture is performed on Yeti-Root servers using
          either dnscap <eref
          target="https://www.dns-oarc.net/tools/dnscap"/> or
          pcapdump (part of the pcaputils Debian package <eref
          target="https://packages.debian.org/sid/pcaputils"/>,
          with a patch to facilitate triggered file upload <eref
          target="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985"/>.
          PCAP-format files containing packet captures are uploaded
          using rsync to central storage.</t>
      </section>
    </section>

    <section title="Operational Experience with Yeti DNS Testbed">
      <t>The following sections provide commentary on the operation
        and impact analyses of the Yeti-DNS Testbed described in
        <xref target="testbed"/>.  More detailed descriptions of
        observed phenomena are available in Yeti DNS mailing list
        archives <eref
        target="http://lists.yeti-dns.org/pipermail/discuss/"/> and
        on the Yeti DNS blog <eref
        target="https://yeti-dns.org/blog.html"/>.</t>

      <section title="Viability of IPv6-Only Operation">
        <t>All Yeti-Root servers were deployed with IPv6 connectivity,
          and no IPv4 addresses for any Yeti-Root server were made
          available (e.g. in the Yeti hints file). This implementation
          decision constrained the Yeti-Root system to be v6-only.</t>

        <t>DNS implementations are generally adept at using both
          IPv4 and IPv6 when both are available. Servers that cannot
          be reliably reached over one protocol might be better
          queried over the other, to the benefit of end-users in
          the common case where DNS resolution is on the critical
          path for end-users' perception of performance. However,
          this optimisation also means that systemic problems with
          one protocol can be masked by the other. By forcing all
          traffic to be carried over IPv6, the Yeti DNS testbed
          aimed to expose any such problems and make them easier
          to identify and understand. Several examples of IPv6-specific
          phenomena observed during the operation of the testbed
          are described in the sections that follow.</t>

        <t>Although the Yeti-Root servers themselves were only
          reachable using IPv6, real-world end-users often have no
          IPv6 connectivity. The testbed was also able to explore
          the degree to which IPv6-only Yeti-Root servers were able
          to serve single-stack, IPv4-only end-user populations
          through the use of dual-stack Yeti resolvers.</t>

        <section title="IPv6 Fragmentation">
          <t>In the Root Server System, structural changes with the
            potential to increase response sizes (and hence
            fragmentation, fallback to TCP transport or both) have
            been exercised with great care, since the impact on
            clients has been difficult to predict or measure. The
            Yeti DNS Testbed is experimental and has the luxury of
            a known client base, making it far easier to make such
            changes and measure their impact.</t>

          <t>Many of the experimental design choices described in
            this document were expected to trigger larger responses.
            For example, the choice of naming scheme for Yeti-Root
            Servers described in <xref target="naming"/> defeats
            label compression. It makes a large priming response
            (up to 1754 octets with 25 NS server and their glue) ;
            the Yeti-Root zone transformation approach described
            in <xref target="trans2"/> greatly enlarges the apex
            DNSKEY RRSet especially during the KSK rollover (up to
            1975 octets with 3 ZSK and 2 KSK).  An increased incidence
            of fragmentation was therefore expected.</t>

          <t>The Yeti-DNS Testbed provides service on IPv6 only.
            IPv6 has a fragmentation model that is different from
            IPv4 -- in particular, fragmentation always takes place
            on the sending host, and not on an intermediate router.</t>

          <t>Fragmentation may cause serious issues; if a single
            fragment is lost, it results in the loss of the entire
            datagram of which the fragment was a part, and in the
            DNS frequently triggers a timeout. It is known at this
            moment that only a limited number of security middle-box
            implementations support IPv6 fragments. Some public
            measurements and reports <xref
            target="I-D.taylor-v6ops-fragdrop"/> <xref target="RFC7872"/>
            shows that there is notable packets drop rate due to
            the mistreatment of middle-box on IPv6 fragment.  One
            APNIC study <xref target="IPv6-frag-DNS"/> reported
            that 37% of endpoints using IPv6-capable DNS resolver
            cannot receive a fragmented IPv6 response over UDP.</t>

          <t>To study the impact, RIPE Atlas probes were used.
            For each Yeti-Root server, an Atlas measurement was
            setup using 100 IPv6-enabled probes from five regions,
            sending a DNS query for ./IN/DNSKEY using UDP transport
            with DO=1. This measurement, when carried out concurrently
            with a Yeti KSK rollover, further exacerbating the
            potential for fragmentation, identified a 7% failure
            rate compared with a non-fragmented control. A failure
            rate of 2% was observed with response sizes of 1414
            octets, which was surprising given the expected prevalence
            of 1500-octet (Ethernet-framed) MTUs.</t>

          <t>The consequences of fragmentation were not limited to
            failures in delivering DNS responses over UDP transport.
            There were two cases where a Yeti-Root server
            failed to transfer the Yeti-Root zone from a DM. DM log
            files revealed "socket is not connected" errors corresponding
            to zone transfer requests. Further experimentation
            revealed that combinations of NetBSD 6.1, NetBSD 7.0RC1,
            FreeBSD 10.0, Debian 3.2 and VMWare ESXI 5.5 resulted
            in a high TCP MSS value of 1440 octets being negotiated
            between client and server despite the presence of the
            IPV6_USE_MIN_MTU socket option, as described in <xref
            target="I-D.andrews-tcp-and-ipv6-use-minmtu"/>. The
            mismatch appears to cause outbound segments greater in
            size than 1280 octets to be dropped before sending.
            Setting the local TCP MSS to 1220 octets (chosen as
            1280-60, the size of the IPv6/TCP header with no other
            extension headers) was observed to be a pragmatic
            mitigation.</t>
        </section>

        <section title="Serving IPv4-Only End-Users">
          <t>Yeti resolvers have been successfully used by real-world
            end-users for general name resolution within a number of
            participant organisations, including resolution of names
            to IPv4 addresses and resolution by IPv4-only end-user
            devices.</t>

          <t>Some participants, recognising the operational importance
            of reliability in resolver infrastructure and concerned
            about the stability of their IPv6 connectivity, chose
            to deploy Yeti resolvers in parallel to conventional
            resolvers, making both available to end-users. While
            the viability of this approach provides a useful data
            point, end-users using Yeti resolvers exclusively
            provided a better opportunity to identify and understand
            any failures in the Yeti DNS testbed infrastructure.</t>

          <t>Resolvers deployed in IPv4-only environments were
            able to join the Yeti DNS testbed by way of upstream,
            dual-stack Yeti resolvers, or in one case, in CERNET2,
            by assigning IPv4 addresses to Yeti-Root servers and
            mapping them in dual-stack IVI translation devices <xref
            target="RFC6219"/>.</t>
        </section>
      </section>

      <section title="Zone Distribution">
        <t>The Yeti DNS testbed makes use of multiple DMs to
          distribute the Yeti-Root zone, an approach that would
          allow the number of Yeti-Root servers to scale to a higher
          number than could be supported by a single distribution
          source and which provided redundancy. The use of multiple
          DMs introduced some operational challenges, however, which
          are described in the following sections.</t>

        <section title="Zone Transfers">
          <t>Yeti-Root Servers were configured to serve the Yeti-Root
            zone as slaves. Each slave had all DMs configured as
            masters, providing redundancy in zone synchronisation.</t>

          <t>Each DM in the Yeti testbed served a Yeti-Root zone
            which is functionally equivalent but not congruent to
            that served by every other DM (see <xref target="distr"/>).
            The differences included variations in the SOA.MNAME
            field and, more critically, in the RRSIGs for everything
            other than the apex DNSKEY RRSet, since signatures for
            all other RRSets are generated using a private key that
            is only available to the DM serving its particular
            variant of the zone (see <xref target="transformation"/>,
            <xref target="trans2"/>).</t>

          <t>Incremental Zone Transfer (IXFR), as described in
            <xref target="RFC1995"/>, is a viable mechanism to use
            for zone synchronization between any Yeti-Root server and
            a consistent, single DM. However, if that Yeti-Root server
            ever selected a different DM, IXFR would no longer be a
            safe mechanism; structural changes between the incongruent
            zones on different DMs would not be included in any
            transferred delta and the result would be a zone that was
            not internally self-consistent.  For this reason the first
            transfer after a change of DM would require AXFR, not
            IXFR.</t>
  
          <t>None of the DNS software in use on Yeti-Root Servers
            supports this mixture of IXFR/AXFR according to the master
            server in use. This is unsurprising, given that the
            environment described above in the Yeti-Root system is
            idiosyncratic; conventional zone transfer graphs involve
            zones that are congruent between all nodes.  For this
            reason, all Yeti-Root servers are configured to use AXFR
            at all times, and never IXFR, to ensure that zones being
            served are internally self-consistent.</t>
        </section>

        <section title="Delays in Yeti-Root Zone Distribution">
          <t>Each Yeti DM polled the Root Server System for a new revision
            of the root zone on an interleaved schedule, as described in
            <xref target="retrieval"/>. Consequently, different DMs
            were expected to retrieve each revision of the root
            zone, and make a corresponding revision of the Yeti-Root
            zone available, at different times. The availability
            of a new revision of the Yeti-Root zone on the first
            DM would typically precede that of the last by 40
            minutes.</t>

          <t>It might be expected given this distribution mechanism
            that the maximum latency between the publication of a
            new revision of the root zone and the availability of the
            corresponding Yeti-Root zone on any Yeti-Root server would
            be 20 minutes, since in normal operation at least one DM
            should serve that Yeti-Zone within 20 minutes of root zone
            publication. In practice, this was not observed.</t>

          <t>In one case a Yeti-Root server running Bundy 1.2.0 on
            FreeBSD 10.2-RELEASE was found to lag root zone publication
            by as much as ten hours, which upon investigation was
            due to software defects that were subsequently
            corrected.</t>

          <t>More generally, Yeti-Root servers were observed routinely
            to lag root zone publication by more than 20 minutes, and
            relatively often by more than 40 minutes. Whilst in
            some cases this might be assumed to be a result of
            connectivity problems, perhaps suppressing the delivery
            of NOTIFY messages, it was also observed that Yeti-Root
            servers receiving a NOTIFY from one DM would often send
            SOA queries and AXFR requests to a different DM. If
            that DM was not yet serving the new revision of the
            Yeti-Root zone, a delay in updating the Yeti-Root server
            would naturally result.</t>
        </section>
      </section>

      <section title="XXX Question for Davey XXX">
        <t>I need some help understanding what this next section
          is trying to say. As written right now, it looks like
          this section says:

          <list style="numbers">
            <t>A mistake was made with an RFC 5011 rollover, which
              caused relying parties to break. The mistake was
              identified and fixed.</t>

            <t>A second KSK rollover was performed. Some relying
              parties were badly configured and broke. They were
              fixed.</t>

            <t>A third KSK rollover was performed. We don't
              describe what happened, but we imagine that large
              responses were triggered during the process, which
              might be a problem, maybe.</t>
          </list>

          If that's all we have to say about DNSSEC in the testbed,
          perhaps it is better to remove this whole section? There
          is not much substance here.</t>
      </section>

      <section title="DNSSEC experiences in Yeti DNS">
        <t>The Root Zone KSK is expected to undergo a
          carefully-orchestrated rollover as described in <xref
          target="ICANN2016"/>. ICANN has commissioned various tests
          and has published an external test plan <xref
          target="ICANN2017"/>.  After some studies and data analysis,
          ICANN announced a postponement of the KSK rollover on 27
          September 2017.</t>

        <t>(DaveyNote: More background should be inserted here.
          Should we introduce the basic concept of KSK rollover in
          the beginning of this section. Or should we put some
          concept introduction in the study areas?)</t>

        <t>During the period, tests were performed on KSK rollover
          and the planned approach was also modeled in the live
          root Testbed.  There are mainly three rollover tests on
          Yeti testbed.</t>

        <t>The first Yeti KSK rollover (started at 2015-06-30) was
          designed to simulate the worst case and to see how to
          recover. In RFC5011 there is a 30-day hold-down timer
          before inactivate the old key but it was deliberately
          ignored. The rollover between the two keys was made in
          one week after new key created. And the old key was deleted
          in another week.  It is found that some clients (Unbound)
          receive SERVFAILs because the new key was still in AddPend
          state when old key was inactive.  To recover from failure,
          the trusted key should be reset on the affected resolvers
          manually by an alert administrator or by an out-of-band
          automatic recovery process.</t>

        <t>It is proved the worth of doing with a recovery mechanism
          out of band.  The experience during ICANN KSK rollover
          and the final desiccation of postponing is due to a fact.
          The fact is that even RFC5011 is implemented on resolvers,
          there are still many reasons causing the failure of KSK
          rollover.</t>

        <t>The second Yeti KSK rollover (started at 2016-07-12) was
          design for a simulation on the ICANN's KSK rollover. It
          waited 30 days after a new KSK is published in the root
          zone. Different from ICANN rollover plan, it revokes the
          old key once the new key become active.  We don't want
          to wait too long, so we shorten the time for key publish
          and delete in server side.</t>

        <t>During the second KSK rolling test one bug was <xref
          target="KROLL-ISSUE"/> spotted on one Yeti resolver (using
          BIND 9.10.4-p2). An resolver is used to configured with
          multiple views for different subnets before the KSK
          rollover.  DNSSEC failures are reported once we added new
          view for new users after rolling the key. It is very
          likely to happen for new-hands to operate their resolvers
          who may intuitively all the views can be inhered the
          managed key during KSK rollover.  In addition, by checking
          the manual of BIND9.10.4-P2, it is said that unlike
          trusted-keys, managed-keys may only be set at the top
          level of named.conf, not within a view.  It gives an
          assumption that for each view, managed-key can not be set
          per view in BIND. So it may happen for senior engineers
          they may also encounter this problem. After setting the
          managed-keys of new views, the DNSSEC validation works
          for this view.</t>

        <t>This issue was reported in 25th DNS-OARC meeting to make
          it widely aware.  We suggest currently BIND multiple-view
          operation needs extra guidance for RFC5011. The manage-keys
          should be set carefully during the KSK rollover for each
          view when the it is created. Afterwards ISC published a
          report on this issue during KSK rollover for BIND users.</t>

        <t>The third KSK rollover is started in 2017-03-02 after
          the ZSK is increased to 2048 bits. The focus was put on
          the monitoring and impact analysis of large DNS response
          in Yeti testbed especially for DNSKEY queries. As it is
          introduced in section 4.2.1, Yeti KSK rollover has issue
          when large DNS response is generated and transmitted via
          IPv6. A gloomy atmosphere in the community is observed
          for DNS in IPv6-only environment.  In the authors' opinion
          maybe it is time to consider different transmit protocol
          for large DNS response, for example the DNSKEY query
          during KSK rollover.</t>
      </section>

      <section title="XXX Question for Davey XXX">
        <t>With reference to the following section on capture of
          large messages:</t>

        <t>The main reason operators are able to use generic packet
          capture to sample or record active traffic without worrying
          about fragmentation or TCP reassembly is that they capture
          DNS queries, not responses. It is very unusual for a query
          to be so big that it can't fit in a small DNS message,
          and although there are various aspects of the Yeti project
          that make large responses more prevalent, there is nothing
          that would inflate queries.</t>

        <t>I think it's entirely reasonable to mention the tool
          written by BII that reassembles fragmented UDP responses
          and recovers DNS messages from TCP streams, but I don't
          see that the description belongs in the parent section
          that discusses operational experiences. Perhaps recast
          this as a description of a tool in <xref
          target="Yeti_tools"/>?</t>

        <t>On the other hand if there's something novel about the
          Yeti infrastructure that makes it more important to
          capture responses, perhaps that should be spelt out?</t>
      </section>

      <section title="Capture of Large DNS Messages">

        <t>Many DNS operators capture DNS packets for operational
          purposes, such as gathering statistics, tracking abuse,
          and so on.  While in principle something designed for
          logging DNS like dnstap might be better, in practice
          capturing packets with an external tool and then storing
          them in pcap format is much more common. These tools may
          be something DNS-specific, like dnscap, or they may be a
          generic tool like tcpdump.</t>

        <t>Yeti testbed captures DNS packets using dnscap to collect
          all packet information. The problem with fragmentation
          for pcap is that pcap files usually contain the IP fragments
          as separate packets, which means that the DNS message is
          split up and cannot be easily parsed or otherwise processed.
          For DNS over TCP, it will be worse that a single message
          may be split over multiple TCP packets, or a single TCP
          packet may contain multiple messages. Since pcap is
          packet-oriented, it is difficult to get to the underlying
          DNS messages.</t>

        <t>Since very few DNS messages are large enough to require
          fragmentation or truncation, a common approach is simply
          to ignore these messages when analyzing at pcap packet
          captures. However, in the Yeti testbed we have been using
          DNS configurations where our servers return large answers.
          This means that we have a lot of fragmented IP packets
          and TCP.</t>

        <t>The approach that we took was to build a tool that takes
          a pcap input and:

          <list style="numbers">
            <t>Defragments any fragmented IP packets; and</t>
            <t>Extracts any DNS messages that it finds in TCP streams.</t>
          </list>

          IP-format pcap was used, because that's what is used in
          Yeti testbed. Ethernet framing information is not necessary,
          for example. The output of this tool is a pcap file without
          DNS in IP fragments or carried by TCP. Any non-DNS packets
          are copied across unchanged (this may be useful for looking
          at ICMP unreachable messages, for example). Any non-fragmented
          UDP DNS packets are also copied unchanged.</t>

        <t>Most of the "heavy lifting" of this processing is done
          via Google's gopacket library. This allows us to read and
          write pcap files, and also provides functionality for
          IPv4 defragmentation as well as TCP reassembly. The library
          did not support IPv6 defragmentation. We are pushing the
          changes upstream, so hopefully these will be a part of
          the base library soon.</t>
      </section>

      <section title="Automated Hints File Maintenance">
        <t>Renumbering events in the Root Server System are relatively
          rare. Although each such event is accompanied by the
          publication of an updated hints file in standard locations,
          the task of updating local copies of that file used by
          DNS resolvers is manual, and the process has an observably-long
          tail: for example, in 2015 J-Root was still receiving
          traffic at its old address some thirteen years after
          renumbering <xref target="Wessels2015"/>.</t>

        <t>The observed impact of these old, deployed hints file
          is minimal, likely due to the very low frequency of such
          renumbering events. Even the oldest of hints file would
          still contain some accurate root server addresses from
          which priming responses could be obtained.</t>

        <t>By contrast, due to the experimental nature of the system
          and the fact that it is operated mainly by volunteers,
          Yeti-Root Servers are added, removed and renumbered with
          much greater frequency. A tool to facilitate automatic
          maintenance of hints files was therefore created, <xref
          target="hintUpdate"/>.</t>

        <t>The automated procedure followed by the hintUpdate tool
          is as follows.

          <list style="numbers">
            <t>Use the local resolver to obtain a response to the
              query ./IN/NS;</t>

            <t>Use the local resolver to obtain a set of IPv4 and
              IPv6 addresses for each name server;</t>

            <t>Validate all signatures obtained from the local
              resolvers, and confirm that all data is signed;</t>

            <t>Compare the data obtained to that contained within
              the currently-active hints file; if there are differences,
              rotate the old one away and replace it with a new
              one.</t>
          </list>
        </t>

        <t>This tool would not function unmodified when used in the
          Root Server System, since the names of individual Root
          Servers (e.g. A.ROOT-SERVERS.NET) are not signed. All
          Yeti-Root Server names are signed, however, and hence
          this tool functions as expected in that environment.</t>
      </section>

<!-- Too trivial as an experience
      <section title="Root Label Compression in Knot">
        <t><xref target="RFC1035"/> specifies that domain names can
          be compressed when encoded in DNS messages, being represented
          as one of

          <list style="numbers">
            <t>a sequence of labels ending in a zero octet;</t>

            <t>a pointer; or</t>

            <t>a sequence of labels ending with a pointer.</t>
          </list>

          The purpose of this flexibility is to reduce the size of
          domain names encoded in DNS messages.</t>

        <t>It was observed that Yeti-Root Servers running knot 2.0
          would compress the zero-length label (the root domain, often
          represented as ".") using a pointer to an earlier example.
          Although legal, this encoding increases the encoded size
          of the root label from one octet to two; it was also found
          to break some client software, in particular the Go DNS
          library. Bug reports were filed against both knot and the
          Go DNS library, and both were resolved in subsequent
          releases.</t>
      </section>

      <section title="Increased ZSK Key Size">
        <t>The ZSK key size used in the Yeti-DNS Testbed was initially
          1024 bits, consistent with the size of the ZSK used in
          the Root Zone at the time the Yeti DNS Project was started.
          It later became clear that the ZSK key size in the Root
          Zone was to be increased.</t>

        <t>The ZSK key size in the Yeti-Root zone was subsequently
          increased in an attempt to identify any unexpected
          operational effects of doing so.</t>

        <t>XXX Note to reviewers: observations following that change
          to be inserted here. XXX DaveyNOte: There are some
          background in the discusion of inreasing the ZSK key Size.
          Calculate the increase of DNS response</t>

        <t>The ZSK key size in the Root Zone was increased from
          1024 bits to 2048 bits in October 2016.  <xref
          target="Verisign2016"/>.</t>
      </section>

      <section title="The KSK Rollover Experiment in Yeti">
        <t>The Yeti DNS Project provides a good basis to conduct a
          real-world experiment of a KSK rollover in the root zone.
          It is not a perfect analogy to the IANA root because all
          of the resolvers to the Yeti experiment are "opt-in", and
          are presumably run by administrators who are interested
          in the DNS and knowledgeable about it. Still, it can
          inform the IANA root KSK roll.</t>

        <t>The IANA root KSK has not been rolled as of the writing.
          ICANN put together a design team to analyze the problem
          and make recommendations.  The design team put together
          a plan<xref target="ICANN-ROOT-ROLL"/>.  The Yeti DNS
          Project may evaluate this scenario for an experimental
          KSK roll.  The experiment may not be identical, since the
          time-lines laid out in the current IANA plan are very
          long, and the Yeti DNS Project would like to conduct the
          experiment in a shorter time, which may considered much
          difficult.</t>

        <t>The Yeti KSK is rolled twice in Yeti testbed as of the
          writing.  In the first trial, it made old KSK inactive
          and new key active in one week after new key created, and
          deleted the old key in another week, which was totally
          unaware the timer specified in RFC5011. Because the
          hold-down timer was not correctly set in the server side,
          some clients (like Unbound) receive SERVFAILs (like dig
          without +cd) because the new key was still in AddPend
          state when old key was inactive. The lesson from the first
          KSK trial is that both server and client should compliant
          to RFC5011.</t>

        <t>For the second KSK rollover, it waited 30 days after a
          new KSK is published in the root zone. Different from
          ICANN rollover plan, it revokes the old key once the new
          key become active.  We don't want to wait too long, so
          we shorten the time for key publish and delete in server
          side. As of the writing, only one bug <xref
          target="KROLL-ISSUE"/>spotted on one Yeti resolver (using
          BIND 9.10.4-p2) during the second Yeti KSK rollover. The
          resolver is configured with multiple views before the KSK
          rollover. DNSSEC failures are reported once we added new
          view for new users after rolling the key. By checking the
          manual of BIND9.10.4-P2, it is said that unlike trusted-keys,
          managed-keys may only be set at the top level of named.conf,
          not within a view. It gives an assumption that for each
          view, managed-key can not be set per view in BIND. But
          right after setting the managed-keys of new views, the
          DNSSEC validation works for this view.  As a conclusion
          for this issue, we suggest currently BIND multiple-view
          operation needs extra guidance for RFC5011.  The manage-keys
          should be set carefully during the KSK rollover for each
          view when the it is created.</t>

        <t>Another of the questions of KSK rollover is how can an
          authority server know the resolver is ready for RFC5011.
          Two Internet-Drafts <xref target="I-D.wessels-edns-key-tag"/>
          and <xref target="I-D.wkumari-dnsop-trust-management"/>
          try to address the problem. In addition a compliant
          resolver implementation may fail without any complain if
          it is not correctly configured. In the case of Unbound
          1.5.8, the key is only readable for DNS users <xref
          target="auto-trust-anchor-file"/>.</t>
      </section>

      <section title="Bigger ZSK for Yeti">
        <t>Currently IANA root system uses 1024-bits ZSK which is
          no longer recommended cryptography.  VeriSign announced
          at DNS-OARC 24th workshop that the IANA root zone ZSK
          will be increased from 1024 bits to 2048 bits in 2016.
          However, it is not fully tested by the real environment.</t>

        <t>Bigger key tend to produce a larger response which
          requires IP fragmentation and is commonly considered harm
          for DNS system. In Yeti DNS Project, it is desirable to
          test bigger responses in many aspects. The Big ZSK
          experiment is designed to test operating the Yeti root
          with a 2048-bit ZSK. The traffic is monitored before and
          after we lengthen the ZSK to see if there are any changes,
          such as a drop off of packets or a increase in retries.
          The current status of this experiment is under monitoring
          data analysis.</t>
      </section>
-->
    </section>
<!--
    <section title="XXX Question for Davey XXX">
      <t>This conclusion doesn't seem to say anything that wasn't
        already described in greater detail in the document; I
        don't see any actual conclusions here; it's more like a
        repeat of the abstract.</t>

      <t>If we are going to include a conclusion section I think
        it would make sense to identify some more concrete
        conclusions. Perhaps

        <list style="symbols">
          <t>commentary on the practicalities of building a testbed
            across the Internet, as an alternative to building a
            captive environment within a segregated lab;</t>

          <t>observations about the way that the many participants
            were coordinated across organisational boundaries and
            time zones, the mechanisms that work and those that
            did not;</t>

          <t>any findings that are applicable to the real root
            server system, and which might plausibly provide
            useful data to future efforts to modify it;</t>

          <t>any areas of future study prompted by experiences
            with the Yeti testbed that might be worthy of
            investigation.</t>
        </list>
      </t>
    </section>
-->
    <section title="Conclusion">
            <t>Yeti DNS is designed as a live root testbed. It serves the
        root zone published by the IANA with only those structural
        modifications necessary to ensure its function as a testbed
        system. There are a list of questions and issues in the
        beginning of Yeti DNS testbed was built up. The experience
        of Yeti DNS did not answered all the issues, but it did
        give a clue from running such testbed with structural changes
        to the root server system's infrastructure. </t>

        <t>The feature of testbed and findings are listed briefly 
        in the below:
        <list style="symbols">
            <t>Yeti DNS has been running in IPv6-only environment. Around 7% 
              Packets drop rate were observed due to large DNS response. No 
              notable failures reported. </t>

              <t>
              There are two cases in Yeti testbed reported that some Yeti 
              root servers failed to pull the zone during to the IPv6 fragments 
              of large TCP segment. The TCP MSS for both DM and root server is 
              1220 octets.
              </t>

             <t>Yeti DNS is composed with 3 root zone Signers and 25 root 
              servers. Due to the weakness of IXFR in multiple Signers model. 
              IXFR is turned off in 3 DMs. </t>

            <t>
              ZSK is increased to 2048 bits long and the KSK were rolled 
              three times promptly to test against RFC5011. It is only 
              founded that new views added in BIND9 during the rollover 
              without configure the new KSK will be affected.</t>
              
            <t>
              Yeti use normal domains for root servers. One finding is called
              BIND9 glue issues. BIND9 server responses to priming queries 
              doesn't give addresses for all of the servers in the additional 
              section. It is not technically necessary, but resolvers expect 
              this information to save additional queries. 
            </t>
            <t>
              It is found that Knot 2.0 server perform the message compression 
              on root. It means in a DNS message the name of root (a zero octet) 
              is replaced by a pointer of 2 octets. 
            </t>
            <t>
              It is observed one server on Yeti testbed have some bugs on SOA 
              update with more than 10 hours delay. It is running on Bundy 1.2.0
              on FreeBSD 10.2-RELEASE.
            </t>
            <t>
              In Yeti more dynamics was observed due to its feature as a testbed. 
              A tool to facilitate automatic maintenance of hints files was therefore
              created 
            </t>
            <t>
              Some tools are developed to fit the requirement of Yeti testbed: DNS 
              fragment and DNS ATR for large DNS response issue, BIND9 Patch for 
              glue issues, YmmV and IPv6 defrag for capturing and mirroring 
              traffic. In addition a tool to facilitate automatic maintenance of 
              hints files was created.
            </t>
        </list>
      </t>


      <t>Though there are some issues and bug reported, no serious issues 
        are discovered blocking Yeti down. It shows the ability to run
        new experiments and ideas without breaking current production
        system. It also shows a ability of Yeti-style locally signed
        root zone for pervasively distribution of signed root zone. 
      </t>
      <t>
        There are some areas listed below which is worth of future study 
        base on the experience of Yeti testbed: 
      <list style="symbols">
        <t>
        Priming Truncation and TCP-only Root. To measure the worst-possible case for priming  truncation by truncating all priming query answers. This should also give some insight into the usefulness of TCP-only DNS in general.
        </t>

        <t>
         KSK ECDSA Roll. One possible way to reduce packet sizes is to change to an ecliptic curve for signing. ECDSA is a standard way of doing that in DNSSEC. While in principle this can be done separately for the KSK and the ZSK, the RIPE NCC has done research recently and discovered that some resolvers require that both KSK and ZSK use the same algorithm. This means that an algorithm roll also involves a KSK roll. Performing an algorithm roll at the root is an interesting challenge.
        </t>

        <t>
         Sticky Notify for zone transfer.The IXFR issue in Yeti testbed stems 
         from the fact that the slave has no preference and view each DM functional 
         equivalent. The slave will pull the zone from whichever it receives 
         the notify. To make IXFR workable for multiple signer case, it is more 
         desirable that slave always sticky to one master until the zone transfer fail.  
        </t>

        <t>
          Signature for Notify message. One security consideration mentioned in 
          RFC1996 is that a NOTIFY request with a forged IP/UDP source address 
          can cause a denial of service to the Master. It also true that the 
          slave is also compromised during on-path attack without TSIG or any 
          validation process for that notify. The master can encounter this 
          issues by signing the Notify message to authenticate itself. 
        </t>

        <t>
          Secure distribution of TSIG Key. TSIG uses shared secret keys and 
          one-way hashing to provide authenticating DNS zone update. If more 
          slaves added into the DNS system, there are more likely leakage of 
          TSIG key to make the system weak. So some secure distribution of TSIG 
          Key is worth of study and consideration for more open root system 
          with more slaves.
        </t>

      </list>
    </t>


<!--
      <t>Yeti testbed was built to address includes IPv6-only
        operation, DNSSEC key rollover, renumbering issues, scalability
        issues and multiple zone file signers etc. Most of them are
        not well studied before Yeti DNS was setup.  Yeti did not
        answered all the issues, but it did give a clue from
        experience of running such testbed with structural changes
        to the root server system's infrastructure. As a conclusion,
        some experiences and recommendations are listed below.

        <list style="numbers">
          <t>The first one is that IPv6-only Root is operational
            feasible and reasonable, but necessary care should be
            taken to handle Large response.  As IPv6-only Internet
            is the future, Yeti DNS provides a place to "sunset"
            IPv4 in root-level DNS.  From our experience the only
            requirement is that the resolver should be dual-stack.
            The risk for root system running in pure IPv6 network
            is that there is no IPv4 fallback if IPv6 DNS fails.
            However, from our experience the resolvers can survive
            and go though by falling back to TCP or retries to other
            root servers.  It is not fatal but puts gloomy atmosphere
            for IPv6 transition and sunsetting IPv4 for DNS as an
            important Internet infrastructure.</t>

          <t>The Multiple signer is a good proof-of-concept
            implementation for the idea of "Share Zone Control".
            It successfully demonstrated the ability to add more
            signers(three in Yeti) to manage the root zone
            independently. It increases publicity,operational
            diversity and redundancy for root system. The only
            impact is that more ZSKs will be included in DNSKEY
            RRset which increases the size of response for DNSKEY
            query. Anyone who want to implement multiple signer in
            their DNS system, they should take care the fragmentation
            issues.</t>

          <t>Yeti DNS proved the concept of "one name space, multiple
            circles" as an answer to scaling properties of root
            zone distribution.  Now Yeti DNS runs 25 root servers
            by increasing the priming response. Before additional
            treatment adopted to handle large DNS response issue,
            it is not wise to increase the number of root server
            operator by adding more NS server to the base 13.  On
            the contrary Yeti DNS serves a root zone which is
            slightly different from original one generated by IANA.
            It is technical and operational feasible maintain
            multiple versions of root zone signed by IANA's KSK.
            It will introduce more than one set of 13 or 25 root
            severs.</t>

          <t>From Yeti experiences on renumbering issue and KSK
            rollover in Yeti, it is worth of doing out-of-bound
            automatic mechanisms to improve the rate at which clients
            of Yeti-Root servers are able to react to the events
            of both Root server renumbering and KSK rollover. For
            example for DNS implementations, additional threads or
            coroutines can be implemented coordinating with the
            main process of BIND and Unbound. Resource lists like
            hint file and KSK should be fixed and available as
            well.</t>
        </list>
      </t>
-->
    </section>

    <section title="IANA Considerations">
      <t>This document requests no action of the IANA.</t>
    </section>

    <section title="Acknowledgments">
      <t>The editors would like to acknowledge the contributions of
        the various and many subscribers to the Yeti DNS Project
        mailing lists, including the following people who were
        involved in the implementation and operation of the Yeti
        DNS testbed itself:</t>

      <t>
        <list style="empty">
          <t>Tomohiro Ishihara, Antonio Prado, Stephane Bortzmeyer,
            Mickael Jouanne, Pierre Beyssac, Joao Damas, Pavel
            Khramtsov, Ma Yan, Otmar Lendl, Praveen Misra, Carsten
            Strotmann, Edwin Gomez, Remi Gacogne, Guillaume de Lafond,
            Yves Bovard, Hugo Salgado-Hernandez, Li Zhen, Daobiao
            Gong, Runxia Wan.</t>
        </list>
      </t>

      <t>The editors also acknowledge the assistance of the Independent
        Submissions Editorial Board, and of the following reviewers
        whose opinions helped improve the clarity of this document:</t>

      <t>
        <list style="empty">
          <t>Subramanian Moonesamy, Joe Abley.</t>
        </list>
      </t>
    </section>
  </middle>

  <back>
    <references title="References">
      &RFC1034;
      &RFC1035;
      &RFC1995;
      &RFC1996;
      &RFC2826;
      &RFC2845;
      &RFC5011;
      &RFC5890;
      <!-- &RFC5936; -->
      &RFC6219;
      &RFC6891;
      &RFC7720;
      &RFC7872;
      &RFC8109;
      &I-D.muks-dns-message-fragments;
      <!-- &I-D.wkumari-dnsop-trust-management; -->
      &I-D.andrews-tcp-and-ipv6-use-minmtu;
      <!-- &I-D.wessels-edns-key-tag; -->
      <!-- &I-D.bortzmeyer-dname-root; -->
      &I-D.taylor-v6ops-fragdrop;
      &I-D.song-atr-large-resp;

      <reference anchor="ITI2014"
        target="https://www.icann.org/en/system/files/files/iti-report-15may14-en.pdf">
        <front>
          <title>Identifier Technology Innovation Report</title>
          <author/>
          <date day="15" month="May" year="2014"/>
        </front>
      </reference>

<!--
      <reference anchor="Fragmenting-IPv6"
        target="http://blog.apnic.net/2016/05/19/fragmenting-ipv6/">
        <front>
          <title>Fragmenting-IPv6</title>
          <author fullname="Geoff Huston" initials="G." surname="Huston"/>
          <date month="May" year="2016" />
        </front>
      </reference>
-->

      <reference anchor="Wessels2015"
        target="https://indico.dns-oarc.net/event/24/session/10/contribution/10/material/slides/0.pdf">
        <front>
          <title>Thirteen Years of Old J-Root</title>
          <author fullname="Duane Wessels" initials="D." surname="Wessels"/>
          <date year="2015" />
        </front>
      </reference>
<!--
      <reference anchor="Verisign2016"
        target="https://blog.verisign.com/security/increasing-the-strength-of-the-zone-signing-key-for-the-root-zone/">
        <front>
          <title>Increasing the Strength of the Zone Signing Key
            for the Root Zone</title>
          <author fullname="Duane Wessels" initials="D." surname="Wessels"/>
          <date day="6" month="May" year="2016"/>
        </front>
      </reference>

      <reference anchor="Root-Zone-Database"
        target="http://www.iana.org/domains/root/db">
        <front>
          <title>Root Zone Database</title>
          <author/>
          <date/>
        </front>
      </reference>
-->

      <reference anchor="ICANN2010"
        target="http://www.root-dnssec.org/wp-content/uploads/2010/05/draft-icann-dnssec-keymgmt-01.txt">
        <front>
          <title>DNSSEC Key Management Implementation for the Root Zone</title>
          <author/>
          <date day="11" month="May" year="2010"/>
        </front>
      </reference>

      <reference anchor="ICANN2016"
        target="https://www.iana.org/reports/2016/root-ksk-rollover-design-20160307.pdf">
        <front>
          <title>Root Zone KSK Rollover Plan</title>
          <author/>
          <date year="2016" />
        </front>
      </reference>

      <reference anchor="ICANN2017"
        target="https://www.icann.org/en/system/files/files/ksk-rollover-external-test-plan-22jul16-en.pdf">
        <front>
          <title>2017 KSK Rollover External Test Plan</title>
          <author/>
          <date day="22" month="July" year="2016"/>
        </front>
      </reference>

     <reference anchor="IPv6-frag-DNS" target="https://blog.apnic.net/2017/08/22/dealing-ipv6-fragmentation-dns">
            <front>
                <title>Dealing with IPv6 fragmentation in the DNS</title>
                <author fullname= "Goeff Huston">
                  <organization></organization>
                  <address></address>
                </author>
                <date year="2017" month="August" day="22"/>
            </front>
     </reference>  
<!--
      <reference anchor="ROOT-FAQ"
        target="https://www.isoc.org/briefings/020/">
        <front>
          <title>DNS Root Name Server FAQ</title>
          <author fullname="Daniel Karrenberg" initials="D." surname="Karrenberg"/>
          <date year="2007" />
        </front>
      </reference>
-->

<!--
      <reference anchor="pcapdump-bug-report"
        target="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985">
        <front>
          <title>pcaputils: IWBN to have an option to run a program
            after file rotation in pcapdump </title>
          <author fullname="Stephane Bortzmeyer"  initials="S."
            surname="Bortzmeyer"/>
          <date year="2009"/>
        </front>
      </reference>
-->

<!--
      <reference anchor="Yeti-DNS-Project"
        target="http://www.yeti-dns.org">
        <front>
          <title>Website of Yeti DNS Project</title>
          <author/>
          <date />
        </front>
      </reference>
-->

<!--
      <reference anchor="Experiment-MZSK-notes"
        target="https://github.com/shane-kerr/Yeti-Project/blob/experiment-mzsk/doc/Experiment-MZSK-notes.md">
        <front>
          <title>MZSK Experiment Notes</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>
-->

<!--
      <reference anchor="Yeti-DM-Sync-MZSK"
        target="https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Yeti-DM-Sync-MZSK.md">
        <front>
          <title>Yeti DM Synchronization for MZSK</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>
-->

      <reference anchor="hintUpdate"
        target="https://github.com/BII-Lab/Hintfile-Auto-Update">
        <front>
          <title>Hintfile Auto Update</title>
          <author/>
          <date year="2015"/>
        </front>
      </reference>

<!--
      <reference anchor="auto-trust-anchor-file"
        target="https://www.nlnetlabs.nl/bugs-script/show_bug.cgi?id=758">
        <front>
          <title> Unbound should test that auto-* files are writable</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>
-->

      <reference anchor="KROLL-ISSUE"
        target="http://yeti-dns.org/yeti/blog/2016/10/26/A-DNSSEC-issue-during-Yeti-KSK-rollover.html">
        <front>
          <title>A DNSSEC issue during Yeti KSK rollover</title>
          <author/>
          <date year="2016" />
        </front>
      </reference>

      <reference anchor="TNO2009"
        target="https://www.icann.org/en/system/files/files/root-scaling-model-description-29sep09-en.pdf">
        <front>
          <title>Root Scaling Study: Description of the DNS Root Scaling
            Model</title>
          <author fullname="Bart Gijsen" initials="B." surname="Gijsen"/>
          <author fullname="Almerima Jamakovic" initials="A."
            surname="Jamakovic"/>
          <author fullname="Frank Roijers" initials="F." surname="Roijers"/>
          <date day="29" month="September" year="2009"/>
        </front>
      </reference>

      <reference anchor="ISC-TN-2003-1"
        target="http://ftp.isc.org/isc/pubs/tn/isc-tn-2003-1.txt">
        <front>
          <title>Hierarchical Anycast for Global Service Distribution</title>
          <author fullname="Joe Abley" initials="J." surname="Abley"/>
          <date month="March" year="2003"/>
        </front>
      </reference>

      <reference anchor="RSSAC001"
        target="https://www.icann.org/en/system/files/files/rssac-001-root-service-expectations-04dec15-en.pdf">
        <front>
          <title>Service Expectations of Root Servers</title>
          <author/>
          <date day="4" month="December" year="2015"/>
        </front>
      </reference>

      <reference anchor="RSSAC023"
        target="https://www.icann.org/en/system/files/files/rssac-023-04nov16-en.pdf">
        <front>
          <title>History of the Root Server System</title>
          <author/>
          <date day="4" month="November" year="2016"/>
        </front>
      </reference>

      <reference anchor="RRL"
        target="http://www.redbarn.org/dns/ratelimits">
        <front>
          <title>Response Rate Limiting (RRL)</title>
          <author fullname="Paul Vixie" initials="P." surname="Vixie"/>
          <author fullname="Vernon Schryver" initials="V." surname="Schryver"/>
          <date day="10" month="June" year="2012"/>
        </front>
      </reference>
    </references>

    <section anchor="yeti-hints" title="Yeti-Root Hints File">
      <t>The following hints file (complete and accurate at the
        time of writing) causes a DNS resolver to use the Yeti DNS
        testbed in place of the production Root Server System and
        hence participate in experiments running on the testbed.</t>

      <t>Note that some lines have been wrapped in the text that
        follows in order to fit within the production constraints of
        this document. Wrapped lines are indicated with a blackslash
        character ("\"), following common convention.</t>

      <figure>
        <artwork>
          <![CDATA[
.                     3600000  IN   NS     bii.dns-lab.net
bii.dns-lab.net       3600000  IN   AAAA   240c:f:1:22::6
.                     3600000  IN   NS     yeti-ns.tisf.net
yeti-ns.tisf.net      3600000  IN   AAAA   2001:559:8000::6
.                     3600000  IN   NS     yeti-ns.wide.ad.jp
yeti-ns.wide.ad.jp    3600000  IN   AAAA   2001:200:1d9::35
.                     3600000  IN   NS     yeti-ns.as59715.net
yeti-ns.as59715.net   3600000  IN   AAAA   \
                           2a02:cdc5:9715:0:185:5:203:53
.                     3600000  IN   NS     dahu1.yeti.eu.org
dahu1.yeti.eu.org     3600000  IN   AAAA   \
                           2001:4b98:dc2:45:216:3eff:fe4b:8c5b
.                     3600000  IN   NS     ns-yeti.bondis.org
ns-yeti.bondis.org    3600000  IN   AAAA   2a02:2810:0:405::250
.                     3600000  IN   NS     yeti-ns.ix.ru
yeti-ns.ix.ru         3600000  IN   AAAA   2001:6d0:6d06::53
.                     3600000  IN   NS     yeti.bofh.priv.at
yeti.bofh.priv.at     3600000  IN   AAAA   2a01:4f8:161:6106:1::10
.                     3600000  IN   NS     yeti.ipv6.ernet.in
yeti.ipv6.ernet.in    3600000  IN   AAAA   2001:e30:1c1e:1::333
.                     3600000  IN   NS     yeti-dns01.dnsworkshop.org
yeti-dns01.dnsworkshop.org \
                      3600000  IN   AAAA   2001:1608:10:167:32e::53
.                     3600000  IN   NS     yeti-ns.conit.co
yeti-ns.conit.co      3600000  IN   AAAA   \
                          2604:6600:2000:11::4854:a010
.                     3600000  IN   NS     dahu2.yeti.eu.org
dahu2.yeti.eu.org     3600000  IN   AAAA   2001:67c:217c:6::2
.                     3600000  IN   NS     yeti.aquaray.com
yeti.aquaray.com      3600000  IN   AAAA   2a02:ec0:200::1
.                     3600000  IN   NS     yeti-ns.switch.ch
yeti-ns.switch.ch     3600000  IN   AAAA   2001:620:0:ff::29
.                     3600000  IN   NS     yeti-ns.lab.nic.cl
yeti-ns.lab.nic.cl    3600000  IN   AAAA   2001:1398:1:21::8001
.                     3600000  IN   NS     yeti-ns1.dns-lab.net
yeti-ns1.dns-lab.net  3600000  IN   AAAA   2001:da8:a3:a027::6
.                     3600000  IN   NS     yeti-ns2.dns-lab.net
yeti-ns2.dns-lab.net  3600000  IN   AAAA   2001:da8:268:4200::6
.                     3600000  IN   NS     yeti-ns3.dns-lab.net
yeti-ns3.dns-lab.net  3600000  IN   AAAA   2400:a980:30ff::6
.                     3600000  IN   NS     \
                        ca978112ca1bbdcafac231b39a23dc.yeti-dns.net
ca978112ca1bbdcafac231b39a23dc.yeti-dns.net \
                      3600000  IN   AAAA   2c0f:f530::6
.                     3600000  IN   NS     \
                        3e23e8160039594a33894f6564e1b1.yeti-dns.net
3e23e8160039594a33894f6564e1b1.yeti-dns.net \
                      3600000  IN   AAAA   2803:80:1004:63::1
.                     3600000  IN   NS     \
                        3f79bb7b435b05321651daefd374cd.yeti-dns.net
3f79bb7b435b05321651daefd374cd.yeti-dns.net \
                      3600000  IN   AAAA   2401:c900:1401:3b:c::6
.                     3600000  IN   NS     \
                        xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c
xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c \
                      3600000  IN   AAAA   2001:e30:1c1e:10::333
.                     3600000  IN   NS     yeti1.ipv6.ernet.in
yeti1.ipv6.ernet.in   3600000  IN   AAAA   2001:e30:187d::333
.                     3600000  IN   NS     yeti-dns02.dnsworkshop.org
yeti-dns02.dnsworkshop.org \
                      3600000  IN   AAAA   2001:19f0:0:1133::53
.                     3600000  IN   NS     yeti.mind-dns.nl
yeti.mind-dns.nl      3600000  IN   AAAA   2a02:990:100:b01::53:0
]]>
        </artwork>
      </figure>
    </section>

    <section anchor="yeti-priming" title="Yeti-Root Server Priming Response">
      <t>Here is the reply of a Yeti root name server to a priming
      request. The authoritative server runs NSD.</t>

      <figure>
        <artwork>
<![CDATA[
...
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 62391
;; flags: qr aa rd; QUERY: 1, ANSWER: 26, AUTHORITY: 0, ADDITIONAL: 7
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags: do; udp: 1460
;; QUESTION SECTION:
;.                      IN NS

;; ANSWER SECTION:
.            86400 IN NS bii.dns-lab.net.
.            86400 IN NS yeti.bofh.priv.at.
.            86400 IN NS yeti.ipv6.ernet.in.
.            86400 IN NS yeti.aquaray.com.
.            86400 IN NS yeti.jhcloos.net.
.            86400 IN NS yeti.mind-dns.nl.
.            86400 IN NS dahu1.yeti.eu.org.
.            86400 IN NS dahu2.yeti.eu.org.
.            86400 IN NS yeti1.ipv6.ernet.in.
.            86400 IN NS ns-yeti.bondis.org.
.            86400 IN NS yeti-ns.ix.ru.
.            86400 IN NS yeti-ns.lab.nic.cl.
.            86400 IN NS yeti-ns.tisf.net.
.            86400 IN NS yeti-ns.wide.ad.jp.
.            86400 IN NS yeti-ns.datev.net.
.            86400 IN NS yeti-ns.switch.ch.
.            86400 IN NS yeti-ns.as59715.net.
.            86400 IN NS yeti-ns1.dns-lab.net.
.            86400 IN NS yeti-ns2.dns-lab.net.
.            86400 IN NS yeti-ns3.dns-lab.net.
.            86400 IN NS xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c.
.            86400 IN NS yeti-dns01.dnsworkshop.org.
.            86400 IN NS yeti-dns02.dnsworkshop.org.
.            86400 IN NS 3f79bb7b435b05321651daefd374cd.yeti-dns.net.
.            86400 IN NS ca978112ca1bbdcafac231b39a23dc.yeti-dns.net.
.            86400 IN RRSIG NS 8 0 86400 (
                         20171121050105 20171114050105 26253 .
                         FUvezvZgKtlLzQx2WKyg+D6dw/pITcbuZhzStZfg+LNa
                         DjLJ9oGIBTU1BuqTujKHdxQn0DcdFh9QE68EPs+93bZr
                         VlplkmObj8f0B7zTQgGWBkI/K4Tn6bZ1I7QJ0Zwnk1mS
                         BmEPkWmvo0kkaTQbcID+tMTodL6wPAgW1AdwQUInfy21
                         p+31GGm3+SU6SJsgeHOzPUQW+dUVWmdj6uvWCnUkzW9p
                         +5en4+85jBfEOf+qiyvaQwUUe98xZ1TOiSwYvk5s/qiv
                         AMjG6nY+xndwJUwhcJAXBVmGgrtbiR8GiGZfGqt748VX
                         4esLNtD8vdypucffem6n0T0eV1c+7j/eIA== )

;; ADDITIONAL SECTION:
bii.dns-lab.net.        86400 IN AAAA 240c:f:1:22::6
yeti.bofh.priv.at.      86400 IN AAAA 2a01:4f8:161:6106:1::10
yeti.ipv6.ernet.in.     86400 IN AAAA 2001:e30:1c1e:1::333
yeti.aquaray.com.       86400 IN AAAA 2a02:ec0:200::1
yeti.jhcloos.net.       86400 IN AAAA 2001:19f0:5401:1c3::53
yeti.mind-dns.nl.       86400 IN AAAA 2a02:990:100:b01::53:0

;; Query time: 163 msec
;; SERVER: 2001:4b98:dc2:45:216:3eff:fe4b:8c5b#53
;; WHEN: Tue Nov 14 16:45:37 +08 2017
;; MSG SIZE  rcvd: 1222
]]>
        </artwork>
      </figure>
    </section>
    
    <section title="Active IPv6 Prefixes in Yeti DNS testbed"
      anchor="activev6">
      <texttable>
        <ttcol>Prefix</ttcol><ttcol>Originator</ttcol><ttcol>Location</ttcol>

        <c>240c::/28</c><c>BII</c><c>CN</c>
        <c>2001:6d0:6d06::/48</c><c>MSK-IX</c><c>RU</c>
        <c>2001:1488::/32</c><c>CZ.NIC</c><c>CZ</c>
        <c>2001:620::/32</c><c>SWITCH</c><c>CH</c>
        <c>2001:470::/32</c><c>Hurricane Electric, Inc.</c><c>US</c>
        <c>2001:0DA8:0202::/48</c><c>BUPT6-CERNET2</c><c>CN</c>
        <c>2001:19f0:6c00::/38</c><c>Choopa, LLC</c><c>US</c>
        <c>2001:da8:205::/48</c><c>BJTU6-CERNET2</c><c>CN</c>
        <c>2001:62a::/31</c><c>Vienna University Computer Center</c><c>AT</c>
        <c>2a02:2478::/32</c><c>Profitbricks GmbH</c><c>DE</c>
        <c>2001:1398:4::/48</c><c>BII</c><c>CN</c>
        <c>240c::/28</c><c>NIC Chile</c><c>CL</c>
        <c>2001:4490:dc4c::/46</c><c>NIB (National Internet Backbone)</c>
          <c>IN</c>
        <c>2a02:aa8:0:2000::/52</c><c>T-Systems-Eltec</c><c>ES</c>
        <c>2a03:b240::/32</c><c>Netskin GmbH</c><c>CH</c>
        <c>2801:1a0::/42</c><c>Universidad de Ibague</c><c>CO</c>
        <c>2a00:1cc8::/40</c><c>ICT Valle Umbra s.r.l.</c><c>IT</c>
        <c>2a02:cdc0::/29</c><c>ORG-CdSB1-RIPE</c><c>IT</c>
      </texttable>
    </section>

    <section title="Tools developed for Yeti DNS testbed" anchor="Yeti_tools">
      <t>Various tools were developed to support the Yeti DNS
        testbed, a selection of which are described briefly below.</t>

      <t>YmmV ("Yeti Many Mirror Verifier") is designed to make it
        easy and safe for a DNS administrator to capture traffic
        sent from a resolver to the Root Server System and to replay
        it towards Yeti-Root servers. Responses from both systems
        are recorded and compared, and differences are logged. See
        <eref target="https://github.com/BII-Lab/ymmv"/>.</t>

      <t>PcapParser is a module used by YmmV which reassembles
        fragmented IPv6 datagrams and TCP segments from a PCAP
        archive and extracts DNS messages contained within them.
        See <eref target="https://github.com/RunxiaWan/PcapParser"/>.</t>

      <t>DNS-layer-fragmentation implements DNS proxies that perform
        application-level fragmentation of DNS messages, based on
        <xref target="I-D.muks-dns-message-fragments"/>. The idea
        with these proxies is to explore splitting DNS messages in
        the protocol itself, so they will not by fragmented by the
        IP layer. See <eref
        target="https://github.com/BII-Lab/DNS-layer-Fragmentation"/>.</t>

      <t>DNS_ATR is an implementation of DNS ATR, as described in
         <xref target="I-D.song-atr-large-resp"/>.  DNS_ATR acts
         as a proxy between resolver and authoritative servers,
         forwarding queries and responses as a silent and transparent
         listener. Responses that are larger than a nominated
         threshold (1280 octets by default) trigger additional
         truncated responses to be sent immediately following the
         large response. See <eref
         target="https://github.com/songlinjian/DNS_ATR"/>.</t>
    </section>

    <section title="Controversy" anchor="controversy">
      <t>The Yeti DNS Project, its infrastructure and the various
        experiments that have been carried out using that infrastructure,
        have been described by people involved in the project in
        many public meetings at technical venues since its inception.
        The mailing lists using which the operation of the
        infrastructure has been coordinated are open to join, and
        their archives are public. The project as a whole has been
        the subject of robust public discussion.</t>

      <t>Some commentators have expressed concern that the Yeti DNS
        Project is, in effect, operating an alternate root,
        challenging the IAB's comments published in <xref
        target="RFC2826"/>. Other such alternate roots are considered
        to have caused end-user confusion and instability in the
        namespace of the DNS by the introduction of new top-level
        labels or the different use of top-level labels present in
        the Root Server System. The coordinators of the Yeti DNS
        Project do not consider the Yeti DNS Project to be an
        alternate root in this sense, since by design the namespace
        enabled by the Yeti-Root Zone is identical to that of the
        Root Zone.</t>

      <t>Some commentators have expressed concern that the Yeti DNS
        Project seeks to influence or subvert administrative policy
        relating to the Root Server System, in particular in the
        use of DNSSEC trust anchors not published by the IANA and
        the use of Yeti-Root Servers in regions where governments
        or other organisations have expressed interest in operating
        a Root Server. The coordinators of the Yeti-Root project
        observe that their mandate is entirely technical and has
        no ambition to influence policy directly; they do hope,
        however, that technical findings from the Yeti DNS Project
        might act as a useful resource for the wider technical
        community.</t>

      <t>Finally, some concern has been expressed about the possible
        applications of the Yeti DNS Project to the governments of
        countries where access to the Internet is subject to
        substantial centralised control, in contrast to most other
        jurisdictions where such controls are either lighter or not
        present. The coordinators of the Yeti DNS Project have taken
        care to steer all discussions and related decisions about
        the technical work of the project to public venues in the
        interests of full transparency, and encourage anybody
        concerned about the decision-making process to participate
        in those venues and review their archives directly.</t>
    </section>

    <section title="About This Document">
      <t>This section (and sub-sections) has been included as an
        aid to reviewers of this document, and should be removed
        prior to publication.</t>

      <section title="Venue">
        <t>The authors propose that this document proceed as an
          Independent Submission, since it documents work that,
          although relevant to the IETF, has been carried out
          externally to any IETF working group. However, a suitable
          venue for discussion of this document is the dnsop working
          group.</t>

        <t>Information about the Yeti DNS project and discussion
          relating to particular experiments described in this
          document can be found at <eref
          target="https://yeti-dns.org/"/>.</t>

        <t>This document is maintained in GitHub at
          <eref target="https://github.com/BII-Lab/yeti-testbed-experience"/>.</t>
      </section>

      <section title="Revision History">
        <section title="draft-song-yeti-testbed-experience-00 through -03">
          <t>Change history is available in the public GitHub
            repository where this document is maintained: <eref
            target="https://github.com/BII-Lab/yeti-testbed-experience"/>.</t>
        </section>

        <section title="draft-song-yeti-testbed-experience-04">
          <t>Substantial editorial review and rearrangement of text
            by Joe Abley at request of BII.</t>

          <t>Added what is intended to be a balanced assessment of
            the controversy that has arisen around the Yeti DNS
            Project, at the request of the Independent Submissions
            Editorial Board.</t>

          <t>Changed the focus of the document from the description
            of individual experiments on a Root-like testbed to the
            construction and motivations of the testbed itself,
            since that better describes the output of the Yeti DNS
            Project to date. In the considered opinion of this
            reviewer, the novel approaches taken in the construction
            of the testbed infrastructure and the technical challenges
            met in doing so are useful to record, and the RFC series
            is a reasonable place to record operational experiences
            related to core Internet infrastructure.</t>

          <t>Note that due to draft cut-off deadlines some of the
            technical details described in this revision of the
            document may not exactly match operational reality;
            however, this revision provides an indicative level of
            detail, focus and flow which it is hoped will be helpful
            to reviewers.</t>
        </section>

        <section title="draft-song-yeti-testbed-experience-05">
          <t>Added commentary on IPv6-only operation, IPv6
            fragmentation, applicablity to and use by IPv4-only
            end-users and use of multiple signers.</t>
        </section>

        <section title="draft-song-yeti-testbed-experience-06">
          <t>Conclusion; tools; editorial changes.</t>
        </section>
      </section>
    </section>
  </back>
</rfc> 
