<?xml version="1.0"?>

<!-- This template is for creating an Internet Draft using xml2rfc,
     which is available here: http://xml.resource.org. -->

<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
  <!-- One method to get references from the online citation libraries.
       There has to be one entity for each item to be referenced.
       An alternate method (rfc include) is described in the references. -->

  <!ENTITY RFC1034 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1034.xml">
  <!ENTITY RFC1035 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1035.xml">
  <!ENTITY RFC4968 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.4986.xml">
  <!ENTITY RFC5011 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5011.xml">
  <!ENTITY RFC7626 SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7626.xml">
  <!ENTITY RFC7720 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7720.xml">

  <!ENTITY I-D.muks-dns-message-fragments SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-muks-dns-message-fragments-00.xml">
  <!ENTITY I-D.wkumari-dnsop-trust-management SYSTEM 
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-wkumari-dnsop-trust-management-01.xml">
  <!ENTITY I-D.wessels-edns-key-tag SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-wessels-edns-key-tag-00.xml">
  <!ENTITY I-D.andrews-tcp-and-ipv6-use-minmtu
    SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-andrews-tcp-and-ipv6-use-minmtu-04.xml">
  <!ENTITY I-D.bortzmeyer-dname-root SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-bortzmeyer-dname-root-00.xml">
  <!ENTITY I-D.ietf-dnsop-resolver-priming SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-ietf-dnsop-resolver-priming-07.xml">
]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<!-- used by XSLT processors -->
<!-- For a complete list and description of processing instructions (PIs),
     please see http://xml.resource.org/authoring/README.html. -->

<!-- Below are generally applicable Processing Instructions (PIs) that most
     I-Ds might want to use. (Here they are set differently than their
     defaults in xml2rfc v1.32) -->

<!-- give errors regarding ID-nits and DTD validation -->
<?rfc strict="yes" ?>

<!-- generate a ToC -->
<?rfc toc="yes"?>

<!-- control the table of contents (ToC) -->
<?rfc tocappendix="yes"?>

<!-- the number of levels of subsections in ToC. default: 3 -->
<?rfc tocdepth="3"?>

<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc symrefs="yes"?>

<!-- sort the reference entries alphabetically -->
<?rfc sortrefs="yes" ?>

<!-- control vertical white space
     (using these PIs as follows is recommended by the RFC Editor) -->
<!-- do not start each main section on a new page -->
<?rfc compact="yes" ?>

<!-- keep one blank line between list items -->
<?rfc subcompact="no" ?>

<!-- end of list of popular I-D processing instructions -->

<?rfc comments="no" ?>
<?rfc inline="yes" ?>

<rfc category="info" docName="draft-song-yeti-testbed-experience-04"
    ipr="trust200902">

  <front>
    <title>Yeti DNS Testbed: Interim Results</title>

    <author fullname="Linjian Song" initials="L." surname="Song" role="editor">
      <organization abbrev="BII">Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>songlinjian@gmail.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <author fullname="Dong Liu" initials="D." surname="Liu" role="editor">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>dliu@biigroup.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <author fullname="Paul Vixie" initials="P." surname="Vixie" role="editor">
      <organization>TISF</organization>
      <address>
        <postal>
        <street>11400 La Honda Road</street>
        <city>Woodside</city>
        <region>California</region>
        <code>94062</code>
        <country>US</country>
      </postal>
      <email>vixie@tisf.net</email>
      <uri>http://www.redbarn.org/</uri>
      </address>
    </author>

    <author fullname="Akira Kato" initials="" surname="Kato" role="editor">
      <organization>Keio University/WIDE Project</organization>
      <address>
        <postal>
          <street>Graduate School of Media Design, 4-1-1 Hiyoshi, Kohoku</street>
          <city>Yokohama</city>
          <region></region>
          <code>223-8526</code>
          <country>JAPAN</country>
        </postal>
        <email>kato@wide.ad.jp</email>
        <uri>http://www.kmd.keio.ac.jp/</uri>
      </address>
    </author>

    <author fullname="Joe Abley" initials="J." surname="Abley" role="editor">
      <organization>Snake Hill Labs</organization>
      <address>
        <postal>
          <street>300-184 York Street</street>
          <city>London</city>
          <region>ON</region>
          <code>N6A 1B5</code>
          <country>Canada</country>
        </postal>
        <email>jabley@shl.io</email>
        <uri>http://www.shl.io/</uri>
      </address>
    </author>

    <date/>

    <!-- Meta-data Declarations -->

    <area>Internet Area</area>
    <workgroup>Internet Engineering Task Force</workgroup>

    <!-- <keyword>dns</keyword> -->

    <abstract>
      <t>The Internet's Domain Name System is built upon the
        foundation provided by the Root Server System -- that is,
        the critical infrastructure that serves the DNS root zone.</t>

      <t>Yeti DNS is an experimental, non-production testbed that
        aims to provide an environment where technical and operational
        experiments can safely be performed without risk to production
        infrastructure.  This testbed has been used by a broad
        community of participants to perform experiments that aim
	to inform operations and future development of the production
	DNS. Yeti DNS is an independently-coordinated project and
	is not affiliated with ICANN, IANA or any Root Server
	Operator.</t>

      <t>This document presents a summary of experiments performed
	during the first two years of the Yeti DNS project with
	their findings, and is intended to be a stable reference
	that supports ongoing DNS protocol and operations work in
	the IETF.</t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction">
      <t>The Domain Name System (DNS), as originally specified in
        <xref target="RFC1034"/> and <xref target="RFC1035"/>, has
        proved to be an enduring and important platform upon which
        almost every user of Internet services relies. Despite its
        longeivity, extensions to the protocol, new implementations
        and refinements to DNS operations continue to emerge both
        inside and outside the IETF.</t>

      <t>The Root Server System in particular has seen technical
	innovation and development in recent years, for example in
	the form of wide-scale anycast deployment, the mitigation
	of unwanted traffic on a global scale, the widespread
	deployment of Response Rate Limiting <xref target="RRL"/>,
	the introduction of IPv6 transport, the deployment of DNSSEC,
	changes in DNSSEC key sizes and preparations to roll the
	root zone's trust anchor. Together, even the projects listed
	in this brief summary imply tremendous operational change,
	all the more impressive when considered the necessary caution
	when managing Internet critical infrastructure, and the
	context of the adjacent administrative changes involved in
	root zone management and the (relatively speaking) massive
	increase in the the number of delegations in the root zone
	itself.</t>

      <t>Aspects of the operational structure of the Root Server
	System have been described in such documents as <xref
	target="TNO"/>, <xref target="ISC-TN-2003-1"/>, <xref
	target="RSSAC001"/> and <xref target="RFC7720"/>. Such
	references, considered together, provide sufficient insight
	into the operations of the system as a whole that it is
	straightforward to imagine structural changes to the root
	server system's infrastructure, and to wonder what the
	operational implications of such changes might be.</t>

      <t>The Yeti DNS Project was conceived in May 2015 to provide
	a captive, non-production testbed upon which the technical
	community could propose and run experiments designed to
	answer these kinds of questions. Coordination for the project
	was provided by the Dr Paul Vixie, Dr Akira Kato and the
	Beijing Internet Institute. Many volunteers collaborated
	to build a distributed testbed that at the time of writing
	includes 25 Yeti root servers with 15 operators and 
	handles experimental traffic from individual volunteers,
	universities, DNS vendors and distributed measurement
	networks.</t>

      <t>By design, the Yeti testbed system serves the root zone
	published by the IANA with only those structural modifications
	necessary to ensure that it is able to function usefully
	in the Yeti testbed system instead of the production Root
	Server system.  In particular, no delegation for any top-level
	zone is changed, added or removed from the IANA-published
	root zone to construct the root zone served by the Yeti
	testbed system. In this document, for clarity, we refer to
	the zone derived from the IANA-published root zone as the
	Yeti-Root zone.</t>

      <t>This document presents a summary of the experiments carried
        out as part of the Yeti DNS Project during its first two years
        of operation.</t>
    </section>

    <section title="Areas of Study">
      <t>The experiments described in this document are grouped loosely
	into multiple sets, each representing a somewhat distinct
	set of problem statements. These areas of study are summarised
	here in the form of lists of questions to give context to
	the experiments that follow.</t>

      <section title="Yeti-Root Zone Distribution">
        <t>
          <list style="symbols">
	    <t>What are the scaling properties of Yeti-Root zone
	      distribution as the number of Yeti-Root servers,
	      Yeti-Root server instances or intermediate distribution
	      points increase?</t>
          </list>
        </t>
      </section>

      <section title="Yeti-Root Server Names and Addressing">
        <t>
          <list style="symbols">
            <t>What naming schemes other than those closely analogous
	      to the use of ROOT-SERVERS.NET in the production root
	      zone are practical, and what are their respective
	      advantages and disadvantages?</t>

            <t>What are the risks and benefits of signing the zone that
              contains the names of the Yeti-Root servers?</t>

            <t>What automatic mechanisms might be useful to improve
	      the rate at which clients of Yeti-Root servers are
	      able to react to a Yeti-Root server renumbering
	      event?</t>
	  </list>
	</t>
      </section>

      <section title="IPv6-Only Yeti-Root Servers">
        <t>
          <list style="symbols">
            <t>Are there negative operational effects in the use of
              IPv6-only Yeti-Root servers, compared to the use of
              servers that are dual-stack?</t>

            <t>What effect does the IPv6 fragmentation model have on the
              operation of Yeti-Root servers, compared with that of IPv4?</t>
          </list>
        </t>
      </section>

      <section title="DNSSEC in the Yeti-Root Zone">
        <t>
          <list style="symbols">
            <t>Is it practical to sign the Yeti-Root zone using multiple,
              independently-operated DNSSEC signers and multiple
              corresponding ZSKs?</t>

            <t>To what extent is <xref target="RFC5011"/> supported by
              resolvers?</t>

	    <t>Does the KSK Rollover plan designed and in the process
	      of being implemented by ICANN work as expected on the
              Yeti testbed?</t>

            <t>What is the operational impact of using much larger RSA
              key sizes in the ZSKs used in the Yeti-Root?</t>

	    <t>What are the operational consequences of choosing
	      DNSSEC algorithms other than RSA to sign the Yeti-Root
	      zone?</t>
	  </list>
	</t>
      </section>
    </section>

    <section title="Yeti Testbed and Experiment Setup">
      <t>To use the Yeti testbed operationally, the information
        that is required for correct root name service is a matching
        set of the following:</t>

      <t>
        <list style="symbols">
          <t>a root "hints file"</t>
          <t>the root zone apex NS record set</t>
          <t>the root zone's signing key</t>
          <t>root zone trust anchor</t>
        </list>
      </t>

      <t>Although Yeti DNS Project publishes strictly IANA information
        for TLD data and meta-data, it is necessary to use a special
        hint file to replace the apex NS RRset with Yeti authority
        name servers, which will enable the resolves to find and
        stick to the Yeti root system.</t>

      <t>Below is a figure to demonstrate the topology of Yeti and
        the basic data flow, which consists of the Yeti distribution
        master, Yeti root server, and Yeti resolver: </t>

      <figure>
        <artwork> 
          <![CDATA[ 
                        +------------------------+
                      +-+     IANA Root Zone     +--+
                      | +-----------+------------+  |
+-----------+         |             |               | IANA root zone
|    Yeti   |         |             |               |
|  Traffic  |      +--v---+     +---v--+      +-----v+
| Collection|      |  BII |     | WIDE |      | TISF |
|           |      |  DM  |     |  DM  |      |  DM  |
+---+----+--+      +------+     +-+----+      +---+--+
    ^    ^         |              |               |
    |    |         |              |               |   Yeti root zone
    |    |         v              v               v
    |    |   +------+      +------+               +------+
    |    +---+ Yeti |      | Yeti |  . . . . . .  | Yeti |
    |        | Root |      | Root |               | Root |
    |        +---+--+      +---+--+               +--+---+
    |            |             |                      |
    | pcap       ^             ^                      ^ DNS lookup
    | upload     |             |                      |
    |
    |                   +--------------------------+
    +-------------------+      Yeti Resolvers      |
                        |     (with Yeti Hint)     |
                        +--------------------------+
]]>
        </artwork>
      </figure>
      <t>Figure 1. The topology of Yeti testbed</t>

      <section title="Distribution Master">
      <t>As shown in figure 1, the Yeti Root system takes the IANA
        root zone and performs minimal changes needed to serve the
        zone from the Yeti root servers instead of the IANA root
        servers. In Yeti, this modified root zone is generated by
        the Yeti Distribution Masters (DM), which provide it to the
        Yeti root servers.</t>

      <t>The zone generation process is:</t>

      <t>
        <list style="symbols">
          <t>DM downloads the latest IANA root zone at a certain time</t>
          <t>DM makes modifications to change from the IANA to Yeti
            root servers</t>
          <t>DM signs the new Yeti root zone with Yeti key</t>
          <t>DM publishes the new Yeti root zone to Yeti root servers</t>
        </list>
      </t>

      <t>While in principle this could be done by a single DM, Yeti
        uses a set of three DMs to avoid any sense that the Yeti
        DNS Project is run by a single organization.  Each of three
        DMs independently fetches the root zone from IANA, signs
        it and publishes the latest zone data to Yeti root servers.</t>

      <t>In the same while, these DMs coordinate their work so that
        the resulting Yeti root zone is always consistent.  There
        are two aspects of coordination between three DMs: timing
        and information synchronization.</t>

      <section title="Yeti root zone SOA SERIAL">

        <t>Consistency with IANA root zone except the apex record
          is one of most important point for the project. As part
          of Yeti DM design, the Yeti SOA SERIAL which reflects the
          changes of yeti root zone is one factor to be considered.</t>

        <t>Currently IANA SOA SERIAL number for root zone is in the
          form of YYYYMMDDNN, like 2015111801. In Yeti root system,
          IANA SOA SERIAL is directly copied in to Yeti SOA SERIAL.
          So once the IANA root zone has changed with a new SOA
          SERIAL, a new version of the Yeti root zone is generated
          with the same SOA SERIAL.</t>

        <t>There is a case of Yeti DM operation that when a new
          Yeti root server added, DM operators change the Yeti root
          zone without change the SOA SERIAL which introduces
          inconsistency of Yeti root system. To avoid inconsistency,
          the DMs publish changes only when new IANA SOA SERIAL is
          observed.</t>

        <t>An analysis of IANA convention shows IANA SOA SERIAL
          change twice a day (NN=00, 01). Since October 2007 the
          maximum of NN was 03 while NN=2 was observed in 13
          times.</t>
      </section>

      <section title="Timing of Root Zone Fetch">
        <t>Yeti root system operators do not receive notify messages
          when IANA root zone is updated.  So each Yeti DM checks
          the root zone serial periodically. At the time of writing,
          each Yeti DM checks to see if the IANA root zone has
          changed hourly, on the following schedule:</t>

        <texttable>
          <ttcol>DM Operator</ttcol><ttcol>Time</ttcol>
          <c>BII</c><c>hour+00</c>
          <c>WIDE</c><c>hour+20</c> 
          <c>TISF</c><c>hour+40</c> 
        </texttable>

        <t>Note that Yeti DMs can check IANA root zone more frequently
          (every minute for example). A test done by Yeti participant
          shows that the delay of IANA root zone update from the
          first IANA root server to last one is around 20 minute.
          Once a Yeti DM fetch the new root zone, it will notify
          all the Yeti root servers with a new SOA serial number.
          So normally Yeti root server will be notified in less
          than 20 minute after new IANA root zone generated. Ideally,
          if an IANA DM notifies the Yeti DMs, Yeti root zone will
          be updated in more timely manner.</t>
      </section>

      <section title="Information Synchronization">
        <t>Given three DMs operational in Yeti root system, it is
          necessary to prevent any inconsistency caused by human
          mistakes in operation.  The straight method is to share
          the same parameters to produce the Yeti root zone.  There
          parameters includes following set of files:</t>

        <t>
          <list style="symbols">
            <t>the list of Yeti root servers, including:
              <list style="symbols">
                <t>public IPv6 address and host name </t>
                <t>IPv6 addresses originating zone transfer</t>
                <t>IPv6 addresses to send DNS notify to</t>
              </list></t>
            <t>the ZSKs used to sign the root</t>
            <t>the KSK used to sign the root</t>
            <t>the SERIAL when this information is active</t>
          </list>
        </t>

        <t>The operation is simple that each DM operator synchronize
          the files with the information needed to produce the Yeti
          root zone.  When a change is desired (such as adding a
          new server or rolling the ZSK), a DM operator updates the
          local file and push to other DM.  A SOA SERIAL in the
          future is chosen for when the changes become active. </t>
      </section>
    </section>
      
    <section title="Yeti Root Servers">
      <t>In Yeti root system, authoritative servers donated and
        operated by Yeti volunteers are configured as a slave to
        the Yeti DM.  As the time of writing, there are 25 Yeti
        root servers distributed around the world, one of which use
        IDN as its name (see Yeti hint file in Appendix A). As one
        of operational research goal, all authoritative servers are
        required to work in an IPv6-only environment. In addition,
        different from the IANA root, Yeti root server only serve
        the Yeti root zone. No root-servers.org zone and .arpa zone
        are served.</t>

      <t>Since Yeti is a scientific research project, it needs to
        capture DNS traffic sent to one of the Yeti root servers
        for later analysis.  Today some servers use dnscap, which
        is a DNS-specific tool to produce pcap files.  There are
        several versions of dnscap floating around; some people use
        the VeriSign one.  Since dnscap loses packets in some cases
        (tested on a Linux kernel), some people use pcapdump.  It
        requires the patch attached to this bug report <xref
        target="pcapdump-bug-report"/></t>

      <t>System diversity is also a requirement and observed for
        current 15 Yeti root server. Here are the results of a
        survey regarding the machine, operation system and DNS
        software:</t>

      <t>
        <list style="symbols">
          <t>Machine: 20 out of 25 root server operator are using
            a VPS to provide service.</t>
          <t>OS: 6 operators use Linux (including Ubuntu, Debian,
            CentOS, ArchLinux). 5 operators use FreeBSD and 1 NetBSD.
            And other servers are unknown.</t>
          <t>DNS software: 18 our of 25 root server use BIND (varying
            from 9.9.7 to 9.10.3). 4 of them use NSD (4.10 and
            4.15). The other 2 servers use Knot (2.0.1 and 2.1.0).
            And one use Bundy (1.2.0)</t>
        </list>
      </t>
    </section>
      
    <section title="Yeti Resolvers and Experimental Traffic" >

      <t>In client side of Yeti DNS Project, there are DNS resolvers
        with IPv6 support, updated with Yeti "hints" file to use
        the Yeti root servers instead of the IANA root servers, and
        using Yeti KSK as trust anchor. The Yeti KSK rollover
        experiment is expected to change key often (typically every
        three months), it is required that resolver operator to
        configure the resolver compliant to RFC 5011 for automatic
        update. For Yeti resolver, it is also interesting to try
        some mechanism end-system resolvers to signal to a server
        about their DNSSEC key status, like <xref
        target="I-D.wessels-edns-key-tag"/> and <xref
        target="I-D.wkumari-dnsop-trust-management"/> mentioned.</t>

      <t>Participants and volunteers are expected from individual
        researchers, labs of universities, companies and institutes,
        and vendors (for example, the DNS software implementers),
        developers of CPE devices &amp; IoT devices, and middle box
        developers who can test their products and connect their
        own testbed into Yeti testbed. Resolvers donated by Yeti
        volunteers are required to be configured with Yeti hint
        file and Yeti DNSSEC KSK.  It is required that Yeti resolver
        can speak both IPv4 and IPv6, given that not all authoritative
        servers on the Internet are IPv6 capable. </t>

      <t>At the time of writing several universities and labs have
        joined us and contributed certain amount of traffic to Yeti
        testbed. To introduce desired volume of experiment traffic,
        Yeti Project adopts two alternative ways to increase the
        experimental traffic in the Yeti testbed and check the
        functionality of Yeti root system.</t>

      <t>One approach is to mirror the real DNS query to IANA root
        system by off-path method and replay it into Yeti testbed;
        this is implemented by some Yeti root server operators.
        Another approach is to use traffic generating tool such as
        RIPE Atlas probes to generate specific queries against Yeti
        servers.</t>
    </section>
  </section>

  <section title="Experiments in Yeti Testbed">
      
    <t>The main goal of Yeti DNS Project is to act as an experimental
      network. Experiments will be conducted on this network.  In
      order to make the findings that result from these experiments
      more rigorous, an experiment protocol is proposed.</t>

    <t>A Yeti experiment goes through four phases:</t>

    <t>
      <list style="symbols">
        <t>Proposal. The first step is to make a proposal. It is
          discussed and if accepted by the Yeti participants then
          it can proceed to the next phase.</t>

        <t>Lab Test. The next phase is to run a version of the
          experiment in a controlled environment. The goal is to
          check for problems such as software crashes or protocol
          errors that may cause failures on the Yeti network, before
          putting onto the experimental network.</t>

        <t>Yeti Test. The next phase actually running the experiment
          on the Yeti network.  Details of this will depend on the
          experiment. It must be coordinated with the Yeti
          participants.</t>

        <t>Report of Findings. When completed, a report of the
          findings of the experiment should be made. It need not
          be an extensive document.</t>
      </list>
    </t>

    <t>In this section, we are going to introduce some experiments
      implemented and planned in the Yeti DNS Project.</t>

    <section title="Root Naming Scheme">
      <t>In root server history, the naming scheme for individual
        root servers was not fixed.  Current IANA Root server adopt
        [a-m].root-servers.net naming scheme to represent 13 servers
        which are labeled with letter from A to M. The authoritativeness
        is achieved by hosting "root-servers.net" zone in every
        root server. One reason behind this naming scheme is that
        DNS label compression can be used to produce a smaller DNS
        response within 512 bytes. But in Yeti testbed there is a
        chance to design and test alternative naming schemes to
        solve some issues with current naming scheme.</t>

      <t>
        <list style="symbols">
          <t>Currently root-servers.net is not signed. Kaminsky-like
            attacks are still possible for the the important information
            of Root server.</t>

          <t>The dependency to a single name(i.e..net) make the
            root system fragile in extreme case that all .net servers
            are down or unreachable but the root server still alive.</t>
        </list>
      </t>

      <t>Currently, there are two naming schemes proposed in Yeti
        Project. One is to use separate and normal domains for
        root servers (Appendix A).  One consideration is to get rid
        of the dependency on the single name.  Another consideration
        is to intentionally produces larger packets for priming
        responses for less name compression efficiency. Note that
        the Yeti root has a priming response which is 1031 Bytes
        as of the writing.</t>

      <t>There is also a issue for this naming scheme in which the
        priming response may not contain all glue record for Yeti
        Root servers. It is documented as a technical findings <xref
        target="Yeti-glue-issue"/>.  There are two approaches to
        solve the issue: one is to patch BIND 9 to includes the
        glue records in the additional section. The other one is
        to add a zone file for each root server and answer for all
        of them at each Yeti server.  That means each Yeti root
        server would have a small zone file for "bii.dns-lab.net",
        "yeti-ns.wide.ad.jp", "yeti-ns.tisf.net", and so on.</t>

      <t>Another naming scheme under Yeti lab test is to use a
        special non-delegated TLD, like .yeti-dns for root server
        operated by BII. The benefit of non-delegated TLD naming
        scheme are in two aspects: 1) the response to a priming
        query is protected by DNSSEC; 2) To meet a political reason
        that the zone authoritative for root server is not delegated
        and belong to particular companies or organizations except
        IANA; 3) reduce the dependency of root server names to other
        DNS service; 4) to mitigate some kind of cache poisoning
        activities.</t>

      <t>The obvious concern of this naming scheme is the size of
        the signed response with RRSIG for each root server and
        optionally DNSKEY RRs. There is a Lab test result regarding
        the different size of priming response in Octet : 1) with
        no additional data, with RRISG in additional section , with
        DNSKEY+RRSIG in additional section (7 keys in MZSK experiment.
        MZSK is to be described in section 4.2)</t>

      <texttable>
        <ttcol>No additional data</ttcol>
        <ttcol>RRSIG</ttcol>
        <ttcol>RRISG +DNSKEY</ttcol>

        <c>753</c><c>3296</c><c>4004</c>
      </texttable>

      <t>We found that modification of IANA root zone by adding a
        new TLD is so controversial even for scientific purpose.
        There are non-trivial discussions on this issue in Yeti
        discuss mailing list, regarding the proposal .yeti-dns for
        root name or .local for new AS112 <xref
        target="I-D.bortzmeyer-dname-root"/>.  It is argued that
        this kind of experiment should based on community consensus
        from technical bodies like IETF and be operated within a
        limited duration in some cases.</t>

      <t>Note that a document named "Technical Analysis of the
        Naming Scheme used for Individual Root Servers" is being
        developed in RSSAC Caucus. And it will be published soon</t>
    </section>

    <section title="Multiple-Signers with Multi-ZSK">
      <t>According to the Problem statement of Yeti DNS Project,
        more independent participants and operators of the root
        system is desirable.  As the name implies, multi-ZSK (MZSK)
        mode introduces different ZSKs sharing a single unique KSK,
        as opposed to the IANA root system (which uses a single ZSK
        to sign the root zone).  On the condition of good availability
        and consistency on the root system, the Multi-ZSK proposal
        is designed to give each DM operator enough room to manage
        their own ZSK, by choosing different ZSK, length, duration,
        and so on; even the encryption algorithm may vary (although
        this may cause some problem with older versions of the
        Unbound resolver).</t>

      <section title="MZSK lab experiment">

        <t>In the lab test phase, we simply setup two root servers
          (A and B) and a resolver switch between them (BIND only).
          Root A and Root B use their own ZSK to sign the zone. It
          is proved that Multi-ZSK works by adding multiple ZSK to
          the root zone. As a result, the resolver will cache the
          key sets instead of a single ZSK to validate the data no
          matter it is signed by Root A or Root B. We also tested
          Unbound and the test concluded in success with more than
          10 DMs and 10 ZSKs.</t>

        <t>Although more DMs and ZSKs can be added into the test,
          adding more ZSKs to the root zone enlarges the DNS response
          size for DNSKEY queries which may be a concern given the
          limitation of DNS packet size.  Current IANA root server
          operators are inclined to keep the packets size as small
          as possible.  So the number of DM and ZSK will be parameter
          which is decided based on operation experience.  In the
          current Yeti root testbed, there are 3 DMs, each with a
          separate ZSK.</t>
      </section> <!-- title="MZSK lab experiment" -->

      <section title="MZSK Yeti experiment">
        <t>After the lab test, the MZSK experiment is being conducted
          on the Yeti platform. There are two phases:</t>

        <t>
          <list style="symbols">
            <t>Phase 1. In the first phase, we confirmed that using
              multiple ZSKs works in the wild. We insured that using
              the maximum number of ZSKs continues to work in the
              resolver side.  Here one of the DM (BII) created and
              added 5 ZSKs using the existing synchronization
              mechanism.  (If all 3 ZSKs are rolling then we have
              6 total. To get this number we add 5.)</t>

            <t>Phase 2. In the second phase, we delegated the
              management of the ZSKs so that each DM creates and
              publishes a separate ZSK. For this phase, modified
              zone generation protocol and software was used <xref
              target="Yeti-DM-Sync-MZSK"/>, which allows the DM to
              sign without access to the private parts of ZSKs
              generated by other DMs. In this phase we roll all
              three ZSKs separately.</t>
          </list>
        </t>

        <t>The MZSK experiment was finished by the end of 2016-04.
          Almost everything appears to be working. But there have
          been some findings <xref target="Experiment-MZSK-notes"/>,
          including discovering that IPv6 fragmented packets are
          not forwarded on an Ethernet bridge with netfilter
          ip6_tables loaded on one authority server, and issue with
          IXFR falling back to AXFR due to multiple signers which
          will be detailed in a separate draft describing as a
          problem statement.</t>
        </section>
      </section>

      <section title="Root Renumbering Issue and Hint File Update">
        <t>With the recent renumbering of H root Server's IP address,
          there is a discussion of ways that resolvers can update
          their hint file.  Traditional ways include using FTP
          protocol by doing a wget and using dig to double-check
          the servers' addresses manually.  Each way would depend
          on manual operation.  As a result, there are many old
          machines that have not updated their hint files.  As a
          proof, after completion of renumbering in thirteen years
          ago, there is an observation that the "Old J-Root" can
          still receive DNS query traffic <xref
          target="Renumbering-J-Root"/>.</t>

        <t>This experiment proposal aims to find an automatic way
          for hint-file updating.  The already-completed work is a
          shell script tool which provides the function that updates
          a hint-file in file system automatically with DNSSEC and
          trust anchor validation.  <xref
          target="Hintfile-Auto-Update"/>.</t>

        <t>The methodology is straightforward.  The tool first
          queries the NS list for "." domain and queries A and AAAA
          records for every name on the NS list.  It requires DNSSEC
          validation for both the NS list and the A and AAAA answers.
          After getting all the answers, the tool compares the new
          hint file with the old one.  If there is a difference,
          it renames the old one with a time-stamp and replaces the
          old one with the new one.  Otherwise the tool deletes the
          new hint file and nothing will be changed.</t>

        <t>Note that in current IANA root system the servers named
          in the root NS record are not signed. So the tool can not
          fully work in the production network.  In Yeti root system
          some of the names listed in the NS record are signed,
          which provides a test environment for such a proposal.</t>
      </section>

      <section title="DNS Fragments">
        <t>In consideration of new DNS protocol and operation, there
          is always a hard limit on the DNS packet size.  Take Yeti
          for example: adding more root servers, using the Yeti
          naming scheme, rolling the KSK, and Multi-ZSK all increase
          the packet size.  The fear of large DNS packets mainly
          stem from two aspects: one is IP-fragments and the other
          is frequently falling back to TCP.</t>

        <t>Fragmentation may cause serious issues; if one of the
          fragment is lost at random, it results in the loss of
          entire packet and involve timeout.  If the fragment is
          dropped by a middle-box, the query always results in
          failure, and result in name resolution failure unless the
          resolver falls back to TCP. It is known at this moment
          that limited number of security middle-box implementations
          support IPv6 fragments.</t>

        <t>A possible solution is to split a single DNS message
          across multiple UDP datagrams.  This DNS fragments mechanism
          is documented in <xref target="I-D.muks-dns-message-fragments"/>
          as an experimental IETF draft.</t>
      </section>

      <section title="The KSK Rollover Experiment in Yeti">
        <t>The Yeti DNS Project provides a good basis to conduct a
          real-world experiment of a KSK rollover in the root zone.
          It is not a perfect analogy to the IANA root because all
          of the resolvers to the Yeti experiment are "opt-in", and
          are presumably run by administrators who are interested
          in the DNS and knowledgeable about it. Still, it can
          inform the IANA root KSK roll.</t>

        <t>The IANA root KSK has not been rolled as of the writing.
          ICANN put together a design team to analyze the problem
          and make recommendations.  The design team put together
          a plan<xref target="ICANN-ROOT-ROLL"/>.  The Yeti DNS
          Project may evaluate this scenario for an experimental
          KSK roll.  The experiment may not be identical, since the
          time-lines laid out in the current IANA plan are very
          long, and the Yeti DNS Project would like to conduct the
          experiment in a shorter time, which may considered much
          difficult.</t>

        <t>The Yeti KSK is rolled twice in Yeti testbed as of the
          writing.  In the first trial, it made old KSK inactive
          and new key active in one week after new key created, and
          deleted the old key in another week, which was totally
          unaware the timer specified in RFC5011. Because the
          hold-down timer was not correctly set in the server side,
          some clients (like Unbound) receive SERVFAILs (like dig
          without +cd) because the new key was still in AddPend
          state when old key was inactive. The lesson from the first
          KSK trial is that both server and client should compliant
          to RFC5011.</t>

        <t>For the second KSK rollover, it waited 30 days after a new
          KSK is published in the root zone. Different from ICANN
          rollover plan, it revokes the old key once the new key
          become active.  We don't want to wait too long, so we shorten
          the time for key publish and delete in server side. As of
          the writing, only one bug <xref target="KROLL-ISSUE"/>spotted
          on one Yeti resolver (using BIND 9.10.4-p2) during the
          second Yeti KSK rollover. The resolver is configured with
          multiple views before the KSK rollover. DNSSEC failures are
          reported once we added new view for new users after rolling
          the key. By checking the manual of BIND9.10.4-P2, it is
          said that unlike trusted-keys, managed-keys may only be set
          at the top level of named.conf, not within a view. It gives
          an assumption that for each view, managed-key can not be
          set per view in BIND. But right after setting the managed-keys
          of new views, the DNSSEC validation works for this view.
          As a conclusion for this issue, we suggest currently BIND
          multiple-view operation needs extra guidance for RFC5011.
          The manage-keys should be set carefully during the KSK
          rollover for each view when the it is created.</t>

        <t>Another of the questions of KSK rollover is how can an
          authority server know the resolver is ready for RFC5011.
          Two Internet-Drafts <xref target="I-D.wessels-edns-key-tag"/>
          and <xref target="I-D.wkumari-dnsop-trust-management"/> try
          to address the problem. In addition a compliant resolver
          implementation may fail without any complain if it is not
          correctly configured. In the case of Unbound 1.5.8, the key
          is only readable for DNS users <xref
          target="auto-trust-anchor-file"/>.</t>
      </section>

      <section title="Bigger ZSK for Yeti">
        <t>Currently IANA root system uses 1024-bits ZSK which is no
          longer recommended cryptography.  VeriSign announced at
          DNS-OARC 24th workshop that the IANA root zone ZSK will be
          increased from 1024 bits to 2048 bits in 2016. However, it
          is not fully tested by the real environment.</t>

        <t>Bigger key tend to produce a larger response which requires
          IP fragmentation and is commonly considered harm for DNS
          system. In Yeti DNS Project, it is desirable to test bigger
          responses in many aspects. The Big ZSK experiment is designed
          to test operating the Yeti root with a 2048-bit ZSK. The
          traffic is monitored before and after we lengthen the ZSK
          to see if there are any changes, such as a drop off of
          packets or a increase in retries. The current status of
          this experiment is under monitoring data analysis. </t>
      </section>
    </section>

    <section title="Other Technical findings and bugs">
      <t>Besides the experiments with specific goals and procedures,
        some unexpected bugs have been reported.  It is worthwhile
        to record them as technical findings from Yeti DNS Project.</t>

      <section title="IPv6 fragments issue"> 
        <t>There are two cases in Yeti testbed reported that some
          Yeti root servers failed to pull the zone from a Distribution
          Master via AXFR/IXFR. Two facts have been revealed in
          both client side and server side after trouble shooting.</t>

        <t>One fact in client side is that some operation system
          can not handle IPv6 fragments correctly and AXRF/IXFR in
          TCP fails.  The bug covers several OSs and one VM platform
          (listed below).</t>

        <texttable>
          <ttcol>OS</ttcol>
          <ttcol>VM</ttcol>

          <c>NetBSD 6.1 and 7.0RC1</c><c>VMware ESXI 5.5</c>
          <c>FreeBSD10.0 </c><c></c>
          <c>Debian 3.2</c><c></c>
        </texttable>

        <t>Another fact is from server side in which one TCP segment
          of AXRF/ IXFR is fragmented in IP layer resulting in two
          fragmented packets.  This weird behavior has been documented
          IETF draft<xref target="I-D.andrews-tcp-and-ipv6-use-minmtu"/>.
          It reports a situation that some implementations of TCP
          running over IPv6 neglect to check the IPV6_USE_MIN_MTU
          value when performing MSS negotiation and when constructing
          a TCP segment.  It will cause TCP MSS option set to 1440
          bytes, but IP layer will limit the packet less than 1280
          bytes and fragment the packet to two fragmented packets.</t>

        <t>While the latter is not a technical error, but it will
          cause the error in the former fact which deserves much
          attention in IPv6 operation.</t>
      </section>

      <section title="Root name compression issue"> 
        <t><xref target="RFC1035"/>specifies DNS massage compression
          scheme which allows a domain name in a message to be
          represented as either: 1) a sequence of labels ending in
          a zero octet, 2) a pointer, 3) or a sequence of labels
          ending with a pointer.  It is designed to save more room
          of DNS packet.</t>

        <t>However in Yeti testbed, it is found that Knot 2.0 server
          compresses even the root.  It means in a DNS message the
          name of root (a zero octet) is replaced by a pointer of
          2 octets.  As well, it is legal but breaks some tools (Go
          DNS lib in this bug report) which does not expect such
          name compression for root.  Both Knot and Go DNS lib have
          fixed that bug by now.</t>
      </section>

      <section title= "SOA update delay issue">
        <t>It is observed one server on Yeti testbed have some
          bugs on SOA update with more than 10 hours delay. It
          is running on Bundy 1.2.0 on FreeBSD 10.2-RELEASE. A
          workaround is to check DM's SOA status in regular base.
          But it still need some work to find the bug in code
          path to improve the software.</t>
      </section>
    </section>

    <section title="IANA Considerations">
      <t>This document requires no action from the IANA.</t>
    </section>

    <section title="Acknowledgments">
      <t>The editors fully acknowledge that this memo is based
        on joint work and discussions of many people in the mailing
        list of the Yeti DNS Project <xref target="Yeti-DNS-Project"/>.
        Some of them actually are co-authors of this memo but
        limited by the number of co-authors listed in the headline.
        The people deserve the credit who help to construct the
        Yeti testbed and contribute to this document, so their
        effort is acknowledged here with a name list: </t>

      <t>Tomohiro Ishihara, Antonio Prado, Stephane Bortzmeyer,
        Mickael Jouanne, Pierre Beyssac, Joao Damas, Pavel
        Khramtsov, Ma Yan, Otmar Lendl, Praveen Misra, Carsten
        Strotmann, Edwin Gomez, Remi Gacogne, Guillaume de Lafond,
        Yves Bovard, Hugo Salgado-Hernández, Li Zhen, Daobiao
        Gong, Runxia Wan.</t>

      <t>Acknowledgment to all anonymous Yeti participants and
        volunteers who contribute Yeti resolvers to make the
        experimental testbed functional and workable.</t>
    </section>
  </middle>

  <back>
    <references title="References">
      &RFC1034;
      &RFC1035;
      &RFC4968;
      &RFC7626;
      &RFC5011;
      &RFC7720;
      &I-D.muks-dns-message-fragments;
      &I-D.wkumari-dnsop-trust-management;
      &I-D.andrews-tcp-and-ipv6-use-minmtu;
      &I-D.wessels-edns-key-tag;
      &I-D.bortzmeyer-dname-root;
      &I-D.ietf-dnsop-resolver-priming;

      <reference anchor="Fragmenting-IPv6"
        target="http://blog.apnic.net/2016/05/19/fragmenting-ipv6/">
        <front>
          <title>Fragmenting-IPv6</title>
          <author fullname="Geoff Huston" initials="G." surname="Huston"/>
          <date month="May" year="2016" />            
        </front>
      </reference>

      <reference anchor="Renumbering-J-Root"
        target="https://indico.dns-oarc.net/event/24/session/10/contribution/10/material/slides/0.pdf">
        <front>
          <title>Thirteen Years of “Old J-Root”</title>
          <author fullname="Duane Wessels" initials="D." surname="Wessels"/>
          <date year="2015" />            
        </front>
      </reference>

      <reference anchor="Root-Zone-Database"
        target="http://www.iana.org/domains/root/db">
        <front>
          <title>Root Zone Database</title>
          <author/>
          <date/>
        </front>
      </reference>

      <reference anchor="ICANN-ROOT-ROLL"
        target="https://www.iana.org/reports/2016/root-ksk-rollover-design-20160307.pdf">
        <front>
          <title>Root Zone KSK Rollover Plan</title>
          <author/>
          <date year="2016" /> 
        </front>
      </reference>

      <reference anchor="ROOT-FAQ"
        target="https://www.isoc.org/briefings/020/">
        <front>
          <title>DNS Root Name Server FAQ</title>
          <author fullname="Daniel Karrenberg" initials="D." surname="Karrenberg"/>
          <date year="2007" />            
        </front>
      </reference>

      <reference anchor="pcapdump-bug-report"
        target="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985">
        <front>
          <title>pcaputils: IWBN to have an option to run a program
            after file rotation in pcapdump </title>
          <author fullname="Stephane Bortzmeyer"  initials="S."
            surname="Bortzmeyer"/>
          <date year="2009"/>
        </front>
      </reference>

      <reference anchor="Yeti-DNS-Project"
        target="http://www.yeti-dns.org">
        <front>
          <title>Website of Yeti DNS Project</title>
          <author/>
          <date />
        </front>
      </reference>

      <reference anchor="Experiment-MZSK-notes"
        target="https://github.com/shane-kerr/Yeti-Project/blob/experiment-mzsk/doc/Experiment-MZSK-notes.md">
        <front>
          <title>MZSK Experiment Notes</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>

      <reference anchor="Yeti-DM-Sync-MZSK"
        target="https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Yeti-DM-Sync-MZSK.md">
        <front>
          <title>Yeti DM Synchronization for MZSK</title>
          <author/>
          <date year="2016"/> 
        </front>
      </reference>

      <reference anchor="Hintfile-Auto-Update"
        target="https://github.com/BII-Lab/Hintfile-Auto-Update">
        <front>
          <title>Hintfile Auto Update</title>
          <author/>
          <date year="2015"/> 
        </front>
      </reference>

      <reference anchor="Yeti-glue-issue"
        target="http://yeti-dns.org/resource/Yeti-glue-issue.txt">
        <front>
          <title>Yeti Glue Issue</title>
          <author/>
          <date year="2015" /> 
        </front>
      </reference>

      <reference anchor="auto-trust-anchor-file"
        target="https://www.nlnetlabs.nl/bugs-script/show_bug.cgi?id=758">
        <front>
          <title> Unbound should test that auto-* files are writable</title>
          <author/>
          <date year="2016"/> 
        </front>
      </reference>

      <reference anchor="KROLL-ISSUE"
        target="http://yeti-dns.org/yeti/blog/2016/10/26/A-DNSSEC-issue-during-Yeti-KSK-rollover.html">
        <front>
          <title>A DNSSEC issue during Yeti KSK rollover</title>
          <author/>
          <date year="2016" /> 
        </front>
      </reference>

      <reference anchor="TNO"
        target="https://www.icann.org/en/system/files/files/root-scaling-model-description-29sep09-en.pdf">
        <front>
          <title>Root Scaling Study: Description of the DNS Root Scaling
            Model</title>
          <author fullname="Bart Gijsen" initials="B." surname="Gijsen"/>
          <author fullname="Almerima Jamakovic" initials="A." 
            surname="Jamakovic"/>
          <author fullname="Frank Roijers" initials="F." surname="Roijers"/>
          <date day="29" month="September" year="2009"/>
        </front>
      </reference>

      <reference anchor="ISC-TN-2003-1"
        target="http://ftp.isc.org/isc/pubs/tn/isc-tn-2003-1.txt">
        <front>
          <title>Hierarchical Anycast for Global Service Distribution</title>
          <author fullname="Joe Abley" initials="J." surname="Abley"/>
          <date month="March" year="2003"/>
        </front>
      </reference>

      <reference anchor="RSSAC001"
        target="https://www.icann.org/en/system/files/files/rssac-001-root-service-expectations-04dec15-en.pdf">
        <front>
          <title>Service Expectations of Root Servers</title>
          <author/>
          <date day="4" month="December" year="2015"/>
        </front>
      </reference>

      <reference anchor="RRL"
        target="http://www.redbarn.org/dns/ratelimits">
        <front>
          <title>Response Rate Limiting (RRL)</title>
          <author fullname="Paul Vixie" initials="P." surname="Vixie"/>
          <author fullname="Vernon Schryver" initials="V." surname="Schryver"/>
          <date day="10" month="June" year="2012"/>
        </front>
      </reference>
    </references>

    <section anchor="appendix" title=" The Yeti root server in hint file ">
      <t>REMOVE BEFORE PUBLICATION: Currently in Yeti testbed, there
        are cases that multiple servers run by single operator,
        like VeriSgin runs A and J. It is allowed because we need
        more server to satisfy Yeti experiment requirement. The
        name of those servers share common top domain name like
        yeti.eu.org, dns-lab.net, yeti-dns.net. We intentionally
        pick five random labels (first 30 characters of SHA256([a-e]))
        to offset the effect of name compression. According to the
        Yeti policy those servers will be reclaimed if qualified
        volunteers apply to host a Yeti server.</t>

      <figure>
        <artwork> 
          <![CDATA[
.                              3600000    IN   NS       bii.dns-lab.net                         
bii.dns-lab.net                3600000    IN   AAAA     240c:f:1:22::6                          
.                              3600000    IN   NS       yeti-ns.tisf.net                        
yeti-ns.tisf.net               3600000    IN   AAAA     2001:559:8000::6                        
.                              3600000    IN   NS       yeti-ns.wide.ad.jp                      
yeti-ns.wide.ad.jp             3600000    IN   AAAA     2001:200:1d9::35                        
.                              3600000    IN   NS       yeti-ns.as59715.net                     
yeti-ns.as59715.net            3600000    IN   AAAA     2a02:cdc5:9715:0:185:5:203:53           
.                              3600000    IN   NS       dahu1.yeti.eu.org                       
dahu1.yeti.eu.org              3600000    IN   AAAA     2001:4b98:dc2:45:216:3eff:fe4b:8c5b     
.                              3600000    IN   NS       ns-yeti.bondis.org                      
ns-yeti.bondis.org             3600000    IN   AAAA     2a02:2810:0:405::250                    
.                              3600000    IN   NS       yeti-ns.ix.ru                           
yeti-ns.ix.ru                  3600000    IN   AAAA     2001:6d0:6d06::53                       
.                              3600000    IN   NS       yeti.bofh.priv.at                       
yeti.bofh.priv.at              3600000    IN   AAAA     2a01:4f8:161:6106:1::10                 
.                              3600000    IN   NS       yeti.ipv6.ernet.in                      
yeti.ipv6.ernet.in             3600000    IN   AAAA     2001:e30:1c1e:1::333                    
.                              3600000    IN   NS       yeti-dns01.dnsworkshop.org              
yeti-dns01.dnsworkshop.org     3600000    IN   AAAA     2001:1608:10:167:32e::53                
.                              3600000    IN   NS       yeti-ns.conit.co                        
yeti-ns.conit.co               3600000    IN   AAAA     2604:6600:2000:11::4854:a010            
.                              3600000    IN   NS       dahu2.yeti.eu.org                       
dahu2.yeti.eu.org              3600000    IN   AAAA     2001:67c:217c:6::2                      
.                              3600000    IN   NS       yeti.aquaray.com                        
yeti.aquaray.com               3600000    IN   AAAA     2a02:ec0:200::1                         
.                              3600000    IN   NS       yeti-ns.switch.ch                       
yeti-ns.switch.ch              3600000    IN   AAAA     2001:620:0:ff::29                       
.                              3600000    IN   NS       yeti-ns.lab.nic.cl                      
yeti-ns.lab.nic.cl             3600000    IN   AAAA     2001:1398:1:21::8001                    
.                              3600000    IN   NS       yeti-ns1.dns-lab.net                    
yeti-ns1.dns-lab.net           3600000    IN   AAAA     2001:da8:a3:a027::6                     
.                              3600000    IN   NS       yeti-ns2.dns-lab.net                    
yeti-ns2.dns-lab.net           3600000    IN   AAAA     2001:da8:268:4200::6                    
.                              3600000    IN   NS       yeti-ns3.dns-lab.net                    
yeti-ns3.dns-lab.net           3600000    IN   AAAA     2400:a980:30ff::6                       
.                              3600000    IN   NS       ca978112ca1bbdcafac231b39a23dc.yeti-dns.net
ca978112ca1bbdcafac231b39a23dc.yeti-dns.net 3600000    IN   AAAA     2c0f:f530::6                            
.                              3600000    IN   NS       3e23e8160039594a33894f6564e1b1.yeti-dns.net
3e23e8160039594a33894f6564e1b1.yeti-dns.net 3600000    IN   AAAA     2803:80:1004:63::1                      
.                              3600000    IN   NS       3f79bb7b435b05321651daefd374cd.yeti-dns.net
3f79bb7b435b05321651daefd374cd.yeti-dns.net 3600000    IN   AAAA     2401:c900:1401:3b:c::6                  
.                              3600000    IN   NS       xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c     
xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c 3600000    IN   AAAA     2001:e30:1c1e:10::333                   
.                              3600000    IN   NS       yeti1.ipv6.ernet.in                     
yeti1.ipv6.ernet.in            3600000    IN   AAAA     2001:e30:187d::333                      
.                              3600000    IN   NS       yeti-dns02.dnsworkshop.org              
yeti-dns02.dnsworkshop.org     3600000    IN   AAAA     2001:19f0:0:1133::53                    
.                              3600000    IN   NS       yeti.mind-dns.nl                        
yeti.mind-dns.nl               3600000    IN   AAAA     2a02:990:100:b01::53:0      
]]>
        </artwork>
      </figure>
    </section>

    <section title="About This Document">
      <t>This section (and sub-sections) has been included as an
        aid to reviewers of this document, and should be removed
        prior to publication.</t>

      <section title="Venue">
        <t>The authors propose that this document proceeed as an
          Independent Submission, since it documents work that,
          although relevant to the IETF, has been carried out
          externally to any IETF working group. However, a suitable
          venue for discussion of this document is the dnsop working
          group.</t>

        <t>Information about the Yeti DNS project and discussion
          relating to particular experiments described in this
          document can be found at <eref
          target="https://yeti-dns.org/"/>.</t>

        <t>This document is maintained in GitHub at
          <eref target="https://github.com/BII-Lab/yeti-testbed-experience"/>.</t>
      </section>

      <section title="Revision History">
        <section title="draft-song-yeti-testbed-experience-04">
          <t>Add Joe Abley as co-editor. Substantial editorial review.</t>
        </section>
      </section>
    </section>
  </back>
</rfc>
