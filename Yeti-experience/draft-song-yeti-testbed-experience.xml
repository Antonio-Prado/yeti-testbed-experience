<?xml version="1.0"?>

<!-- This template is for creating an Internet Draft using xml2rfc,
     which is available here: http://xml.resource.org. -->

<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
  <!-- One method to get references from the online citation libraries.
       There has to be one entity for each item to be referenced.
       An alternate method (rfc include) is described in the references. -->

  <!ENTITY RFC1034 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1034.xml">
  <!ENTITY RFC1035 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1035.xml">
  <!ENTITY RFC1996 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1996.xml">
  <!ENTITY RFC2826 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.2826.xml">
  <!ENTITY RFC2845 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.2845.xml">
  <!ENTITY RFC4968 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.4986.xml">
  <!ENTITY RFC5011 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5011.xml">
  <!ENTITY RFC5890 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5890.xml">
  <!ENTITY RFC6891 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.6891.xml">
  <!ENTITY RFC7626 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7626.xml">
  <!ENTITY RFC7720 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7720.xml">

  <!ENTITY I-D.muks-dns-message-fragments SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-muks-dns-message-fragments-00.xml">
  <!ENTITY I-D.wkumari-dnsop-trust-management SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-wkumari-dnsop-trust-management-01.xml">
  <!ENTITY I-D.wessels-edns-key-tag SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-wessels-edns-key-tag-00.xml">
  <!ENTITY I-D.andrews-tcp-and-ipv6-use-minmtu
    SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-andrews-tcp-and-ipv6-use-minmtu-04.xml">
  <!ENTITY I-D.bortzmeyer-dname-root SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-bortzmeyer-dname-root-00.xml">
  <!ENTITY I-D.ietf-dnsop-resolver-priming SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-ietf-dnsop-resolver-priming-07.xml">
]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<!-- used by XSLT processors -->
<!-- For a complete list and description of processing instructions (PIs),
     please see http://xml.resource.org/authoring/README.html. -->

<!-- Below are generally applicable Processing Instructions (PIs) that most
     I-Ds might want to use. (Here they are set differently than their
     defaults in xml2rfc v1.32) -->

<!-- give errors regarding ID-nits and DTD validation -->
<?rfc strict="yes" ?>

<!-- generate a ToC -->
<?rfc toc="yes"?>

<!-- control the table of contents (ToC) -->
<?rfc tocappendix="yes"?>

<!-- the number of levels of subsections in ToC. default: 3 -->
<?rfc tocdepth="3"?>

<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc symrefs="yes"?>

<!-- sort the reference entries alphabetically -->
<?rfc sortrefs="yes" ?>

<!-- control vertical white space
     (using these PIs as follows is recommended by the RFC Editor) -->
<!-- do not start each main section on a new page -->
<?rfc compact="yes" ?>

<!-- keep one blank line between list items -->
<?rfc subcompact="no" ?>

<!-- end of list of popular I-D processing instructions -->

<?rfc comments="no" ?>
<?rfc inline="yes" ?>

<rfc category="info" docName="draft-song-yeti-testbed-experience-04"
    ipr="trust200902">

  <front>
    <title>Yeti DNS Testbed</title>

    <author fullname="Linjian Song" initials="L." surname="Song" role="editor">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>songlinjian@gmail.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <author fullname="Dong Liu" initials="D." surname="Liu" role="editor">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>dliu@biigroup.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <author fullname="Paul Vixie" initials="P." surname="Vixie" role="editor">
      <organization>TISF</organization>
      <address>
        <postal>
        <street>11400 La Honda Road</street>
        <city>Woodside</city>
        <region>California</region>
        <code>94062</code>
        <country>US</country>
      </postal>
      <email>vixie@tisf.net</email>
      <uri>http://www.redbarn.org/</uri>
      </address>
    </author>

    <author fullname="Akira Kato" initials="" surname="Kato" role="editor">
      <organization>Keio University/WIDE Project</organization>
      <address>
        <postal>
          <street>Graduate School of Media Design, 4-1-1 Hiyoshi, Kohoku</street>
          <city>Yokohama</city>
          <region></region>
          <code>223-8526</code>
          <country>JAPAN</country>
        </postal>
        <email>kato@wide.ad.jp</email>
        <uri>http://www.kmd.keio.ac.jp/</uri>
      </address>
    </author>

    <date/>

    <!-- Meta-data Declarations -->

    <area>Internet Area</area>
    <workgroup>Internet Engineering Task Force (IETF)</workgroup>

    <!-- <keyword>dns</keyword> -->

    <abstract>
      <t>The Internet's Domain Name System (DNS) is built upon the
	foundation provided by the Root Server System -- that is,
	the critical infrastructure that serves the DNS root zone.</t>

      <t>Yeti DNS is an experimental, non-production testbed that
        aims to provide an environment where technical and operational
        experiments can safely be performed without risk to production
        infrastructure.  This testbed has been used by a broad
        community of participants to perform experiments that aim
        to inform operations and future development of the production
        DNS. Yeti DNS is an independently-coordinated project and
        is not affiliated with ICANN, IANA or any Root Server
        Operator.</t>

      <t>The Yeti DNS testbed implementation includes various novel
	and experimental components including IPv6-only transport,
	independent, autonomous Zone Signing Key management, large
	cryptographic keys and a large number of component Yeti-Root
	Servers. These differences from the Root Server System have
	operational consequences such as large responses to priming
	queries and the coordination of a large pool of independent
	operators; by deploying such a system globally but outside
	the production DNS system, the Yeti DNS project provides
	an opportunity to gain insight into those consequences
	without threatening the stability of the DNS.</t>

      <t>This document neither addresses the relevant policies under
	which the Root Server System is operated nor makes any
	proposal for changing any aspect of its implementation or
	operation. This document aims solely to document technical
        and operational findings following the deployment of a
        system which is similar but different from the Root Server
        System.</t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction">
      <t>The Domain Name System (DNS), as originally specified in
        <xref target="RFC1034"/> and <xref target="RFC1035"/>, has
        proved to be an enduring and important platform upon which
        almost every user of Internet services relies. Despite its
        longevity, extensions to the protocol, new implementations
        and refinements to DNS operations continue to emerge both
        inside and outside the IETF.</t>

      <t>The Root Server System in particular has seen technical
        innovation and development in recent years, for example in
        the form of wide-scale anycast deployment, the mitigation
        of unwanted traffic on a global scale, the widespread
        deployment of Response Rate Limiting <xref target="RRL"/>,
        the introduction of IPv6 transport, the deployment of DNSSEC,
        changes in DNSSEC key sizes and preparations to roll the
        root zone's trust anchor. Together, even the projects listed
        in this brief summary imply tremendous operational change,
        all the more impressive when considered the necessary caution
        when managing Internet critical infrastructure, and the
        context of the adjacent administrative changes involved in
        root zone management and the (relatively speaking) massive
        increase in the the number of delegations in the root zone
        itself.</t>

      <t>Aspects of the operational structure of the Root Server
        System have been described in such documents as <xref
        target="TNO2009"/>, <xref target="ISC-TN-2003-1"/>, <xref
        target="RSSAC001"/> and <xref target="RFC7720"/>. Such
        references, considered together, provide sufficient insight
        into the operations of the system as a whole that it is
        straightforward to imagine structural changes to the root
        server system's infrastructure, and to wonder what the
        operational implications of such changes might be.</t>

      <t>The Yeti DNS Project was conceived in May 2015 to provide
        a captive, non-production testbed upon which the technical
        community could propose and run experiments designed to
        answer these kinds of questions. Coordination for the project
        was provided by TISF, the WIDE Project and the
        Beijing Internet Institute. Many volunteers collaborated
        to build a distributed testbed that at the time of writing
        includes 25 Yeti root servers with 16 operators and
        handles experimental traffic from individual volunteers,
        universities, DNS vendors and distributed measurement
        networks.</t>

      <t>By design, the Yeti testbed system serves the root zone
	published by the IANA with only those structural modifications
	necessary to ensure that it is able to function usefully
	in the Yeti testbed system instead of the production Root
	Server system.  In particular, no delegation for any top-level
	zone is changed, added or removed from the IANA-published
	root zone to construct the root zone served by the Yeti
	testbed system. In this document, for clarity, we refer to
	the zone derived from the IANA-published root zone as the
	Yeti-Root zone.</t>

      <t>The Yeti DNS testbed serves a similar function to the Root
	Server System in the sense that they both serve similar
	zones (the Yeti-Root zone and the Root zone, respectively).
	However, the Yeti DNS testbed only serves clients that are
	explicitly configured to participate in the experiment,
	whereas the Root Server System serves the whole Internet.
	The known set of clients has allowed structural changes to
	be deployed in the Yeti DNS testbed whose impact on clients
	can be measured and analysed.</t>
    </section>

    <section title="Areas of Study">
     <t>Examples of topics that the Yeti DNS Testbed was built to
       address are included below, each illustrated with indicative
       questions.</t>

     <section title="Implementation of a Root Server System-like Testbed">
       <t>
         <list style="symbols">
           <t>How can a captive testbed be constructed and deployed
             on the Internet, allowing useful public participation
             without any risk of disruption of the Root Server System?</t>

	   <t>How can representative traffic be introduced into
	     such a captive testbed such that insights into the
	     impact of specific differences between the testbed and
	     the Root Server System can be observed?</t>
         </list>
       </t>
     </section>

      <section title="Yeti-Root Zone Distribution">
        <t>
          <list style="symbols">
            <t>What are the scaling properties of Yeti-Root zone
              distribution as the number of Yeti-Root servers,
              Yeti-Root server instances or intermediate distribution
              points increase?</t>
          </list>
        </t>
      </section>

      <section title="Yeti-Root Server Names and Addressing">
        <t>
          <list style="symbols">
            <t>What naming schemes other than those closely analogous
              to the use of ROOT-SERVERS.NET in the production root
              zone are practical, and what are their respective
              advantages and disadvantages?</t>

            <t>What are the risks and benefits of signing the zone that
              contains the names of the Yeti-Root servers?</t>

            <t>What automatic mechanisms might be useful to improve
              the rate at which clients of Yeti-Root servers are
              able to react to a Yeti-Root server renumbering
              event?</t>
          </list>
        </t>
      </section>

      <section title="IPv6-Only Yeti-Root Servers">
        <t>
          <list style="symbols">
            <t>Are there negative operational effects in the use of
              IPv6-only Yeti-Root servers, compared to the use of
              servers that are dual-stack?</t>

            <t>What effect does the IPv6 fragmentation model have on the
              operation of Yeti-Root servers, compared with that of IPv4?</t>
          </list>
        </t>
      </section>

      <section title="DNSSEC in the Yeti-Root Zone">
        <t>
          <list style="symbols">
            <t>Is it practical to sign the Yeti-Root zone using multiple,
              independently-operated DNSSEC signers and multiple
              corresponding ZSKs?</t>

            <t>To what extent is <xref target="RFC5011"/> supported by
              resolvers?</t>

            <t>Does the KSK Rollover plan designed and in the process
              of being implemented by ICANN work as expected on the
              Yeti testbed?</t>

            <t>What is the operational impact of using much larger RSA
              key sizes in the ZSKs used in the Yeti-Root?</t>

            <t>What are the operational consequences of choosing
              DNSSEC algorithms other than RSA to sign the Yeti-Root
              zone?</t>
          </list>
        </t>
      </section>
    </section>

    <section title="Yeti DNS Testbed Infrastructure" anchor="testbed">
      <t>The purpose of the testbed is to allow DNS queries from
	stub resolvers, mediated by recursive resolvers, to be
	delivered to Yeti-Root servers, and for corresponding
	responses generated on the Yeti-Root servers to be returned,
	as illustrated in <xref target="components"/>.</t>

      <figure anchor="components" title="High-Level Testbed Components">
        <artwork>
          <![CDATA[
    ,----------.        ,-----------.        ,------------.
    |   stub   +------> | recursive +------> | Yeti-Root  |
    | resolver | <------+ resolver  | <------+ nameserver |
    `----------'        `-----------'        `------------'
       ^                   ^                    ^
       |  appropriate      |  Yeti-Root hints;  |  Yeti-Root zone
       `- resolver         `- Yeti-Root trust   `- with DNSKEY RRSet,
          configured          anchor               signed by Yeti-KSK
]]>
        </artwork>
      </figure>

      <t>To use the Yeti DNS testbed, a stub resolver must be
        explicitly configured to use recursive resolvers that
        have themselves been configured to use the Yeti-Root
        servers.  On the resolvers, that configuration consists
        of a list of names and addresses for the Yeti-Root servers
        (often referred to as a "hints file") that replaces the
        normal Internet DNS hints. Resolvers also need to be
        configured with a DNSSEC trust anchor that corresponds
        to a KSK used in the Yeti DNS Project, in place of the
        normal trust anchor for the root zone.</t>

      <t>The need for a Yeti-specific trust anchor in the resolver
	stems from the need to make minimal changes to the root
	zone, as retrieved from the IANA, to transform it into the
	Yeti-Root that can be used in the testbed.  Those changes
	would be properly rejected by any validator using an accurate
	root zone trust anchor as bogus. Corresponding changes are
	required in the Yeti-Root hints file
        <xref target="yeti-hints"/>.</t>

      <t>The data flow from IANA to stub resolvers through the
        Yeti testbed is illustrated in <xref target="yeti-root"/>
        and are described in more detail in the sections that
        follow.</t>

      <figure anchor="yeti-root" title="Testbed Data Flow">
        <artwork>
          <![CDATA[
                               ,----------------.
                          ,-- / IANA Root Zone / ---.
                          |  `----------------'     |
                          |            |            |
                          |            |            |       Root Zone
  ,--------------.    ,---V---.    ,---V---.    ,---V---.
  | Yeti Traffic |    | BII   |    | WIDE  |    | TISF  |
  | Collection   |    |  DM   |    |  DM   |    |  DM   |
  `----+----+----'    `---+---'    `---+---'    `---+---'
       |    |       ,-----'    ,-------'            `----.
       |    |       |          |                         |  Yeti-Root
       ^    ^       |          |                         |     Zone
       |    |   ,---V---.  ,---V---.                 ,---V---.
       |    `---+ Yeti  |  | Yeti  |  . . . . . . .  | Yeti  |
       |        | Root  |  | Root  |                 | Root  |
       |        `---+---'  `---+---'                 `---+---'
       |            |          |                         |    DNS
       |            |          |                         |  Response
       |         ,--V----------V-------------------------V--.
       `---------+              Yeti Resolvers              |
                 `--------------------+---------------------'
                                      |                       DNS
                                      |                     Response
                 ,--------------------V---------------------.
                 |            Yeti Stub Resolvers           |
                 `------------------------------------------'
]]>
        </artwork>
      </figure>

      <section title="Root Zone Retrieval">
        <t>Since Yeti DNS servers cannot receive DNS NOTIFY
          <xref target="RFC1996"/> messages from the Root Server
	  System, a polling approach is used. Each Yeti Distribution
	  Master (DM) requests the root zone SOA record from a
	  nameserver that permits unauthenticated zone transfers
	  of the root zone, and performs a zone transfer from that
	  server if the retrieved value of SOA.SERIAL is greater
	  than that of the last retrieved zone.</t>

	 <t>At the time of writing, unauthenticated zone transfers
	  of the root zone are available directly from B-Root,
	  C-Root, F-Root, G-Root and K-Root, and from L-Root via
	  the two servers XFR.CJR.DNS.ICANN.ORG and XFR.LAX.DNS.ICANN.ORG,
	  as well as via FTP from sites maintained by the Root Zone
	  Maintainer and the IANA Functions Operator. The Yeti DNS
	  Testbed retrieves the root zone from using zone transfers
	  from F-Root. The schedule on which F-Root is polled by
	  each Yeti DM is as follows:</t>

        <texttable>
          <ttcol>DM Operator</ttcol><ttcol>Time</ttcol>
          <c>BII</c><c>UTC hour + 00 minutes</c>
          <c>WIDE</c><c>UTC hour + 20 minutes</c>
          <c>TISF</c><c>UTC hour + 40 minutes</c>
        </texttable>

        <t>The Yeti DNS testbed uses multiple DMs, each of which acts
	  autonomously and equivalently to its siblings. Any single
	  DM can act to distribute new revisions of the Yeti-Root
	  zone, and is also responsible for signing the RRSets that
	  are changed as part of the transformation of the Root
	  Zone into the Yeti-Root zone described in <xref
	  target="transformation"/>. This shared control over the
	  processing and distribution of the Yeti-Root zone
	  approximates some of the ideas around shared zone control
	  explored in <xref target="ITI2014"/>.</t>
      </section>

      <section title="Transformation of Root Zone to Yeti-Root Zone"
        anchor="transformation">
	<t>Two distinct approaches have been deployed in the Yeti-DNS
	  Testbed, separately, to transform the Root Zone into the
	  Yeti-Root Zone. At a high level both approaches are
	  equivalent in the sense that they replace a minimal set
	  of information in the Root Zone with corresponding data
	  corresponding to the Yeti DNS Testbed; the mechanisms by
	  which the transforms are executed are different, however.
	  Each is discussed in turn in <xref target="trans1"/> and
	  <xref target="trans2"/>, respectively.</t>

        <t>A third approach has also been proposed, but not yet
          implemented. The motivations and changes implied by that
          approach are also described in <xref target="trans3"/>.</t>

        <section title="ZSK and KSK Key Sets Shared Between DMs"
          anchor="trans1">
	  <t>The approach described here was the first to be
	    implemented. It features entirely autonomous operation
	    of each DM, but also requires secret key material (the
	    private parts of all Yeti-Root KSK and ZSK key-pairs)
	    to be distributed and maintained on each DM in a
	    coordinated way.</t>

	  <t>The Root Zone is transformed as follows to produce the
	    Yeti-Root Zone. This transformation is carried out
	    autonomously on each Yeti DNS Project DM. Each DM carries
	    an authentic copy of the current set of Yeti KSK and
	    ZSK key pairs, synchronised between all DMs (see <xref
	    target="synch"/>).</t>

          <t>
            <list style="numbers">
              <t>SOA.MNAME is set to www.yeti-dns.org.</t>

	      <t>SOA.RNAME is set to &lt;dm-operator&gt;.yeti-dns.org.
	        where &lt;dm-operator&gt; is currently one of "wide",
	        "bii" or "tisf".</t>
  
              <t>All DNSKEY, RRSIG and NSEC records are removed.</t>
  
              <t>The apex NS RRSet is removed, with the corresponding
                root server glue RRSets.</t>

	      <t>A Yeti DNSKEY RRSet is added to the apex, comprising
	        the public parts of all Yeti KSK and ZSKs.</t>

	      <t>A Yeti NS RRSet is added to the apex that includes
	        all Yeti-Root servers.</t>

	      <t>Glue records (AAAA, since Yeti-Root servers are
	        v6-only) for all Yeti-Root servers are added.</t>

              <t>The Yeti-Root Zone is signed: the NSEC chain is
                regenerated; the Yeti KSK is used to
                sign the DNSKEY RRSet, and the DM's local ZSK to generate
                every other RRSet.</t>
            </list>
          </t>

	  <t>Note that the SOA.SERIAL value published in the Yeti-Root
	    Zone is identical to that found in the Root Zone.</t>
        </section>

        <section title="Unique ZSK per DM; No Shared KSK"
          anchor="trans2">
          <t>The approach described here was the second to be
	    implemented. Each DM is provisioned with its own,
	    dedicated ZSK key pairs that are not shared with other
	    DMs. A Yeti-Root DNSKEY RRSet is constructed and signed
	    upstream of all DMs as the union of the set of active
	    KSKs and the set of active ZSKs for every individual
	    DM. Each DM now only requires the secret part of its
	    own dedicated ZSK key pairs to be available locally,
	    and no other secret key material is shared. The high-level
            approach is illustrated in <xref target="multi-ZSK"/>.</t>

          <figure anchor="multi-ZSK" title="Unique ZSK per DM">
            <artwork>
              <![CDATA[
                         ,----------.         ,-----------.
                .--------> BII ZSK  +---------> Yeti-Root |
                | signs  `----------'  signs  `-----------'
                |
  ,-----------. |        ,----------.         ,-----------.
  | Yeti KSK  +-+--------> TISF ZSK +---------> Yeti-Root |
  `-----------' | signs  `----------'  signs  `-----------'
                |
                |        ,----------.         ,-----------.
                `--------> WIDE ZSK +---------> Yeti-Root |
                  signs  `----------'  signs  `-----------'
]]>
            </artwork>
          </figure>

          <t>The process of retrieving the Root Zone from the Root
	    Server System and replacing and signing the apex DNSKEY
	    RRSet no longer takes place on the DMs, and instead
	    takes place on a central Hidden Master. The production
	    of signed DNSKEY RRSets is analogous to the use of
	    Signed Key Responses (SKR) produced during ICANN KSK
	    key ceremonies.</t>

          <t>Each DM now retrieves source data (with pre-modified
	    and Yeti-signed DNSKEY RRset, but otherwise unchanged)
	    from the Yeti DNS Hidden Master instead of from the
	    Root Server System.</t>

	  <t>Each DM carries out a similar transformation to that
	    described in <xref target="trans1"/>, except that DMs
	    no longer need to modify or sign the DNSKEY RRSet.</t>

          <t>The Yeti-Root Zone served by any particular Yeti-Root
	    Server will include signatures generated using the ZSK
	    from the DM that served the Yeti-Root Zone to that
	    Yeti-Root Server. Signatures cached at resolvers might
	    be retrieved from any Yeti-Root Server, and hence are
	    expected to be a mixture of signatures generated by
	    different ZSKs. Since all ZSKs can be trusted through
	    the signature by the Yeti KSK over the DNSKEY RRSet,
	    which includes all ZSKs, the mixture of signatures was
	    predicted not to be a threat to reliable validation.
	    Deployment and experimentation confirms this to be the
	    case, even when individual ZSKs are rolled on different
            schedules.</t>

	  <t>A consequence of this approach is that the apex DNSKEY
	    RRSet in the Yeti-Root zone is much larger than the
	    corresponding DNSKEY RRSet in the Root Zone.</t>
	</section>

        <section title="Preserving Root Zone NSEC Chain and ZSK RRSIGs"
          anchor="trans3">
	  <t>A change to the transformation described in <xref
	    target="trans2"/> has been proposed that would preserve
	    the NSEC chain from the Root Zone and all RRSIG RRs
	    generated using the Root Zone's ZSKs. The DNSKEY RRSet
	    would continue to be modified to replace the Root Zone
	    KSKs, and the Yeti KSK would be used to generate
	    replacement signatures over the apex DNSKEY and NS
	    RRSets. Source data would continue to flow from the
	    Root Server System through the Hidden Master to the set
	    of DMs, but no DNSSEC operations would be required on
	    the DMs and the source NSEC and most RRSIGs would remain
	    intact.</t>

	  <t>This approach has been suggested in order to provide
	    cryptographically-verifiable confidence that no owner
	    name in the root zone had been changed in the process of
	    producing the Yeti-Root zone from the Root Zone, addressing
	    one of the concerns described in <xref target="controversy"/>
	    in a way that can be verified automatically.</t>
        </section>
      </section>

      <section title="Yeti-Root Zone Distribution">
	<t>Each Yeti DM is configured with a full list of Yeti-Root
	  Server addresses to send NOTIFY messages to, and to form
	  the basis for an address-based access-control list for
	  zone transfers. Authentication by address could be replaced
	  with more rigourous mechanisms (e.g. using Transaction
	  Signatures (TSIG) <xref target="RFC2845"/>); this has not
	  been done at the time of writing since the use of
	  address-based controls avoid the need for the distribution
	  of shared secrets amongst the Yeti-Root Server Operators.</t>

        <t>Individual Yeti-Root Servers are configured with a full
          set of Yeti DM addresses to which SOA and AXFR requests may
          be sent in the conventional manner.</t>
      </section>

      <section title="Synchronization of Service Metadata" anchor="synch">
        <t>Changes in the Yeti-DNS Testbed infrastructure such as
	  the addition or removal of Yeti-Root servers, renumbering
	  Yeti-Root Servers or DNSSEC key rollovers require coordinated
	  changes to take place on all DMs. The Yeti-DNS Testbed
	  is subject to more frequent changes than are observed in
	  the Root Server System and includes substantially more
	  Yeti-Root Servers than there are Root Servers, and hence
	  a manual change process in the Yeti Testbed would be more
	  likely to suffer from human error. An automated process
	  was consequently implemented.</t>

	<t>A repository of all service metadata involved in the
	  operation of each DM was implemented as a separate git
	  repository hosted at github.com, since this provided a
	  simple, transparent and familiar mechanism for participants
	  to review. Requests to change the service metadata for a
	  DM are submitted as pull requests from a fork of the
	  corresponding repository; each DM operator reviews pull
	  requests and merges them to indicate approval. Once merged,
	  changes are pulled automatically to individual DMs and
	  promoted to production.</t>
      </section>

      <section title="Yeti-Root Server Naming Scheme" anchor="naming">
	<t>The current naming scheme for Root Servers was normalized
	  to use single-character host names (A through M) under
	  the domain ROOT-SERVERS.NET, as described in <xref
	  target="RSSAC023"/>). The principal benefit of this naming
	  scheme is that DNS label compression can be used to produce
	  a priming response that will fit within 512 bytes, the
	  maximum DNS message size using UDP transport without EDNS0
	  <xref target="RFC6891"/>.</t>

        <t>Yeti-Root Servers do not use this optimisation, but rather
          use free-form nameserver names chosen by their respective
	  operators -- in other words, no attempt is made to minimise
	  the size of the priming response through the use of label
	  compression. This approach aims to challenge the need for
	  a minimally-sized priming response in a modern DNS ecosystem
	  where EDNS(0) is prevalent.</t>

        <t>Priming responses from Yeti-Root Servers do not always
	  include server addresses in the additional section, as
	  is the case with priming responses from Root Servers. In
	  particular, Yeti-Root Servers running BIND9 return an
	  empty additional section, forcing resolvers to complete
	  the priming process with a set of conventional recursive
	  lookups in order to resolve addresses for each Yeti-Root
	  server. Yeti-Root Servers running NSD appeared to return
          a fully-populated additional section.</t>

        <t>Various approaches to normalise the composition of the
          priming response were considered, including:

          <list style="symbols">
            <t>Require use of DNS implementations that exhibit
              the desired behaviour in the priming response (e.g.
              NSD) in favour of BIND9;</t>

            <t>Modification of BIND9 (and any other server with
              similar behaviour) for use by Yeti-Root Servers;</t>

	    <t>Isolate the names of Yeti-Root Servers in one or
	      more zones that could be slaved on each Yeti-Root
	      Server, renaming servers as necessary, giving each a
	      source of authoritative data with which the authority
	      section of a priming response could be fully populated.
	      This is the approach used in the Root Server System.</t>
	  </list>
	</t>

        <t>The potential mitigation of renaming all Yeti-Root Servers
          using a scheme that would allow their names to exist in the
          balliwick of the root zone was not considered, since that
          approach implies the invention of new top-level labels not
          present in the Root Zone.</t>

	<t>Given the relative infrequency of priming queries by
	  individual resolvers and the additional complexity or
	  other compromises implied by each of those mitigations,
	  the decision was made to make no effort to ensure that
	  the composition of priming responses was identical across
	  servers. Even the empty additional sections generated by
	  Yeti-Root Servers running BIND9 seem to be sufficient for
	  all resolver software tested; resolvers simply perform a
	  new recursive lookup for each authoritative server name
	  they need to resolve.</t>
      </section>

      <section title="Yeti Root Servers">
        <t>Various volunteers have donated authoritative servers
          to act as Yeti-Root servers. At the time of writing there
          are 25 Yeti-Root servers distributed globally, one of
          which is named using an IDNA2008 <xref target="RFC5890"/>
          label, shown in the following list in punycode.</t>

        <texttable>
          <ttcol>Name</ttcol><ttcol>Operator</ttcol><ttcol>Location</ttcol>

          <c>bii.dns-lab.net</c><c>BII</c><c>CHINA</c>

          <c>yet-ns.tsif.net</c><c>TSIF</c><c>USA</c>

          <c>yeti-ns.wide.ad.jp</c><c>WIDE Project</c><c>Japan</c>

          <c>yeti-ns.as59715.net</c><c>as59715</c><c>Italy</c>

          <c>dahu1.yeti.eu.org</c><c>Dahu Group</c><c>France</c>

	  <c>ns-yeti.bondis.org</c><c>Bond Internet Systems</c><c>Spain</c>

          <c>yeti-ns.ix.ru</c><c>Russia</c><c>MSK-IX</c>

          <c>yeti.bofh.priv.at</c><c>CERT Austria</c><c>Austria</c>

          <c>yeti.ipv6.ernet.in</c><c>ERNET India</c><c>India</c>

	  <c>yeti-dns01.dnsworkshop.org</c><c>dnsworkshop
	    /informnis</c><c>Germany</c>

          <c>yeti-ns.conit.co</c><c>CONIT S.A.S </c><c>Colombia</c>

          <c>dahu2.yeti.eu.org</c><c>Dahu Group</c><c>France</c>

          <c>yeti.aquaray.com</c><c>Aqua Ray SAS</c><c>France</c>

          <c>yeti-ns.switch.ch</c><c>SWITCH</c><c>Switzerland</c>

          <c>yeti-ns.lab.nic.cl</c><c>CHILE NIC</c><c>Chile</c>

          <c>yeti-ns1.dns-lab.net</c><c>BII</c><c>China</c>

          <c>yeti-ns2.dns-lab.net</c><c>BII</c><c>China</c>

          <c>yeti-ns3.dns-lab.net</c><c>BII</c><c>China</c>

          <c>ca...a23dc.yeti-dns.net</c><c>Yeti-ZA</c><c>South Africa</c>

          <c>3f...374cd.yeti-dns.net</c><c>Yeti-AU</c><c>Australia</c>

          <c>yeti1.ipv6.ernet.in</c><c>ERNET India</c><c>India</c>

	  <c>xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c</c><c>ERNET
	    India</c><c>India</c>

	  <c>yeti-dns02.dnsworkshop.org</c><c>dnsworkshop
	    /informnis</c><c>USA</c>

	  <c>yeti.mind-dns.nl</c><c>Monshouwer Internet
	    Diensten</c><c>Netherlands</c>

          <c>yeti-ns.datev.net</c><c>DATEV</c><c>Germany</c>
        </texttable>

        <t>The current list of Yeti-Root server is made available
          to a participating resolver first using a substitute
          hints file <xref target="yeti-hints"/> and subsequently
          by the usual resolver priming process
          <xref target="I-D.ietf-dnsop-resolver-priming"/>. All
          Yeti-Root servers are IPv6-only, foreshadowing a future
          IPv6-only Internet, and hence the Yeti-Root hints file
          contains no IPv4 addresses and the Yeti-Root zone
          contains no IPv4 glue.</t>

        <t>At the time of writing, all root servers within the
	  Root Server System serve the ROOT-SERVERS.NET zone in
	  addition to the root zone, and all but one also serve the
	  ARPA zone. Yeti-Root servers serve the Yeti-Root zone
	  only.</t>

        <t>Significant software diversity exists across the set of
          Yeti-Root servers, as reported by their volunteer operators:

          <list style="symbols">
            <t>Platform: 20 of 25 Yeti-Root servers are implemented
              on a VPS rather than bare metal.</t>

	    <t>Operating System: 6 Yeti-Root servers run on
	      on Linux (Ubuntu, Debian, CentOS, and ArchLinux); 5
	      run on FreeBSD and 1 on NetBSD.</t>

	    <t>DNS software: 18 of 25 Yeti-Root servers use BIND9
	      (versions varying between 9.9.7 and 9.10.3); four use
	      NSD (4.10 and 4.15); two use Knot (2.0.1 and 2.1.0)
	      and one uses Bundy (1.2.0).</t>
	  </list>
	</t>
      </section>

      <section title="Traffic Capture and Analysis">
	<t>Query and response traffic capture is available in the
	  testbed in both Yeti resolvers and Yeti-Root servers in
	  anticipation of experiments that require packet-level
	  visibility into DNS traffic.</t>

        <t>Traffic capture is performed on Yeti-Root servers using
	  either dnscap <eref
	  target="https://www.dns-oarc.net/tools/dnscap"/> or
	  pcapdump (part of the pcaputils Debian package <eref
	  target="https://packages.debian.org/sid/pcaputils"/>,
	  with a patch to facilitate triggered file upload <eref
	  target="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985"/>.
	  PCAP-format files containing packet captures are uploaded
	  using rsync to central storage.</t>
      </section>
    </section>

    <section title="Operational Experience with Yeti DNS Testbed">
      <t>The following sections provide commentary on the operation
	of the Yeti-DNS Testbed described in <xref target="testbed"/>.
        More detailed descriptions of observed phenomena are available
        in Yeti DNS mailing list archives and on the Yeti DNS blog.</t>

	<section title="Automated Hints File Maintenance">
	  <t>Renumbering events in the Root Server System are
	    relatively rare. Although each such event is accompanied
	    by the publication of an updated hints file in standard
	    locations, the task of updating local copies of that
	    file used by DNS resolvers is manual, and the process
	    has an observably-long tail: for example, in 2015 J-Root
	    was still receiving traffic at its old address some
	    thirteen years after renumbering <xref
	    target="Wessels2015"/>.</t>

	  <t>The observed impact of these old, deployed hints file
	    is minimal, likely due to the very low frequency of
	    such renumbering events. Even the oldest of hints file
	    would still contain some accurate root server addresses
	    from which priming responses could be obtained.</t>

	  <t>By contrast, due to the experimental nature of the
	    system and the fact that it is operated mainly by
	    volunteers, Yeti-Root Servers are added, removed and
	    renumbered with much greater frequency. A tool to
	    facilitate automatic maintenance of hints files was
	    therefore created, <xref target="hintUpdate"/>.</t>

	  <t>The automated procedure followed by the hintUpdate
	    tool is as follows.

            <list style="numbers">
	      <t>Use the local resolver to obtain a response to the
	        query ./IN/NS;</t>

	      <t>Use the local resolver to obtain a set of IPv4 and
	        IPv6 addresses for each nameserver;</t>

              <t>Validate all signatures obtained from the local
                resolvers, and confirm that all data is signed;</t>

              <t>Compare the data obtained to that contained within
                the currently-active hints file; if there are differences,
                rotate the old one way and replace it with a new one.</t>
            </list>
          </t>

          <t>This tool would not function unmodified when used in the
	    Root Server System, since the names of individual Root
	    Servers (e.g. A.ROOT-SERVERS.NET) are not signed. All
	    Yeti-Root Server names are signed, however, and hence
	    this tool functions as expected in that environment.</t>
        </section>

        <section title="Fragmentation of DNS Responses">
	  <t>In the Root Server System, structural changes with the
	    potential to increase response sizes (and hence
	    fragmentation, fallback to TCP transport or both) have
	    been exercised with great care, since the impact on
	    clients has been difficult to predict or measure. The
	    Yeti DNS Testbed is experimental and has the luxury of
	    a known client base, making it far easier to make such
	    changes and measure their impact.</t>

          <t>Many of the experimental design choices described in
	    this document were expected to trigger larger responses.
	    For example, the choice of naming scheme for Yeti-Root
	    Servers described in <xref target="naming"/> defeats
	    label compression the priming response; the Yeti-Root
	    zone transformation approach described in <xref
	    target="trans2"/> greatly enlarges the apex DNSKEY
	    RRSet. An increased incidence of fragmentation was
	    therefore expected.</t>

          <t>XXX Note to reviewers: results of fragmentation and
            TCP fallback measurements to be summarised here XXX</t>

	  <t>The Yeti-DNS Testbed provides service on IPv6 only.
	    IPv6 has a fragmentation model that is different from
	    IPv4 -- in particular, fragmentation always takes place
	    on a sending end-host, and not on an intermediate router.</t>

	  <t>Fragmentation may cause serious issues; if a single
	    fragment is lost, it results in the loss of the entire
	    datagram of which the fragment was a part, and in the
	    DNS frequently triggers a timeout. It is known at this
	    moment that only a limited number of security middle-box
	    implementations support IPv6 fragments.</t>

	  <t>The consequences of fragmentation were not limited to DNS
	    using UDP transpoert. There are two cases reported where
	    some Yeti root servers failed to transfer the Yeti-Root
	    zone from a DM.  Further experimentation revealed that
	    combinations of NetBSD 6.1, NetBSD 7.0RC1, FreeBSD 10.0,
	    Debian 3.2 and VMWare ESXI 5.5 resulted in a high TCP
	    MSS value of 1440 octets being negotiated between client
	    and server despite the presence of the IPV6_USE_MIN_MTU
	    socket option, as described in <xref
	    target="I-D.andrews-tcp-and-ipv6-use-minmtu"/>.  The
	    mismatch appears to cause outbound segments greater in
	    size than 1280 octets to be dropped before sending.</t>
      </section>

      <section title="Root Label Compression in Knot">
	<t><xref target="RFC1035"/> specifies that domain names can
	  be compressed when encoded in DNS messages, being represented
	  as one of

          <list style="numbers">
            <t>a sequence of labels ending in a zero octet;</t>

            <t>a pointer; or</t>

            <t>a sequence of labels ending with a pointer.</t>
          </list>

          The purpose of this flexibility is to reduce the size of
          domain names encoded in DNS messages.</t>

        <t>It was observed that Yeti-Root Servers running knot 2.0
	  would compress the zero-length label (the root domain,
	  often represented as ".") using a pointer to an earlier
	  example. Although legal, this encoding increases the
	  encoded size of the root label from one octet to two; it
	  was also found to break some client software, in particular
	  the Go DNS library. Bug reports were filed against both
	  knot and the Go DNS library, and both were resolved in
	  subsequent releases.</t>
      </section>

<!-- deferred for 04

      <section title="The KSK Rollover Experiment in Yeti">
        <t>The Yeti DNS Project provides a good basis to conduct a
          real-world experiment of a KSK rollover in the root zone.
          It is not a perfect analogy to the IANA root because all
          of the resolvers to the Yeti experiment are "opt-in", and
          are presumably run by administrators who are interested
          in the DNS and knowledgeable about it. Still, it can
          inform the IANA root KSK roll.</t>

        <t>The IANA root KSK has not been rolled as of the writing.
          ICANN put together a design team to analyze the problem
          and make recommendations.  The design team put together
          a plan<xref target="ICANN-ROOT-ROLL"/>.  The Yeti DNS
          Project may evaluate this scenario for an experimental
          KSK roll.  The experiment may not be identical, since the
          time-lines laid out in the current IANA plan are very
          long, and the Yeti DNS Project would like to conduct the
          experiment in a shorter time, which may considered much
          difficult.</t>

        <t>The Yeti KSK is rolled twice in Yeti testbed as of the
          writing.  In the first trial, it made old KSK inactive
          and new key active in one week after new key created, and
          deleted the old key in another week, which was totally
          unaware the timer specified in RFC5011. Because the
          hold-down timer was not correctly set in the server side,
          some clients (like Unbound) receive SERVFAILs (like dig
          without +cd) because the new key was still in AddPend
          state when old key was inactive. The lesson from the first
          KSK trial is that both server and client should compliant
          to RFC5011.</t>

        <t>For the second KSK rollover, it waited 30 days after a new
          KSK is published in the root zone. Different from ICANN
          rollover plan, it revokes the old key once the new key
          become active.  We don't want to wait too long, so we shorten
          the time for key publish and delete in server side. As of
          the writing, only one bug <xref target="KROLL-ISSUE"/>spotted
          on one Yeti resolver (using BIND 9.10.4-p2) during the
          second Yeti KSK rollover. The resolver is configured with
          multiple views before the KSK rollover. DNSSEC failures are
          reported once we added new view for new users after rolling
          the key. By checking the manual of BIND9.10.4-P2, it is
          said that unlike trusted-keys, managed-keys may only be set
          at the top level of named.conf, not within a view. It gives
          an assumption that for each view, managed-key can not be
          set per view in BIND. But right after setting the managed-keys
          of new views, the DNSSEC validation works for this view.
          As a conclusion for this issue, we suggest currently BIND
          multiple-view operation needs extra guidance for RFC5011.
          The manage-keys should be set carefully during the KSK
          rollover for each view when the it is created.</t>

        <t>Another of the questions of KSK rollover is how can an
          authority server know the resolver is ready for RFC5011.
          Two Internet-Drafts <xref target="I-D.wessels-edns-key-tag"/>
          and <xref target="I-D.wkumari-dnsop-trust-management"/> try
          to address the problem. In addition a compliant resolver
          implementation may fail without any complain if it is not
          correctly configured. In the case of Unbound 1.5.8, the key
          is only readable for DNS users <xref
          target="auto-trust-anchor-file"/>.</t>
      </section>

      <section title="Bigger ZSK for Yeti">
        <t>Currently IANA root system uses 1024-bits ZSK which is no
          longer recommended cryptography.  VeriSign announced at
          DNS-OARC 24th workshop that the IANA root zone ZSK will be
          increased from 1024 bits to 2048 bits in 2016. However, it
          is not fully tested by the real environment.</t>

        <t>Bigger key tend to produce a larger response which requires
          IP fragmentation and is commonly considered harm for DNS
          system. In Yeti DNS Project, it is desirable to test bigger
          responses in many aspects. The Big ZSK experiment is designed
          to test operating the Yeti root with a 2048-bit ZSK. The
          traffic is monitored before and after we lengthen the ZSK
          to see if there are any changes, such as a drop off of
          packets or a increase in retries. The current status of
          this experiment is under monitoring data analysis. </t>
      </section>

--> 

    </section>

    <section title="IANA Considerations">
      <t>This document requests no action of the IANA.</t>
    </section>

    <section title="Acknowledgments">
      <t>The editors would like to acknowledge the contributions of
        the various and many subscribers to the Yeti DNS Project
        mailing lists, including the following people who were
        involved in the implementation and operation of the Yeti
        DNS testbed itself:</t>

      <t>
        <list style="empty">
	  <t>Tomohiro Ishihara, Antonio Prado, Stephane Bortzmeyer,
	    Mickael Jouanne, Pierre Beyssac, Joao Damas, Pavel
	    Khramtsov, Ma Yan, Otmar Lendl, Praveen Misra, Carsten
	    Strotmann, Edwin Gomez, Remi Gacogne, Guillaume de Lafond,
	    Yves Bovard, Hugo Salgado-Hernández, Li Zhen, Daobiao
	    Gong, Runxia Wan.</t>
        </list>
      </t>

      <t>The editors also acknowledge the contributions of the
	Independent Submissions Editorial Board, and of the following
	reviewers whose opinions helped improve the clarity of this
	document:</t>

      <t>
        <list style="empty">
          <t>Subramanian Moonesamy, Joe Abley.</t>
        </list>
      </t>
    </section>
  </middle>

  <back>
    <references title="References">
      &RFC1034;
      &RFC1035;
      &RFC1996;
      &RFC2826;
      &RFC2845;
      &RFC4968;
      &RFC7626;
      &RFC5011;
      &RFC5890;
      &RFC6891;
      &RFC7720;
      &I-D.muks-dns-message-fragments;
      &I-D.wkumari-dnsop-trust-management;
      &I-D.andrews-tcp-and-ipv6-use-minmtu;
      &I-D.wessels-edns-key-tag;
      &I-D.bortzmeyer-dname-root;
      &I-D.ietf-dnsop-resolver-priming;

      <reference anchor="ITI2014"
        target="https://www.icann.org/en/system/files/files/iti-report-15may14-en.pdf">
        <front>
          <title>Identifier Technology Innovation Report</title>
          <author/>
          <date day="15" month="May" year="2014"/>
        </front>
      </reference>

      <reference anchor="Fragmenting-IPv6"
        target="http://blog.apnic.net/2016/05/19/fragmenting-ipv6/">
        <front>
          <title>Fragmenting-IPv6</title>
          <author fullname="Geoff Huston" initials="G." surname="Huston"/>
          <date month="May" year="2016" />
        </front>
      </reference>

      <reference anchor="Wessels2015"
        target="https://indico.dns-oarc.net/event/24/session/10/contribution/10/material/slides/0.pdf">
        <front>
          <title>Thirteen Years of “Old J-Root”</title>
          <author fullname="Duane Wessels" initials="D." surname="Wessels"/>
          <date year="2015" />
        </front>
      </reference>

      <reference anchor="Root-Zone-Database"
        target="http://www.iana.org/domains/root/db">
        <front>
          <title>Root Zone Database</title>
          <author/>
          <date/>
        </front>
      </reference>

      <reference anchor="ICANN-ROOT-ROLL"
        target="https://www.iana.org/reports/2016/root-ksk-rollover-design-20160307.pdf">
        <front>
          <title>Root Zone KSK Rollover Plan</title>
          <author/>
          <date year="2016" />
        </front>
      </reference>

      <reference anchor="ROOT-FAQ"
        target="https://www.isoc.org/briefings/020/">
        <front>
          <title>DNS Root Name Server FAQ</title>
          <author fullname="Daniel Karrenberg" initials="D." surname="Karrenberg"/>
          <date year="2007" />
        </front>
      </reference>

      <reference anchor="pcapdump-bug-report"
        target="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985">
        <front>
          <title>pcaputils: IWBN to have an option to run a program
            after file rotation in pcapdump </title>
          <author fullname="Stephane Bortzmeyer"  initials="S."
            surname="Bortzmeyer"/>
          <date year="2009"/>
        </front>
      </reference>

      <reference anchor="Yeti-DNS-Project"
        target="http://www.yeti-dns.org">
        <front>
          <title>Website of Yeti DNS Project</title>
          <author/>
          <date />
        </front>
      </reference>

      <reference anchor="Experiment-MZSK-notes"
        target="https://github.com/shane-kerr/Yeti-Project/blob/experiment-mzsk/doc/Experiment-MZSK-notes.md">
        <front>
          <title>MZSK Experiment Notes</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>

      <reference anchor="Yeti-DM-Sync-MZSK"
        target="https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Yeti-DM-Sync-MZSK.md">
        <front>
          <title>Yeti DM Synchronization for MZSK</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>

      <reference anchor="hintUpdate"
        target="https://github.com/BII-Lab/Hintfile-Auto-Update">
        <front>
          <title>Hintfile Auto Update</title>
          <author/>
          <date year="2015"/>
        </front>
      </reference>

      <reference anchor="auto-trust-anchor-file"
        target="https://www.nlnetlabs.nl/bugs-script/show_bug.cgi?id=758">
        <front>
          <title> Unbound should test that auto-* files are writable</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>

      <reference anchor="KROLL-ISSUE"
        target="http://yeti-dns.org/yeti/blog/2016/10/26/A-DNSSEC-issue-during-Yeti-KSK-rollover.html">
        <front>
          <title>A DNSSEC issue during Yeti KSK rollover</title>
          <author/>
          <date year="2016" />
        </front>
      </reference>

      <reference anchor="TNO2009"
        target="https://www.icann.org/en/system/files/files/root-scaling-model-description-29sep09-en.pdf">
        <front>
          <title>Root Scaling Study: Description of the DNS Root Scaling
            Model</title>
          <author fullname="Bart Gijsen" initials="B." surname="Gijsen"/>
          <author fullname="Almerima Jamakovic" initials="A."
            surname="Jamakovic"/>
          <author fullname="Frank Roijers" initials="F." surname="Roijers"/>
          <date day="29" month="September" year="2009"/>
        </front>
      </reference>

      <reference anchor="ISC-TN-2003-1"
        target="http://ftp.isc.org/isc/pubs/tn/isc-tn-2003-1.txt">
        <front>
          <title>Hierarchical Anycast for Global Service Distribution</title>
          <author fullname="Joe Abley" initials="J." surname="Abley"/>
          <date month="March" year="2003"/>
        </front>
      </reference>

      <reference anchor="RSSAC001"
        target="https://www.icann.org/en/system/files/files/rssac-001-root-service-expectations-04dec15-en.pdf">
        <front>
          <title>Service Expectations of Root Servers</title>
          <author/>
          <date day="4" month="December" year="2015"/>
        </front>
      </reference>

      <reference anchor="RSSAC023"
        target="https://www.icann.org/en/system/files/files/rssac-023-04nov16-en.pdf">
        <front>
          <title>History of the Root Server System</title>
          <author/>
          <date day="4" month="November" year="2016"/>
        </front>
      </reference>

      <reference anchor="RRL"
        target="http://www.redbarn.org/dns/ratelimits">
        <front>
          <title>Response Rate Limiting (RRL)</title>
          <author fullname="Paul Vixie" initials="P." surname="Vixie"/>
          <author fullname="Vernon Schryver" initials="V." surname="Schryver"/>
          <date day="10" month="June" year="2012"/>
        </front>
      </reference>
    </references>

    <section anchor="yeti-hints" title="Yeti-Root Hints File">
      <t>The following hints file (complete and accurate at the
	time of writing) causes a DNS resolver to use the Yeti DNS
	testbed in place of the production Root Server System and
	hence participate in experiments running on the testbed.</t>

      <t>Note that some lines have been wrapped in the text that
        follows in order to fit within the production constraints of
        this document. Wrapped lines are indicated with a blackslash
        character ("\"), following common convention.</t>

      <figure>
        <artwork>
          <![CDATA[
.                     3600000  IN   NS     bii.dns-lab.net
bii.dns-lab.net       3600000  IN   AAAA   240c:f:1:22::6
.                     3600000  IN   NS     yeti-ns.tisf.net
yeti-ns.tisf.net      3600000  IN   AAAA   2001:559:8000::6
.                     3600000  IN   NS     yeti-ns.wide.ad.jp
yeti-ns.wide.ad.jp    3600000  IN   AAAA   2001:200:1d9::35
.                     3600000  IN   NS     yeti-ns.as59715.net
yeti-ns.as59715.net   3600000  IN   AAAA   \
                           2a02:cdc5:9715:0:185:5:203:53
.                     3600000  IN   NS     dahu1.yeti.eu.org
dahu1.yeti.eu.org     3600000  IN   AAAA   \
                           2001:4b98:dc2:45:216:3eff:fe4b:8c5b
.                     3600000  IN   NS     ns-yeti.bondis.org
ns-yeti.bondis.org    3600000  IN   AAAA   2a02:2810:0:405::250
.                     3600000  IN   NS     yeti-ns.ix.ru
yeti-ns.ix.ru         3600000  IN   AAAA   2001:6d0:6d06::53
.                     3600000  IN   NS     yeti.bofh.priv.at
yeti.bofh.priv.at     3600000  IN   AAAA   2a01:4f8:161:6106:1::10
.                     3600000  IN   NS     yeti.ipv6.ernet.in
yeti.ipv6.ernet.in    3600000  IN   AAAA   2001:e30:1c1e:1::333
.                     3600000  IN   NS     yeti-dns01.dnsworkshop.org
yeti-dns01.dnsworkshop.org \
                      3600000  IN   AAAA   2001:1608:10:167:32e::53
.                     3600000  IN   NS     yeti-ns.conit.co
yeti-ns.conit.co      3600000  IN   AAAA   \
                          2604:6600:2000:11::4854:a010
.                     3600000  IN   NS     dahu2.yeti.eu.org
dahu2.yeti.eu.org     3600000  IN   AAAA   2001:67c:217c:6::2
.                     3600000  IN   NS     yeti.aquaray.com
yeti.aquaray.com      3600000  IN   AAAA   2a02:ec0:200::1
.                     3600000  IN   NS     yeti-ns.switch.ch
yeti-ns.switch.ch     3600000  IN   AAAA   2001:620:0:ff::29
.                     3600000  IN   NS     yeti-ns.lab.nic.cl
yeti-ns.lab.nic.cl    3600000  IN   AAAA   2001:1398:1:21::8001
.                     3600000  IN   NS     yeti-ns1.dns-lab.net
yeti-ns1.dns-lab.net  3600000  IN   AAAA   2001:da8:a3:a027::6
.                     3600000  IN   NS     yeti-ns2.dns-lab.net
yeti-ns2.dns-lab.net  3600000  IN   AAAA   2001:da8:268:4200::6
.                     3600000  IN   NS     yeti-ns3.dns-lab.net
yeti-ns3.dns-lab.net  3600000  IN   AAAA   2400:a980:30ff::6
.                     3600000  IN   NS     \
                        ca978112ca1bbdcafac231b39a23dc.yeti-dns.net
ca978112ca1bbdcafac231b39a23dc.yeti-dns.net \
                      3600000  IN   AAAA   2c0f:f530::6
.                     3600000  IN   NS     \
                        3e23e8160039594a33894f6564e1b1.yeti-dns.net
3e23e8160039594a33894f6564e1b1.yeti-dns.net \
                      3600000  IN   AAAA   2803:80:1004:63::1
.                     3600000  IN   NS     \
                        3f79bb7b435b05321651daefd374cd.yeti-dns.net
3f79bb7b435b05321651daefd374cd.yeti-dns.net \
                      3600000  IN   AAAA   2401:c900:1401:3b:c::6
.                     3600000  IN   NS     \
                        xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c
xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c \
                      3600000  IN   AAAA   2001:e30:1c1e:10::333
.                     3600000  IN   NS     yeti1.ipv6.ernet.in
yeti1.ipv6.ernet.in   3600000  IN   AAAA   2001:e30:187d::333
.                     3600000  IN   NS     yeti-dns02.dnsworkshop.org
yeti-dns02.dnsworkshop.org \
                      3600000  IN   AAAA   2001:19f0:0:1133::53
.                     3600000  IN   NS     yeti.mind-dns.nl
yeti.mind-dns.nl      3600000  IN   AAAA   2a02:990:100:b01::53:0
]]>
        </artwork>
      </figure>
    </section>

    <section title="Controversy" anchor="controversy">
      <t>The Yeti DNS Project, its infrastructure and the various
	experiments that have been carried out using that infrastructure,
	have been described by people involved in the project in
	many public meetings at technical venues since its inception.
	The mailing lists using which the operation of the
	infrastructure has been coordinated are open to join, and
	their archives are public. The project as a whole has been
	the subject of robust public discussion.</t>

      <t>Some commentators have expressed concern that the Yeti DNS
	Project is, in effect, operating an "alternate root,"
	challenging the IAB's comments published in <xref
	target="RFC2826"/>. Other such alternate roots are considered
	to have caused end-user confusion and instability in the
	namespace of the DNS by the introduction of new top-level
	labels or the different use of top-level labels present in
	the Root Server System. The coordinators of the Yeti DNS
	Project do not consider the Yeti DNS Project to be an
	alternate root in this sense, since by design the namespace
	enabled by the Yeti-Root Zone is identical to that of the
	Root Zone.</t>

      <t>Some commentators have expressed concern that the Yeti DNS
	Project seeks to influence or subvert administrative policy
	relating to the Root Server System, in particular in the
	use of DNSSEC trust anchors not published by the IANA and
	the use of Yeti-Root Servers in regions where governments
	or other organisations have expressed interest in operating
	a Root Server. The coordinators of the Yeti-Root project
	observe that their mandate is entirely technical and has
	no ambition to influence policy directly; they do hope,
	however, that technical findings from the Yeti DNS Project
	might act as a useful resource for the wider technical
	community.</t>

      <t>Finally, some concern has been expressed about the possible
	applications of the Yeti DNS Project to the governments of
	countries where access to the Internet is subject to
	substantial centralised control, in contrast to most other
	jurisdictions where such controls are either lighter or not
	present. The coordinators of the Yeti DNS Project have taken
	care to steer all discussions and related decisions about
	the technical work of the project to public venues in the
	interests of full transparency, and encourage anybody
	concerned about the decision-making process to participate
	in those venues and review their archives directly.</t>
    </section>

    <section title="About This Document">
      <t>This section (and sub-sections) has been included as an
        aid to reviewers of this document, and should be removed
        prior to publication.</t>

      <section title="Venue">
        <t>The authors propose that this document proceeed as an
          Independent Submission, since it documents work that,
          although relevant to the IETF, has been carried out
          externally to any IETF working group. However, a suitable
          venue for discussion of this document is the dnsop working
          group.</t>

        <t>Information about the Yeti DNS project and discussion
          relating to particular experiments described in this
          document can be found at <eref
          target="https://yeti-dns.org/"/>.</t>

        <t>This document is maintained in GitHub at
          <eref target="https://github.com/BII-Lab/yeti-testbed-experience"/>.</t>
      </section>

      <section title="Revision History">
        <section title="draft-song-yeti-testbed-experience-00 through -03">
	  <t>Change history is available in the public GitHub
	    repository where this document is maintained: <eref
	    target="https://github.com/BII-Lab/yeti-testbed-experience"/>.</t>
	</section>

        <section title="draft-song-yeti-testbed-experience-04">
          <t>Substantial editorial review and rearrangement of text
	    by Joe Abley at request of BII.</t>

	  <t>Added what is intended to be a balanced assessment of
	    the controversy that has arisen around the Yeti DNS
	    Project, at the request of the Independent Submissions
	    Editorial Board.</t>

          <t>Changed the focus of the document from the description
	    of individual experiments on a Root-like testbed to the
	    construction and motivations of the testbed itself,
	    since that better describes the output of the Yeti DNS
	    Project to date. In the considered opinion of this
	    reviewer, the novel approaches taken in the construction
	    of the testbed infrastructure and the technical challenges
	    met in doing so are useful to record, and the RFC series
	    is a reasonable place to record operational experiences
	    related to core Internet infrastructure.</t>

	  <t>Note that due to draft cut-off deadlines some of the
	    technical details described in this revision of the
	    document may not exactly match operational reality;
	    however, this revision provides an indicative level of
	    detail, focus and flow which it is hoped will be helpful
	    to reviewers.</t>
	</section>
      </section>
    </section>
  </back>
</rfc>
