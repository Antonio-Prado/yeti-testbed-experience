



Internet Engineering Task Force (IETF)                      L. Song, Ed.
Internet-Draft                                               D. Liu, Ed.
Intended status: Informational                Beijing Internet Institute
Expires: May 18, 2018                                      P. Vixie, Ed.
                                                                    TISF
                                                               Kato, Ed.
                                            Keio University/WIDE Project
                                                                 S. Kerr
                                                       November 14, 2017


                            Yeti DNS Testbed
                 draft-song-yeti-testbed-experience-06

Abstract

   The Internet's Domain Name System (DNS) is built upon the foundation
   provided by the Root Server System -- that is, the critical
   infrastructure that serves the DNS root zone.

   Yeti DNS is an experimental, non-production testbed that provides an
   environment where technical and operational experiments can safely be
   performed without risk to production infrastructure.  This testbed
   has been used by a broad community of participants to perform
   experiments that aim to inform operations and future development of
   the production DNS.  Yeti DNS is an independently-coordinated project
   and is not affiliated with ICANN, IANA or any Root Server Operator.

   The Yeti DNS testbed implementation includes various novel and
   experimental components including IPv6-only transport, independent,
   autonomous Zone Signing Key management, large cryptographic keys and
   a large number of Yeti-Root Servers.  These differences from the Root
   Server System have operational consequences such as large responses
   to priming queries and the coordination of a large pool of
   independent operators; by deploying such a system globally but
   outside the production DNS system, the Yeti DNS project provides an
   opportunity to gain insight into those consequences without
   threatening the stability of the DNS.

   This document neither addresses the relevant policies under which the
   Root Server System is operated nor makes any proposal for changing
   any aspect of its implementation or operation.  This document aims
   solely to document technical and operational findings following the
   deployment of a system which is similar to but different from the
   Root Server System.






Song, et al.              Expires May 18, 2018                  [Page 1]

Internet-Draft              Yeti DNS Testbed               November 2017


Status of This Memo

   This Internet-Draft is submitted in full conformance with the
   provisions of BCP 78 and BCP 79.

   Internet-Drafts are working documents of the Internet Engineering
   Task Force (IETF).  Note that other groups may also distribute
   working documents as Internet-Drafts.  The list of current Internet-
   Drafts is at http://datatracker.ietf.org/drafts/current/.

   Internet-Drafts are draft documents valid for a maximum of six months
   and may be updated, replaced, or obsoleted by other documents at any
   time.  It is inappropriate to use Internet-Drafts as reference
   material or to cite them other than as "work in progress."

   This Internet-Draft will expire on May 18, 2018.

Copyright Notice

   Copyright (c) 2017 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   (http://trustee.ietf.org/license-info) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License.

Table of Contents

   1.  Introduction  . . . . . . . . . . . . . . . . . . . . . . . .   3
   2.  Areas of Study  . . . . . . . . . . . . . . . . . . . . . . .   5
     2.1.  Implementation of a Root Server System-like Testbed . . .   5
     2.2.  Yeti-Root Zone Distribution . . . . . . . . . . . . . . .   5
     2.3.  Yeti-Root Server Names and Addressing . . . . . . . . . .   5
     2.4.  IPv6-Only Yeti-Root Servers . . . . . . . . . . . . . . .   5
     2.5.  DNSSEC in the Yeti-Root Zone  . . . . . . . . . . . . . .   5
   3.  Yeti DNS Testbed Infrastructure . . . . . . . . . . . . . . .   6
     3.1.  Root Zone Retrieval . . . . . . . . . . . . . . . . . . .   7
     3.2.  Transformation of Root Zone to Yeti-Root Zone . . . . . .   8
       3.2.1.  ZSK and KSK Key Sets Shared Between DMs . . . . . . .   9
       3.2.2.  Unique ZSK per DM; No Shared KSK  . . . . . . . . . .   9
       3.2.3.  Preserving Root Zone NSEC Chain and ZSK RRSIGs  . . .  11
     3.3.  Yeti-Root Zone Distribution . . . . . . . . . . . . . . .  11



Song, et al.              Expires May 18, 2018                  [Page 2]

Internet-Draft              Yeti DNS Testbed               November 2017


     3.4.  Synchronisation of Service Metadata . . . . . . . . . . .  11
     3.5.  Yeti-Root Server Naming Scheme  . . . . . . . . . . . . .  12
     3.6.  Yeti-Root Servers . . . . . . . . . . . . . . . . . . . .  13
     3.7.  Traffic Capture and Analysis  . . . . . . . . . . . . . .  15
   4.  Operational Experience with Yeti DNS Testbed  . . . . . . . .  15
     4.1.  IPv6-only root operation  . . . . . . . . . . . . . . . .  15
       4.1.1.  Impact of IPv6 fragmentation  . . . . . . . . . . . .  16
       4.1.2.  How IPv6-only Root serve IPv4 users?  . . . . . . . .  17
     4.2.  Experience on Multiple Signers  . . . . . . . . . . . . .  18
       4.2.1.  IXFR fallback to AXFR . . . . . . . . . . . . . . . .  18
       4.2.2.  Latency of Root Zone update . . . . . . . . . . . . .  19
     4.3.  DNSSEC experiences in Yeti DNS  . . . . . . . . . . . . .  19
     4.4.  Experience on capturing big DNS Messages  . . . . . . . .  21
     4.5.  Automated Hints File Maintenance  . . . . . . . . . . . .  22
   5.  Conclusion  . . . . . . . . . . . . . . . . . . . . . . . . .  23
   6.  IANA Considerations . . . . . . . . . . . . . . . . . . . . .  24
   7.  Acknowledgments . . . . . . . . . . . . . . . . . . . . . . .  24
   8.  References  . . . . . . . . . . . . . . . . . . . . . . . . .  24
   Appendix A.  Yeti-Root Hints File . . . . . . . . . . . . . . . .  28
   Appendix B.  Yeti-Root Server Priming Response  . . . . . . . . .  29
   Appendix C.  Tools developed for Yeti DNS testbed . . . . . . . .  31
   Appendix D.  Controversy  . . . . . . . . . . . . . . . . . . . .  32
   Appendix E.  About This Document  . . . . . . . . . . . . . . . .  32
     E.1.  Venue . . . . . . . . . . . . . . . . . . . . . . . . . .  33
     E.2.  Revision History  . . . . . . . . . . . . . . . . . . . .  33
       E.2.1.  draft-song-yeti-testbed-experience-00 through -03 . .  33
       E.2.2.  draft-song-yeti-testbed-experience-04 . . . . . . . .  33
   Authors' Addresses  . . . . . . . . . . . . . . . . . . . . . . .  34

1.  Introduction

   The Domain Name System (DNS), as originally specified in [RFC1034]
   and [RFC1035], has proved to be an enduring and important platform
   upon which almost every end-user of the Internet relies.  Despite its
   longevity, extensions to the protocol, new implementations and
   refinements to DNS operations continue to emerge both inside and
   outside the IETF.

   The Root Server System in particular has seen technical innovation
   and development, for example in the form of wide-scale anycast
   deployment, the mitigation of unwanted traffic on a global scale, the
   widespread deployment of Response Rate Limiting [RRL], the
   introduction of IPv6 transport, the deployment of DNSSEC, changes in
   DNSSEC key sizes and preparations to roll the root zone's trust
   anchor.  Together, even the projects listed in this brief summary
   imply tremendous operational change, all the more impressive when
   considered the necessary caution when managing Internet critical
   infrastructure, and the context of the adjacent administrative



Song, et al.              Expires May 18, 2018                  [Page 3]

Internet-Draft              Yeti DNS Testbed               November 2017


   changes involved in root zone management and the (relatively
   speaking) massive increase in the the number of delegations in the
   root zone itself.

   Aspects of the operational structure of the Root Server System have
   been described in such documents as [TNO2009], [ISC-TN-2003-1],
   [RSSAC001] and [RFC7720].  Such references, considered together,
   provide sufficient insight into the operations of the system as a
   whole that it is straightforward to imagine structural changes to the
   root server system's infrastructure and to wonder what the
   operational implications of such changes might be.

   The Yeti DNS Project was conceived in May 2015 to provide a non-
   production testbed upon which the technical community could propose
   and run experiments designed to answer these kinds of questions.
   Coordination for the project was provided by TISF, the WIDE Project
   and the Beijing Internet Institute.  Many volunteers collaborated to
   build a distributed testbed that at the time of writing includes 25
   Yeti root servers with 16 operators and handles experimental traffic
   from individual volunteers, universities, DNS vendors and distributed
   measurement networks.

   By design, the Yeti testbed system serves the root zone published by
   the IANA with only those structural modifications necessary to ensure
   that it is able to function usefully in the Yeti testbed system
   instead of the production Root Server system.  In particular, no
   delegation for any top-level zone is changed, added or removed from
   the IANA-published root zone to construct the root zone served by the
   Yeti testbed system and changes in the root zone are reflected in the
   testbed in near real-time.  In this document, for clarity, we refer
   to the zone derived from the IANA-published root zone as the Yeti-
   Root zone.

   The Yeti DNS testbed serves a similar function to the Root Server
   System in the sense that they both serve similar zones (the Yeti-Root
   zone and the IANA Root zone TODO editorial: too many capitals ODOT,
   respectively).  However, the Yeti DNS testbed only serves clients
   that are explicitly configured to participate in the experiment,
   whereas the Root Server System serves the whole Internet.  Since the
   dependent end-users and systems of the Yeti DNS testbed are known and
   their operations well-coordinated with those of the Yeti project, it
   has been possible to deploy structural changes in the Yeti DNS
   testbed with effective measurement and analysis, something that is
   difficult or simply impractical in the production Root Server System.







Song, et al.              Expires May 18, 2018                  [Page 4]

Internet-Draft              Yeti DNS Testbed               November 2017


2.  Areas of Study

   Examples of topics that the Yeti DNS Testbed was built to address are
   included below, each illustrated with indicative questions.

2.1.  Implementation of a Root Server System-like Testbed

   o  How can a testbed be constructed and deployed on the Internet,
      allowing useful public participation without any risk of
      disruption of the Root Server System?

   o  How can representative traffic be introduced into such a testbed
      such that insights into the impact of specific differences between
      the testbed and the Root Server System can be observed?

2.2.  Yeti-Root Zone Distribution

   o  What are the scaling properties of Yeti-Root zone distribution as
      the number of Yeti-Root servers, Yeti-Root server instances or
      intermediate distribution points increase?

2.3.  Yeti-Root Server Names and Addressing

   o  What naming schemes other than those closely analogous to the use
      of ROOT-SERVERS.NET in the production root zone are practical, and
      what are their respective advantages and disadvantages?

   o  What are the risks and benefits of signing the zone that contains
      the names of the Yeti-Root servers?

   o  What automatic mechanisms might be useful to improve the rate at
      which clients of Yeti-Root servers are able to react to a Yeti-
      Root server renumbering event?

2.4.  IPv6-Only Yeti-Root Servers

   o  Are there negative operational effects in the use of IPv6-only
      Yeti-Root servers, compared to the use of servers that are dual-
      stack?

   o  What effect does the IPv6 fragmentation model have on the
      operation of Yeti-Root servers, compared with that of IPv4?

2.5.  DNSSEC in the Yeti-Root Zone

   o  Is it practical to sign the Yeti-Root zone using multiple,
      independently-operated DNSSEC signers and multiple corresponding
      ZSKs?



Song, et al.              Expires May 18, 2018                  [Page 5]

Internet-Draft              Yeti DNS Testbed               November 2017


   o  To what extent is [RFC5011] supported by resolvers?

   o  Does the KSK Rollover plan designed and in the process of being
      implemented by ICANN work as expected on the Yeti testbed?

   o  What is the operational impact of using much larger RSA key sizes
      in the ZSKs used in the Yeti-Root?

   o  What are the operational consequences of choosing DNSSEC
      algorithms other than RSA to sign the Yeti-Root zone?

3.  Yeti DNS Testbed Infrastructure

   The purpose of the testbed is to allow DNS queries from stub
   resolvers, mediated by recursive resolvers, to be delivered to Yeti-
   Root servers, and for corresponding responses generated on the Yeti-
   Root servers to be returned, as illustrated in Figure 1.


       ,----------.        ,-----------.        ,------------.
       |   stub   +------> | recursive +------> | Yeti-Root  |
       | resolver | <------+ resolver  | <------+ nameserver |
       `----------'        `-----------'        `------------'
          ^                   ^                    ^
          |  appropriate      |  Yeti-Root hints;  |  Yeti-Root zone
          `- resolver         `- Yeti-Root trust   `- with DNSKEY RRSet
             configured          anchor               signed by Yeti-KSK


                  Figure 1: High-Level Testbed Components

   To use the Yeti DNS testbed, a recursive resolver must be configured
   to use the Yeti-Root servers.  On the resolvers, that configuration
   consists of a list of names and addresses for the Yeti-Root servers
   (often referred to as a "hints file") that replaces the corresponding
   hints used for the production Root Server System Appendix A.
   Resolvers also need to be configured with a DNSSEC trust anchor that
   corresponds to a KSK used in the Yeti DNS Project, in place of the
   normal trust anchor set used for the root zone.

   The need for a Yeti-specific trust anchor in the resolver stems from
   the need to make minimal changes to the root zone, as retrieved from
   the IANA, to transform it into the Yeti-Root that can be used in the
   testbed.  Corresponding changes are required in the Yeti-Root hints
   file Appendix A.  Those changes would be properly rejected by any
   validator using the production Root Server System's root zone trust
   anchor set as bogus.




Song, et al.              Expires May 18, 2018                  [Page 6]

Internet-Draft              Yeti DNS Testbed               November 2017


   The data flow from IANA to stub resolvers through the Yeti testbed is
   illustrated in Figure 2 and are described in more detail in the
   sections that follow.


                                  ,----------------.
                             ,-- / IANA Root Zone / ---.
                             |  `----------------'     |
                             |            |            |
                             |            |            |       Root Zone
     ,--------------.    ,---V---.    ,---V---.    ,---V---.
     | Yeti Traffic |    | BII   |    | WIDE  |    | TISF  |
     | Collection   |    |  DM   |    |  DM   |    |  DM   |
     `----+----+----'    `---+---'    `---+---'    `---+---'
          |    |       ,-----'    ,-------'            `----.
          |    |       |          |                         |  Yeti-Root
          ^    ^       |          |                         |     Zone
          |    |   ,---V---.  ,---V---.                 ,---V---.
          |    `---+ Yeti  |  | Yeti  |  . . . . . . .  | Yeti  |
          |        | Root  |  | Root  |                 | Root  |
          |        `---+---'  `---+---'                 `---+---'
          |            |          |                         |    DNS
          |            |          |                         |  Response
          |         ,--V----------V-------------------------V--.
          `---------+              Yeti Resolvers              |
                    `--------------------+---------------------'
                                         |                       DNS
                                         |                     Response
                    ,--------------------V---------------------.
                    |            Yeti Stub Resolvers           |
                    `------------------------------------------'


                        Figure 2: Testbed Data Flow

3.1.  Root Zone Retrieval

   The Yeti-Root Zone is distributed within the Yeti DNS testbed through
   a set of internal master servers that are referred to as Distribution
   Masters (DMs).  These server elements distribute the Yeti-Root zone
   to all Yeti-Root servers.  The means by which the Yeti DMs construct
   the Yeti-Root zone for distribution is described below.

   Since Yeti DNS DMs do not receive DNS NOTIFY [RFC1996] messages from
   the Root Server System, a polling approach is used to determine when
   new revisions of the root zone is available from the production Root
   Server System.  Each Yeti DM requests the root zone SOA record from a
   nameserver that permits unauthenticated zone transfers of the root



Song, et al.              Expires May 18, 2018                  [Page 7]

Internet-Draft              Yeti DNS Testbed               November 2017


   zone, and performs a zone transfer from that server if the retrieved
   value of SOA.SERIAL is greater than that of the last retrieved zone.

   At the time of writing, unauthenticated zone transfers of the root
   zone are available directly from B-Root, C-Root, F-Root, G-Root and
   K-Root, and from L-Root via the two servers XFR.CJR.DNS.ICANN.ORG and
   XFR.LAX.DNS.ICANN.ORG, as well as via FTP from sites maintained by
   the Root Zone Maintainer and the IANA Functions Operator.  The Yeti
   DNS Testbed retrieves the root zone from using zone transfers from
   F-Root.  The schedule on which F-Root is polled by each Yeti DM is as
   follows:

                  +-------------+-----------------------+
                  | DM Operator | Time                  |
                  +-------------+-----------------------+
                  | BII         | UTC hour + 00 minutes |
                  | WIDE        | UTC hour + 20 minutes |
                  | TISF        | UTC hour + 40 minutes |
                  +-------------+-----------------------+

   The Yeti DNS testbed uses multiple DMs, each of which acts
   autonomously and equivalently to its siblings.  Any single DM can act
   to distribute new revisions of the Yeti-Root zone, and is also
   responsible for signing the RRSets that are changed as part of the
   transformation of the Root Zone into the Yeti-Root zone described in
   Section 3.2.  This shared control over the processing and
   distribution of the Yeti-Root zone approximates some of the ideas
   around shared zone control explored in [ITI2014].

3.2.  Transformation of Root Zone to Yeti-Root Zone

   Two distinct approaches have been deployed in the Yeti-DNS Testbed,
   separately, to transform the Root Zone into the Yeti-Root Zone.  At a
   high level both approaches are equivalent in the sense that they
   replace a minimal set of information in the Root Zone with
   corresponding data corresponding to the Yeti DNS Testbed; the
   mechanisms by which the transforms are executed are different,
   however.  Each is discussed in turn in Section 3.2.1 and
   Section 3.2.2, respectively.

   A third approach has also been proposed, but not yet implemented.
   The motivations and changes implied by that approach are described in
   Section 3.2.3.








Song, et al.              Expires May 18, 2018                  [Page 8]

Internet-Draft              Yeti DNS Testbed               November 2017


3.2.1.  ZSK and KSK Key Sets Shared Between DMs

   The approach described here was the first to be implemented.  It
   features entirely autonomous operation of each DM, but also requires
   secret key material (the private key in each of the Yeti-Root KSK and
   ZSK key-pairs) to be distributed and maintained on each DM in a
   coordinated way.

   The Root Zone is transformed as follows to produce the Yeti-Root
   Zone.  This transformation is carried out autonomously on each Yeti
   DNS Project DM.  Each DM carries an authentic copy of the current set
   of Yeti KSK and ZSK key pairs, synchronised between all DMs (see
   Section 3.4).

   1.  SOA.MNAME is set to www.yeti-dns.org.

   2.  SOA.RNAME is set to <dm-operator>.yeti-dns.org.  where <dm-
       operator> is currently one of "wide", "bii" or "tisf".

   3.  All DNSKEY, RRSIG and NSEC records are removed.

   4.  The apex NS RRSet is removed, with the corresponding root server
       glue RRSets.

   5.  A Yeti DNSKEY RRSet is added to the apex, comprising the public
       parts of all Yeti KSK and ZSKs.

   6.  A Yeti NS RRSet is added to the apex that includes all Yeti-Root
       servers.

   7.  Glue records (AAAA, since Yeti-Root servers are v6-only) for all
       Yeti-Root servers are added.

   8.  The Yeti-Root Zone is signed: the NSEC chain is regenerated; the
       Yeti KSK is used to sign the DNSKEY RRSet, and the DM's local ZSK
       is used to sign every other RRSet.

   Note that the SOA.SERIAL value published in the Yeti-Root Zone is
   identical to that found in the Root Zone.

3.2.2.  Unique ZSK per DM; No Shared KSK

   The approach described here was the second to be implemented.  Each
   DM is provisioned with its own, dedicated ZSK key pairs that are not
   shared with other DMs.  A Yeti-Root DNSKEY RRSet is constructed and
   signed upstream of all DMs as the union of the set of active KSKs and
   the set of active ZSKs for every individual DM.  Each DM now only
   requires the secret part of its own dedicated ZSK key pairs to be



Song, et al.              Expires May 18, 2018                  [Page 9]

Internet-Draft              Yeti DNS Testbed               November 2017


   available locally, and no other secret key material is shared.  The
   high-level approach is illustrated in Figure 3.


                            ,----------.         ,-----------.
                   .--------> BII ZSK  +---------> Yeti-Root |
                   | signs  `----------'  signs  `-----------'
                   |
     ,-----------. |        ,----------.         ,-----------.
     | Yeti KSK  +-+--------> TISF ZSK +---------> Yeti-Root |
     `-----------' | signs  `----------'  signs  `-----------'
                   |
                   |        ,----------.         ,-----------.
                   `--------> WIDE ZSK +---------> Yeti-Root |
                     signs  `----------'  signs  `-----------'


                        Figure 3: Unique ZSK per DM

   The process of retrieving the Root Zone from the Root Server System
   and replacing and signing the apex DNSKEY RRSet no longer takes place
   on the DMs, and instead takes place on a central Hidden Master.  The
   production of signed DNSKEY RRSets is analogous to the use of Signed
   Key Responses (SKR) produced during ICANN KSK key ceremonies.

   Each DM now retrieves source data (with pre-modified and Yeti-signed
   DNSKEY RRset, but otherwise unchanged) from the Yeti DNS Hidden
   Master instead of from the Root Server System.

   Each DM carries out a similar transformation to that described in
   Section 3.2.1, except that DMs no longer need to modify or sign the
   DNSKEY RRSet.

   The Yeti-Root Zone served by any particular Yeti-Root Server will
   include signatures generated using the ZSK from the DM that served
   the Yeti-Root Zone to that Yeti-Root Server.  Signatures cached at
   resolvers might be retrieved from any Yeti-Root Server, and hence are
   expected to be a mixture of signatures generated by different ZSKs.
   Since all ZSKs can be trusted through the signature by the Yeti KSK
   over the DNSKEY RRSet, which includes all ZSKs, the mixture of
   signatures was predicted not to be a threat to reliable validation.
   Deployment and experimentation confirms this to be the case, even
   when individual ZSKs are rolled on different schedules.

   A consequence of this approach is that the apex DNSKEY RRSet in the
   Yeti-Root zone is much larger than the corresponding DNSKEY RRSet in
   the Root Zone.




Song, et al.              Expires May 18, 2018                 [Page 10]

Internet-Draft              Yeti DNS Testbed               November 2017


3.2.3.  Preserving Root Zone NSEC Chain and ZSK RRSIGs

   A change to the transformation described in Section 3.2.2 has been
   proposed that would preserve the NSEC chain from the Root Zone and
   all RRSIG RRs generated using the Root Zone's ZSKs.  The DNSKEY RRSet
   would continue to be modified to replace the Root Zone KSKs, and the
   Yeti KSK would be used to generate replacement signatures over the
   apex DNSKEY and NS RRSets.  Source data would continue to flow from
   the Root Server System through the Hidden Master to the set of DMs,
   but no DNSSEC operations would be required on the DMs and the source
   NSEC and most RRSIGs would remain intact.

   This approach has been suggested in order to provide
   cryptographically-verifiable confidence that no owner name in the
   root zone had been changed in the process of producing the Yeti-Root
   zone from the Root Zone, addressing one of the concerns described in
   Appendix D in a way that can be verified automatically.

3.3.  Yeti-Root Zone Distribution

   Each Yeti DM is configured with a full list of Yeti-Root Server
   addresses to send NOTIFY messages to, and to form the basis for an
   address-based access-control list for zone transfers.  Authentication
   by address could be replaced with more rigourous mechanisms (e.g.
   using Transaction Signatures (TSIG) [RFC2845]); this has not been
   done at the time of writing since the use of address-based controls
   avoids the need for the distribution of shared secrets amongst the
   Yeti-Root Server Operators.

   Individual Yeti-Root Servers are configured with a full set of Yeti
   DM addresses to which SOA and AXFR requests may be sent in the
   conventional manner.

3.4.  Synchronisation of Service Metadata

   Changes in the Yeti-DNS Testbed infrastructure such as the addition
   or removal of Yeti-Root servers, renumbering Yeti-Root Servers or
   DNSSEC key rollovers require coordinated changes to take place on all
   DMs.  The Yeti-DNS Testbed is subject to more frequent changes than
   are observed in the Root Server System and includes substantially
   more Yeti-Root Servers than there are IANA Root Servers, and hence a
   manual change process in the Yeti Testbed would be more likely to
   suffer from human error.  An automated process was consequently
   implemented.

   A repository of all service metadata involved in the operation of
   each DM was implemented as a separate git repository hosted at
   github.com, since this provided particpants with a simple,



Song, et al.              Expires May 18, 2018                 [Page 11]

Internet-Draft              Yeti DNS Testbed               November 2017


   transparent and familiar mechanism.  Requests to change the service
   metadata for a DM are submitted as pull requests from a fork of the
   corresponding repository; each DM operator reviews pull requests and
   merges them to indicate approval.  Once merged, changes are pulled
   automatically to individual DMs and promoted to production.

3.5.  Yeti-Root Server Naming Scheme

   The current naming scheme for Root Servers was normalized to use
   single-character host names (A through M) under the domain ROOT-
   SERVERS.NET, as described in [RSSAC023]).  The principal benefit of
   this naming scheme is that DNS label compression can be used to
   produce a priming response that would fit within 512 bytes at the
   time it was introduced, 512 bytes being the maximum DNS message size
   using UDP transport without EDNS(0) [RFC6891].

   Yeti-Root Servers do not use this optimisation, but rather use free-
   form nameserver names chosen by their respective operators -- in
   other words, no attempt is made to minimise the size of the priming
   response through the use of label compression.  This approach aims to
   challenge the need for a minimally-sized priming response in a modern
   DNS ecosystem where EDNS(0) is prevalent.

   Priming responses from Yeti-Root Servers do not always include server
   addresses in the additional section, as is the case with priming
   responses from Root Servers.  (In particular, Yeti-Root Servers
   running BIND9 return an empty additional section if the configuration
   parameter minimum-responses is set , forcing resolvers to complete
   the priming process with a set of conventional recursive lookups in
   order to resolve addresses for each Yeti-Root server.  Yeti-Root
   Servers running NSD return a fully-populated additional section.

   Various approaches to normalise the composition of the priming
   response were considered, including:

   o  Require use of DNS implementations that exhibit a desired
      behaviour in the priming response;

   o  Modification of BIND9 (and any other server with similar
      behaviour) for use by Yeti-Root Servers;

   o  Isolate the names of Yeti-Root Servers in one or more zones that
      could be slaved on each Yeti-Root Server, renaming servers as
      necessary, giving each a source of authoritative data with which
      the authority section of a priming response could be fully
      populated.  This is the approach used in the Root Server System.





Song, et al.              Expires May 18, 2018                 [Page 12]

Internet-Draft              Yeti DNS Testbed               November 2017


   The potential mitigation of renaming all Yeti-Root Servers using a
   scheme that would allow their names to exist directly in the root
   zone was not considered, since that approach implies the invention of
   new top-level labels not present in the Root Zone.

   Given the relative infrequency of priming queries by individual
   resolvers and the additional complexity or other compromises implied
   by each of those mitigations, the decision was made to make no effort
   to ensure that the composition of priming responses was identical
   across servers.  Even the empty additional sections generated by
   Yeti-Root Servers running BIND9 seem to be sufficient for all
   resolver software tested; resolvers simply perform a new recursive
   lookup for each authoritative server name they need to resolve.

3.6.  Yeti-Root Servers

   Various volunteers have donated authoritative servers to act as Yeti-
   Root servers.  At the time of writing there are 25 Yeti-Root servers
   distributed globally, one of which is named using an IDNA2008
   [RFC5890] label, shown in the following list in punycode.































Song, et al.              Expires May 18, 2018                 [Page 13]

Internet-Draft              Yeti DNS Testbed               November 2017


   +-------------------------------------+---------------+-------------+
   | Name                                | Operator      | Location    |
   +-------------------------------------+---------------+-------------+
   | bii.dns-lab.net                     | BII           | CHINA       |
   | yeti-ns.tsif.net                    | TSIF          | USA         |
   | yeti-ns.wide.ad.jp                  | WIDE Project  | Japan       |
   | yeti-ns.as59715.net                 | as59715       | Italy       |
   | dahu1.yeti.eu.org                   | Dahu Group    | France      |
   | ns-yeti.bondis.org                  | Bond Internet | Spain       |
   |                                     | Systems       |             |
   | yeti-ns.ix.ru                       | Russia        | MSK-IX      |
   | yeti.bofh.priv.at                   | CERT Austria  | Austria     |
   | yeti.ipv6.ernet.in                  | ERNET India   | India       |
   | yeti-dns01.dnsworkshop.org          | dnsworkshop   | Germany     |
   |                                     | /informnis    |             |
   | dahu2.yeti.eu.org                   | Dahu Group    | France      |
   | yeti.aquaray.com                    | Aqua Ray SAS  | France      |
   | yeti-ns.switch.ch                   | SWITCH        | Switzerland |
   | yeti-ns.lab.nic.cl                  | CHILE NIC     | Chile       |
   | yeti-ns1.dns-lab.net                | BII           | China       |
   | yeti-ns2.dns-lab.net                | BII           | China       |
   | yeti-ns3.dns-lab.net                | BII           | China       |
   | ca...a23dc.yeti-dns.net             | Yeti-ZA       | South       |
   |                                     |               | Africa      |
   | 3f...374cd.yeti-dns.net             | Yeti-AU       | Australia   |
   | yeti1.ipv6.ernet.in                 | ERNET India   | India       |
   | xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c | ERNET India   | India       |
   | yeti-dns02.dnsworkshop.org          | dnsworkshop   | USA         |
   |                                     | /informnis    |             |
   | yeti.mind-dns.nl                    | Monshouwer    | Netherlands |
   |                                     | Internet      |             |
   |                                     | Diensten      |             |
   | yeti-ns.datev.net                   | DATEV         | Germany     |
   | yeti.jhcloos.net.                   | jhcloos       | USA         |
   +-------------------------------------+---------------+-------------+

   The current list of Yeti-Root server is made available to a
   participating resolver first using a substitute hints file Appendix A
   and subsequently by the usual resolver priming process [RFC8109].
   All Yeti-Root servers are IPv6-only, foreshadowing a future IPv6-only
   Internet, and hence the Yeti-Root hints file contains no IPv4
   addresses and the Yeti-Root zone contains no IPv4 glue.

   At the time of writing, all root servers within the Root Server
   System serve the ROOT-SERVERS.NET zone in addition to the root zone,
   and all but one also serve the ARPA zone.  Yeti-Root servers serve
   the Yeti-Root zone only.




Song, et al.              Expires May 18, 2018                 [Page 14]

Internet-Draft              Yeti DNS Testbed               November 2017


   Significant software diversity exists across the set of Yeti-Root
   servers, as reported by their volunteer operators at the time of
   recording:

   o  Platform: 18 of 25 Yeti-Root servers are implemented on a VPS
      rather than bare metal.

   o  Operating System: 15 Yeti-Root servers run on Linux (Ubuntu,
      Debian, CentOS, Red Hat and ArchLinux); 4 run on FreeBSD , 1 on
      NetBSD and 1 in Windows server 2016.

   o  DNS software: 18 of 25 Yeti-Root servers use BIND9 (versions
      varying between 9.9.7 and 9.10.3); 4 use NSD (4.10 and 4.15); 2
      use Knot (2.0.1 and 2.1.0) , 1 uses Bundy (1.2.0) and 1 use MS DNS
      (10.0.14300.1000).

3.7.  Traffic Capture and Analysis

   Query and response traffic capture is available in the testbed in
   both Yeti resolvers and Yeti-Root servers in anticipation of
   experiments that require packet-level visibility into DNS traffic.

   Traffic capture is performed on Yeti-Root servers using either dnscap
   <https://www.dns-oarc.net/tools/dnscap> or pcapdump (part of the
   pcaputils Debian package <https://packages.debian.org/sid/pcaputils>,
   with a patch to facilitate triggered file upload
   <https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985>.  PCAP-
   format files containing packet captures are uploaded using rsync to
   central storage.

4.  Operational Experience with Yeti DNS Testbed

   The following sections provide commentary on the operation and impact
   analyses of the Yeti-DNS Testbed described in Section 3.  More
   detailed descriptions of observed phenomena are available in Yeti DNS
   mailing list archives and on the Yeti DNS blog.

4.1.  IPv6-only root operation

   Yeti DNS testbed was designed to explore whether it can survive in
   pure IPv6 environment or not.  So every root server required to run
   only with non-EUI64 IPv6 addressed.  There are mainly two questions
   in designers' mind when constructing this testbed: 1) is there any
   gap between IPv6-only Root and IPv4 Root to provide full function of
   root server. 2) is it possible that IPv6-only root can serve the
   Internet, even part of which still only speak IPv4.  There are some
   findings and impacts which IPv6-only property bring to Root system.




Song, et al.              Expires May 18, 2018                 [Page 15]

Internet-Draft              Yeti DNS Testbed               November 2017


4.1.1.  Impact of IPv6 fragmentation

   In the Root Server System, structural changes with the potential to
   increase response sizes (and hence fragmentation, fallback to TCP
   transport or both) have been exercised with great care, since the
   impact on clients has been difficult to predict or measure.  The Yeti
   DNS Testbed is experimental and has the luxury of a known client
   base, making it far easier to make such changes and measure their
   impact.

   Many of the experimental design choices described in this document
   were expected to trigger larger responses.  For example, the choice
   of naming scheme for Yeti-Root Servers described in Section 3.5
   defeats label compression.  It makes a large priming response (up to
   1754 octets with 25 NS server and their glue) ; the Yeti-Root zone
   transformation approach described in Section 3.2.2 greatly enlarges
   the apex DNSKEY RRSet especially during the KSK rollover (up to 1975
   octets with 3 ZSK and 2 KSK).  An increased incidence of
   fragmentation was therefore expected.

   The Yeti-DNS Testbed provides service on IPv6 only.  IPv6 has a
   fragmentation model that is different from IPv4 -- in particular,
   fragmentation always takes place on the sending host, and not on an
   intermediate router.

   Fragmentation may cause serious issues; if a single fragment is lost,
   it results in the loss of the entire datagram of which the fragment
   was a part, and in the DNS frequently triggers a timeout.  It is
   known at this moment that only a limited number of security middle-
   box implementations support IPv6 fragments.  Some public measurements
   and reports [I-D.taylor-v6ops-fragdrop] [RFC7872] shows that there is
   notable packets drop rate due to the mistreatment of middle-box on
   IPv6 fragment.  One APNIC study [IPv6-frag-DNS] reported that 37% of
   endpoints using IPv6-capable DNS resolver cannot receive a fragmented
   IPv6 response over UDP.

   To study the impact, RIPE Atlas probes are used to spot failures like
   timeout for DNSKEY queries via UDP.  For each Yeti server, a Atlas
   measurement was setup asking for 100 IPv6-enabled probes from 5
   regions, in each 2 hours sending DNS query for DNSKEY via UDP with DO
   bit set.  An monitoring report during Yeti KSK rollover shows that
   statistically large packets will trigger higher failure rate (up to
   7%) due to IPv6 fragmentation issues, which accordingly increase
   probability of retries and TCP fallback.  Even within 1500 bytes,
   when response size reaches 1414 bytes, the failure rate reaches
   around 2%.  Note that ICANN KSK rollover will produce packets
   exceeding 1414 Bytes.




Song, et al.              Expires May 18, 2018                 [Page 16]

Internet-Draft              Yeti DNS Testbed               November 2017


   Regarding the large DNS response via UDP, some existing root
   servers(A, B, G and J) truncate the response once the large IPv6
   packet surpasses 1280 octets.  In Yeti DNS Testbed, there are two
   proposals are discussed and implemented in Yeti experiments.One
   proposal is called DNS fragments [I-D.muks-dns-message-fragments]
   which is to fragment the large response in DNS level.  Another
   proposal is called DNS ATR [I-D.song-atr-large-resp] which introduces
   an simple improvement on authoritative server by replying additional
   truncated response just after the normal large response.

   The consequences of fragmentation were not limited to DNS using UDP
   transport.  There are two cases reported where some Yeti root servers
   failed to transfer the Yeti-Root zone from a DM.  When checking the
   DM log file, it is found that some root servers experienced " socket
   is not connected" errors when they pulled the zone file.  Further
   experimentation revealed that combinations of NetBSD 6.1, NetBSD
   7.0RC1, FreeBSD 10.0, Debian 3.2 and VMWare ESXI 5.5 resulted in a
   high TCP MSS value of 1440 octets being negotiated between client and
   server despite the presence of the IPV6_USE_MIN_MTU socket option, as
   described in [I-D.andrews-tcp-and-ipv6-use-minmtu].  The mismatch
   appears to cause outbound segments greater in size than 1280 octets
   to be dropped before sending.

   One proposal to handle this issue is to change the Local TCP MSS to
   be 1220 (1280-ip6/tcp header)and advise it if IPV6_USE_MINMTU=1.
   Yeti root from WIDE and SWITCH set this during the test one year ago.
   Now at the time of writing, 3 out of 25 change the MSS setting in
   Yeti DNS Testbed.

4.1.2.  How IPv6-only Root serve IPv4 users?

   Although It is straightforward to setup the IPv6-only root, but it is
   unknown if it is practical for IPv6-only root to serve the production
   networks which are still largely speak only in IPv4.  In Yeti DNS
   Testbed it is demonstrated that IPv6-only root can serve the Internet
   in a incremental approach, even for IPv4 network and users.

   It is intuitive to propose to update the resolver to dual-stack and
   configured it with hint file including IPv6 glues.  The dual-stack
   resolver connects IPv6 root with IPv4-only or dual-stack end users.
   However, when we approached some partners who agreed to try IPv6-only
   root in experimental network, they normally do not want to give up
   the IPv4 root for redundancy reason due to unstable IPv6 network
   performance.  So it is adopted in campuses that one IPv4 resolver
   address (using current IPv4 addresses of A-M root) and one IPv6
   resolver address (using Yeti root) are configured for their customer
   via DHCPv4 and DHCPv6 respectively.  The end users can choose which
   DNS they use (normally IPv6 first or using Happy eyeballs).  Ideally,



Song, et al.              Expires May 18, 2018                 [Page 17]

Internet-Draft              Yeti DNS Testbed               November 2017


   the end users DNS traffic will largely be sent to the resolver
   consuming IPv6-only root when IPv6 is widely deployed.

   For resolvers who resident in IPv4 only networks, they can forward
   the query to dual stack resolvers they have trust in.  Or they can
   configure the resolver with a hint file containing a set of IPv4
   addresses which are mapped to IPv6 addresses of root in a IPv4/IPv6
   translation devices.  The query will be routed to the translation
   devices and forward in IPv6 to IPv6-only root.  It is designed and
   going to be implemented in CERNET2 using IVI [RFC6219] technology.

4.2.  Experience on Multiple Signers

   In Section 3 it is introduced how three Distributor Masters (DM)
   works and how they share the control over the Yeti root zone.  This
   section will describe some findings and experiences on its operation.

4.2.1.  IXFR fallback to AXFR

   In DNS specifications authoritative name server uses full zone
   transfer (AXFR) [RFC5936], incremental Zone Transfer (IXFR)[RFC1995],
   and NOTIFY [RFC1996] to achieve coherency of the zone contents.  IXFR
   is an optimization for large DNS zone transfer, which allows server
   only transfer the changed portion(s) to client.  AXFR fallback
   usually happens at server side by simply returning IXFR client the
   entire new zone in condition that IXFR server cannot fulfill the
   given delta-update request.

   One experiment in Yeti is designed to test multiple signers with
   Multiple ZSKs (MZSK).  It is required that all public ZSKs used by
   DMs are included in the zone as a key set; and resolver can validate
   the message by picking one key from the key set.  From DNSSEC point
   of view, it is technically workable.  However, different signers do
   produce different RRSIG RR which introduces zone inconsistency from
   beginning in this case.  In current setting of Yeti experiment, it is
   possible that one client does AXFR/IXFR from one server and later
   asks for IXFR from another server.

   It is observed that when the IXFR client switched from one IXFR
   server to another, it received a IXFR response deleting RRSIG record
   that does not exist.  One IXFR client running NSD 4.1.7 rejected IXFR
   response, made a log indicating a bad data and then asked for full
   zone transfer.  Luckily, Yeti root zone is relatively small (691K),
   so the fallback to AXFR does not cause significant performance
   degeneration.  But if operator does host big zone with MZSK model, it
   will cause problem based on current IXFR.





Song, et al.              Expires May 18, 2018                 [Page 18]

Internet-Draft              Yeti DNS Testbed               November 2017


   Another observation is that another IXFR client running Knot 2.1.0 in
   similar situation just accepts the IXFR response, ignores the
   differences and generates a merged zone with two RRSIG RRs.  It not
   only produces larger response, but also causes DNSSEC failure when a
   new zone is generated given that old RRSIG is the signature of old
   zone RRs.

   One possible solutions is asking for development of RRSIG-aware IXFR
   format in which the RRSIG is treated as a special and RRSIG RR should
   always be transfered in full (like it does in AXFR).  Another
   solution is to detect the issue, and to fall back to AXFR
   automatically in the event of an IXFR incoherence error.  NSD does
   it.

4.2.2.  Latency of Root Zone update

   Regarding the timing of Root Zone fetch and soa update, Each Yeti DM
   checks the root zone serial hourly (in 20 minutes interval) to see if
   the IANA root zone has changed . A new version of the Yeti root zone
   is generated if the IANA root zone has changed.  In this model, root
   servers is expected pull the zone from one DM for each new update,
   because 20 min is expected to be enough for root zone publication.
   But it is not the true in Yeti testbed in a monitoring test.

   It once was reported that one server running on Bundy 1.2.0 on
   FreeBSD 10.2-RELEASE had some bugs on SOA update with more than 10
   hours delay.  Besides that server, half of Yeti servers has more than
   20 min delay, some even with 40 min delay.  One possible reason may
   be that the server failed to pull the Zone on one DM due to network
   failure(for example IPv6 fragmentation issue introduce previously)
   and turn to another DM which introduces the delay.  Another reason
   may be the NOTIFY was lost.  A last possible reason is that the root
   name server had a wrong IP address in the ACL for NOTIFY acceptation.
   It is also observed that even in the same 20-minutes time frame, not
   all servers pull from a single DM.  It is possible that some servers
   not use FCFS strategy to pull the zone after they receive the notify.
   They may pull the zone based on other metrics like the rtt , or
   manual preference.

4.3.  DNSSEC experiences in Yeti DNS

   The Root Zone KSK is expected to undergo a carefully-orchestrated
   rollover as described in [ICANN2016].  ICANN has commissioned various
   tests and has published an external test plan [ICANN2017].  After
   some studies and data analysis, ICANN announced a postponing of the
   KSK rollover at 27th September 2017 .





Song, et al.              Expires May 18, 2018                 [Page 19]

Internet-Draft              Yeti DNS Testbed               November 2017


   (DaveyNote: More background should be inserted here.  Should we
   introduce the basic concept of KSK rollover in the beginning of this
   section.  Or should we put some concept introduction in the study
   areas?)

   During the period, tests were performed on KSK rollover and the
   planned approach was also modeled in the live root Testbed.  There
   are mainly three rollover tests on Yeti testbed.

   The first Yeti KSK rollover (started at 2015-06-30) was designed to
   simulate the worst case and to see how to recover.  In RFC5011 there
   is a 30-day hold-down timer before inactivate the old key but it was
   deliberately ignored.  The rollover between the two keys was made in
   one week after new key created.  And the old key was deleted in
   another week.  It is found that some clients (Unbound) receive
   SERVFAILs because the new key was still in AddPend state when old key
   was inactive.  To recover from failure, the trusted key should be
   reset on the affected resolvers manually by an alert administrator or
   by an out-of-band automatic recovery process.

   It is proved the worth of doing with a recovery mechanism out of
   band.  The experience during ICANN KSK rollover and the final
   desiccation of postponing is due to a fact.  The fact is that even
   RFC5011 is implemented on resolvers, there are still many reasons
   causing the failure of KSK rollover.

   The second Yeti KSK rollover (started at 2016-07-12) was design for a
   simulation on the ICANN's KSK rollover.  It waited 30 days after a
   new KSK is published in the root zone.  Different from ICANN rollover
   plan, it revokes the old key once the new key become active.  We
   don't want to wait too long, so we shorten the time for key publish
   and delete in server side.

   During the second KSK rolling test one bug was [KROLL-ISSUE]spotted
   on one Yeti resolver (using BIND 9.10.4-p2).  An resolver is used to
   configured with multiple views for different subnets before the KSK
   rollover.  DNSSEC failures are reported once we added new view for
   new users after rolling the key.  It is very likely to happen for
   new-hands to operate their resolvers who may intuitively all the
   views can be inhered the managed key during KSK rollover.  In
   addition, by checking the manual of BIND9.10.4-P2, it is said that
   unlike trusted-keys, managed-keys may only be set at the top level of
   named.conf, not within a view.  It gives an assumption that for each
   view, managed-key can not be set per view in BIND.  So it may happen
   for senior engineers they may also encounter this problem.  After
   setting the managed-keys of new views, the DNSSEC validation works
   for this view.




Song, et al.              Expires May 18, 2018                 [Page 20]

Internet-Draft              Yeti DNS Testbed               November 2017


   This issue was reported in 25th DNS-OARC meeting to make it widely
   aware.  We suggest currently BIND multiple-view operation needs extra
   guidance for RFC5011.  The manage-keys should be set carefully during
   the KSK rollover for each view when the it is created.  Afterwards
   ISC published a report on this issue during KSK rollover for BIND
   users

   The third KSK rollover is started in 2017-03-02 after the ZSK is
   increased to 2048 bits.  The focus was put on the monitoring and
   impact analysis of large DNS response in Yeti testbed especially for
   DNSKEY queries.  As it is introduced in section 4.2.1, Yeti KSK
   rollover has issue when large DNS response is generated and
   transmitted via IPv6.  A gloomy atmosphere in the community is
   observed for DNS in IPv6-only environment.  In the authors' opinion
   maybe it is time to consider different transmit protocol for large
   DNS response, for example the DNSKEY query during KSK rollover.

4.4.  Experience on capturing big DNS Messages

   Many DNS operators capture DNS packets for operational purposes, such
   as gathering statistics, tracking abuse, and so on.  While in
   principle something designed for logging DNS like dnstap might be
   better, in practice capturing packets with an external tool and then
   storing them in pcap format is much more common.  These tools may be
   something DNS-specific, like dnscap, or they may be a generic tool
   like tcpdump.

   Yeti testbed captures DNS packets using dnscap to collect all packet
   information.  The problem with fragmentation for pcap is that pcap
   files usually contain the IP fragments as separate packets, which
   means that the DNS message is split up and cannot be easily parsed or
   otherwise processed.  For DNS over TCP, it will be worse that a
   single message may be split over multiple TCP packets, or a single
   TCP packet may contain multiple messages.  Since pcap is packet-
   oriented, it is difficult to get to the underlying DNS messages.

   Since very few DNS messages are large enough to require fragmentation
   or truncation, a common approach is simply to ignore these messages
   when analyzing at pcap packet captures.  However, in the Yeti testbed
   we have been using DNS configurations where our servers return large
   answers.  This means that we have a lot of fragmented IP packets and
   TCP.

   The approach that we took was to build a tool that takes a pcap input
   and: 1) Defragments any fragmented IP packets; 2) Extracts any DNS
   messages that it finds in TCP streams.  IP-format pcap was used,
   because that's what is used in Yeti testbed.  Ethernet framing
   information is not necessary, for example.  The output of this tool



Song, et al.              Expires May 18, 2018                 [Page 21]

Internet-Draft              Yeti DNS Testbed               November 2017


   is a pcap file without DNS in IP fragments or carried by TCP.  Any
   non-DNS packets are copied across unchanged (this may be useful for
   looking at ICMP unreachable messages, for example).  Any non-
   fragmented UDP DNS packets are also copied unchanged.

   Most of the "heavy lifting" of this processing is done via Google's
   gopacket library.  This allows us to read and write pcap files, and
   also provides functionality for IPv4 defragmentation as well as TCP
   reassembly.  The library did not support IPv6 defragmentation.  We
   are pushing the changes upstream, so hopefully these will be a part
   of the base library soon.

4.5.  Automated Hints File Maintenance

   Renumbering events in the Root Server System are relatively rare.
   Although each such event is accompanied by the publication of an
   updated hints file in standard locations, the task of updating local
   copies of that file used by DNS resolvers is manual, and the process
   has an observably-long tail: for example, in 2015 J-Root was still
   receiving traffic at its old address some thirteen years after
   renumbering [Wessels2015].

   The observed impact of these old, deployed hints file is minimal,
   likely due to the very low frequency of such renumbering events.
   Even the oldest of hints file would still contain some accurate root
   server addresses from which priming responses could be obtained.

   By contrast, due to the experimental nature of the system and the
   fact that it is operated mainly by volunteers, Yeti-Root Servers are
   added, removed and renumbered with much greater frequency.  A tool to
   facilitate automatic maintenance of hints files was therefore
   created, [hintUpdate].

   The automated procedure followed by the hintUpdate tool is as
   follows.

   1.  Use the local resolver to obtain a response to the query ./IN/NS;

   2.  Use the local resolver to obtain a set of IPv4 and IPv6 addresses
       for each name server;

   3.  Validate all signatures obtained from the local resolvers, and
       confirm that all data is signed;

   4.  Compare the data obtained to that contained within the currently-
       active hints file; if there are differences, rotate the old one
       away and replace it with a new one.




Song, et al.              Expires May 18, 2018                 [Page 22]

Internet-Draft              Yeti DNS Testbed               November 2017


   This tool would not function unmodified when used in the Root Server
   System, since the names of individual Root Servers (e.g.  A.ROOT-
   SERVERS.NET) are not signed.  All Yeti-Root Server names are signed,
   however, and hence this tool functions as expected in that
   environment.

5.  Conclusion

   Yeti DNS is designed as a live root testbed.  It serves the root zone
   published by the IANA with only those structural modifications
   necessary to ensure its function as a testbed system.  After 2-years
   operation, Yeti DNS consists of 3 Distribution Master (DM), 25 root
   servers run by 16 operators, and around 3 thousands resolvers who
   contributed experimental traffic to Yeti DNS.

   The topics that the Yeti testbed was built to address includes
   IPv6-only operation, DNSSEC key rollover, renumbering issues,
   scalability issues and multiple zone file signers etc.  Most of them
   are not well studied before Yeti DNS was setup.  Yeti did not
   answered all the issues, but it did give a clue from experience of
   running such testbed with structural changes to the root server
   system's infrastructure.  As a conclusion, some experiences and
   recommendations are listed below.

   (1) The first one is that IPv6-only Root is operational feasible and
   reasonable, but necessary care should be taken to handle Large
   response.  As IPv6-only Internet is the future, Yeti DNS provides a
   place to "sunset" IPv4 in root-level DNS.  From our experience the
   only requirement is that the resolver should be dual-stack.  The risk
   for root system running in pure IPv6 network is that there is no IPv4
   fallback if IPv6 DNS fails.  However, from our experience the
   resolvers can survive and go though by falling back to TCP or retries
   to other root servers.  It is not fatal but puts gloomy atmosphere
   for IPv6 transition and sunsetting IPv4 for DNS as an important
   Internet infrastructure.

   (2) The Multiple signer is a good proof-of-concept implementation for
   the idea of "Share Zone Control".  It successfully demonstrated the
   ability to add more signers(three in Yeti) to manage the root zone
   independently.  It increases publicity,operational diversity and
   redundancy for root system.  The only impact is that more ZSKs will
   be included in DNSKEY RRset which increases the size of response for
   DNSKEY query.  Anyone who want to implement multiple signer in their
   DNS system, they should take care the fragmentation issues.

   (3) Yeti DNS proved the concept of "one name space, multiple circles"
   as an answer to scaling properties of root zone distribution.  Now
   Yeti DNS runs 25 root servers by increasing the priming response.



Song, et al.              Expires May 18, 2018                 [Page 23]

Internet-Draft              Yeti DNS Testbed               November 2017


   Before additional treatment adopted to handle large DNS response
   issue, it is not wise to increase the number of root server operator
   by adding more NS server to the base 13.  On the contrary Yeti DNS
   serves a root zone which is slightly different from original one
   generated by IANA.  It is technical and operational feasible maintain
   multiple versions of root zone signed by IANA's KSK.  It will
   introduce more than one set of 13 or 25 root severs.

   (4) From Yeti experiences on renumbering issue and KSK rollover in
   Yeti, it is worth of doing out-of-bound automatic mechanisms to
   improve the rate at which clients of Yeti-Root servers are able to
   react to the events of both Root server renumbering and KSK rollover.
   For example for DNS implementations, additional threads or coroutines
   can be implemented coordinating with the main process of BIND and
   Unbound.  Resource lists like hint file and KSK should be fixed and
   available as well.

6.  IANA Considerations

   This document requests no action of the IANA.

7.  Acknowledgments

   The editors would like to acknowledge the contributions of the
   various and many subscribers to the Yeti DNS Project mailing lists,
   including the following people who were involved in the
   implementation and operation of the Yeti DNS testbed itself:

      Tomohiro Ishihara, Antonio Prado, Stephane Bortzmeyer, Mickael
      Jouanne, Pierre Beyssac, Joao Damas, Pavel Khramtsov, Ma Yan,
      Otmar Lendl, Praveen Misra, Carsten Strotmann, Edwin Gomez, Remi
      Gacogne, Guillaume de Lafond, Yves Bovard, Hugo Salgado-Hernandez,
      Li Zhen, Daobiao Gong, Runxia Wan.

   The editors also acknowledge the contributions of the Independent
   Submissions Editorial Board, and of the following reviewers whose
   opinions helped improve the clarity of this document:

      Subramanian Moonesamy, Joe Abley.

8.  References

   [hintUpdate]
              "Hintfile Auto Update", 2015, <https://github.com/BII-Lab/
              Hintfile-Auto-Update>.






Song, et al.              Expires May 18, 2018                 [Page 24]

Internet-Draft              Yeti DNS Testbed               November 2017


   [I-D.andrews-tcp-and-ipv6-use-minmtu]
              Andrews, M., "TCP Fails To Respect IPV6_USE_MIN_MTU",
              draft-andrews-tcp-and-ipv6-use-minmtu-04 (work in
              progress), October 2015.

   [I-D.muks-dns-message-fragments]
              Sivaraman, M., Kerr, S., and D. Song, "DNS message
              fragments", draft-muks-dns-message-fragments-00 (work in
              progress), July 2015.

   [I-D.song-atr-large-resp]
              Song, L., "ATR: Additional Truncated Response for Large
              DNS Response", draft-song-atr-large-resp-00 (work in
              progress), September 2017.

   [I-D.taylor-v6ops-fragdrop]
              Jaeggli, J., Colitti, L., Kumari, W., Vyncke, E., Kaeo,
              M., and T. Taylor, "Why Operators Filter Fragments and
              What It Implies", draft-taylor-v6ops-fragdrop-02 (work in
              progress), December 2013.

   [ICANN2016]
              "Root Zone KSK Rollover Plan", 2016,
              <https://www.iana.org/reports/2016/root-ksk-rollover-
              design-20160307.pdf>.

   [ICANN2017]
              "2017 KSK Rollover External Test Plan", July 2016,
              <https://www.icann.org/en/system/files/files/ksk-rollover-
              external-test-plan-22jul16-en.pdf>.

   [IPv6-frag-DNS]
              "Dealing with IPv6 fragmentation in the DNS", August 2017,
              <https://blog.apnic.net/2017/08/22/dealing-ipv6-
              fragmentation-dns>.

   [ISC-TN-2003-1]
              Abley, J., "Hierarchical Anycast for Global Service
              Distribution", March 2003,
              <http://ftp.isc.org/isc/pubs/tn/isc-tn-2003-1.txt>.

   [ITI2014]  "Identifier Technology Innovation Report", May 2014,
              <https://www.icann.org/en/system/files/files/iti-report-
              15may14-en.pdf>.







Song, et al.              Expires May 18, 2018                 [Page 25]

Internet-Draft              Yeti DNS Testbed               November 2017


   [KROLL-ISSUE]
              "A DNSSEC issue during Yeti KSK rollover", 2016,
              <http://yeti-dns.org/yeti/blog/2016/10/26/
              A-DNSSEC-issue-during-Yeti-KSK-rollover.html>.

   [RFC1034]  Mockapetris, P., "Domain names - concepts and facilities",
              STD 13, RFC 1034, DOI 10.17487/RFC1034, November 1987,
              <https://www.rfc-editor.org/info/rfc1034>.

   [RFC1035]  Mockapetris, P., "Domain names - implementation and
              specification", STD 13, RFC 1035, DOI 10.17487/RFC1035,
              November 1987, <https://www.rfc-editor.org/info/rfc1035>.

   [RFC1995]  Ohta, M., "Incremental Zone Transfer in DNS", RFC 1995,
              DOI 10.17487/RFC1995, August 1996, <https://www.rfc-
              editor.org/info/rfc1995>.

   [RFC1996]  Vixie, P., "A Mechanism for Prompt Notification of Zone
              Changes (DNS NOTIFY)", RFC 1996, DOI 10.17487/RFC1996,
              August 1996, <https://www.rfc-editor.org/info/rfc1996>.

   [RFC2826]  Internet Architecture Board, "IAB Technical Comment on the
              Unique DNS Root", RFC 2826, DOI 10.17487/RFC2826, May
              2000, <https://www.rfc-editor.org/info/rfc2826>.

   [RFC2845]  Vixie, P., Gudmundsson, O., Eastlake 3rd, D., and B.
              Wellington, "Secret Key Transaction Authentication for DNS
              (TSIG)", RFC 2845, DOI 10.17487/RFC2845, May 2000,
              <https://www.rfc-editor.org/info/rfc2845>.

   [RFC5011]  StJohns, M., "Automated Updates of DNS Security (DNSSEC)
              Trust Anchors", STD 74, RFC 5011, DOI 10.17487/RFC5011,
              September 2007, <https://www.rfc-editor.org/info/rfc5011>.

   [RFC5890]  Klensin, J., "Internationalized Domain Names for
              Applications (IDNA): Definitions and Document Framework",
              RFC 5890, DOI 10.17487/RFC5890, August 2010,
              <https://www.rfc-editor.org/info/rfc5890>.

   [RFC5936]  Lewis, E. and A. Hoenes, Ed., "DNS Zone Transfer Protocol
              (AXFR)", RFC 5936, DOI 10.17487/RFC5936, June 2010,
              <https://www.rfc-editor.org/info/rfc5936>.









Song, et al.              Expires May 18, 2018                 [Page 26]

Internet-Draft              Yeti DNS Testbed               November 2017


   [RFC6219]  Li, X., Bao, C., Chen, M., Zhang, H., and J. Wu, "The
              China Education and Research Network (CERNET) IVI
              Translation Design and Deployment for the IPv4/IPv6
              Coexistence and Transition", RFC 6219,
              DOI 10.17487/RFC6219, May 2011, <https://www.rfc-
              editor.org/info/rfc6219>.

   [RFC6891]  Damas, J., Graff, M., and P. Vixie, "Extension Mechanisms
              for DNS (EDNS(0))", STD 75, RFC 6891,
              DOI 10.17487/RFC6891, April 2013, <https://www.rfc-
              editor.org/info/rfc6891>.

   [RFC7720]  Blanchet, M. and L-J. Liman, "DNS Root Name Service
              Protocol and Deployment Requirements", BCP 40, RFC 7720,
              DOI 10.17487/RFC7720, December 2015, <https://www.rfc-
              editor.org/info/rfc7720>.

   [RFC7872]  Gont, F., Linkova, J., Chown, T., and W. Liu,
              "Observations on the Dropping of Packets with IPv6
              Extension Headers in the Real World", RFC 7872,
              DOI 10.17487/RFC7872, June 2016, <https://www.rfc-
              editor.org/info/rfc7872>.

   [RFC8109]  Koch, P., Larson, M., and P. Hoffman, "Initializing a DNS
              Resolver with Priming Queries", BCP 209, RFC 8109,
              DOI 10.17487/RFC8109, March 2017, <https://www.rfc-
              editor.org/info/rfc8109>.

   [RRL]      Vixie, P. and V. Schryver, "Response Rate Limiting (RRL)",
              June 2012, <http://www.redbarn.org/dns/ratelimits>.

   [RSSAC001]
              "Service Expectations of Root Servers", December 2015,
              <https://www.icann.org/en/system/files/files/rssac-001-
              root-service-expectations-04dec15-en.pdf>.

   [RSSAC023]
              "History of the Root Server System", November 2016,
              <https://www.icann.org/en/system/files/files/rssac-
              023-04nov16-en.pdf>.

   [TNO2009]  Gijsen, B., Jamakovic, A., and F. Roijers, "Root Scaling
              Study: Description of the DNS Root Scaling Model",
              September 2009,
              <https://www.icann.org/en/system/files/files/root-scaling-
              model-description-29sep09-en.pdf>.





Song, et al.              Expires May 18, 2018                 [Page 27]

Internet-Draft              Yeti DNS Testbed               November 2017


   [Wessels2015]
              Wessels, D., "Thirteen Years of "Old J-Root"", 2015,
              <https://indico.dns-
              oarc.net/event/24/session/10/contribution/10/material/
              slides/0.pdf>.

Appendix A.  Yeti-Root Hints File

   The following hints file (complete and accurate at the time of
   writing) causes a DNS resolver to use the Yeti DNS testbed in place
   of the production Root Server System and hence participate in
   experiments running on the testbed.

   Note that some lines have been wrapped in the text that follows in
   order to fit within the production constraints of this document.
   Wrapped lines are indicated with a blackslash character ("\"),
   following common convention.


   .                     3600000  IN   NS     bii.dns-lab.net
   bii.dns-lab.net       3600000  IN   AAAA   240c:f:1:22::6
   .                     3600000  IN   NS     yeti-ns.tisf.net
   yeti-ns.tisf.net      3600000  IN   AAAA   2001:559:8000::6
   .                     3600000  IN   NS     yeti-ns.wide.ad.jp
   yeti-ns.wide.ad.jp    3600000  IN   AAAA   2001:200:1d9::35
   .                     3600000  IN   NS     yeti-ns.as59715.net
   yeti-ns.as59715.net   3600000  IN   AAAA   \
                              2a02:cdc5:9715:0:185:5:203:53
   .                     3600000  IN   NS     dahu1.yeti.eu.org
   dahu1.yeti.eu.org     3600000  IN   AAAA   \
                              2001:4b98:dc2:45:216:3eff:fe4b:8c5b
   .                     3600000  IN   NS     ns-yeti.bondis.org
   ns-yeti.bondis.org    3600000  IN   AAAA   2a02:2810:0:405::250
   .                     3600000  IN   NS     yeti-ns.ix.ru
   yeti-ns.ix.ru         3600000  IN   AAAA   2001:6d0:6d06::53
   .                     3600000  IN   NS     yeti.bofh.priv.at
   yeti.bofh.priv.at     3600000  IN   AAAA   2a01:4f8:161:6106:1::10
   .                     3600000  IN   NS     yeti.ipv6.ernet.in
   yeti.ipv6.ernet.in    3600000  IN   AAAA   2001:e30:1c1e:1::333
   .                     3600000  IN   NS     yeti-dns01.dnsworkshop.org
   yeti-dns01.dnsworkshop.org \
                         3600000  IN   AAAA   2001:1608:10:167:32e::53
   .                     3600000  IN   NS     yeti-ns.conit.co
   yeti-ns.conit.co      3600000  IN   AAAA   \
                             2604:6600:2000:11::4854:a010
   .                     3600000  IN   NS     dahu2.yeti.eu.org
   dahu2.yeti.eu.org     3600000  IN   AAAA   2001:67c:217c:6::2
   .                     3600000  IN   NS     yeti.aquaray.com



Song, et al.              Expires May 18, 2018                 [Page 28]

Internet-Draft              Yeti DNS Testbed               November 2017


   yeti.aquaray.com      3600000  IN   AAAA   2a02:ec0:200::1
   .                     3600000  IN   NS     yeti-ns.switch.ch
   yeti-ns.switch.ch     3600000  IN   AAAA   2001:620:0:ff::29
   .                     3600000  IN   NS     yeti-ns.lab.nic.cl
   yeti-ns.lab.nic.cl    3600000  IN   AAAA   2001:1398:1:21::8001
   .                     3600000  IN   NS     yeti-ns1.dns-lab.net
   yeti-ns1.dns-lab.net  3600000  IN   AAAA   2001:da8:a3:a027::6
   .                     3600000  IN   NS     yeti-ns2.dns-lab.net
   yeti-ns2.dns-lab.net  3600000  IN   AAAA   2001:da8:268:4200::6
   .                     3600000  IN   NS     yeti-ns3.dns-lab.net
   yeti-ns3.dns-lab.net  3600000  IN   AAAA   2400:a980:30ff::6
   .                     3600000  IN   NS     \
                           ca978112ca1bbdcafac231b39a23dc.yeti-dns.net
   ca978112ca1bbdcafac231b39a23dc.yeti-dns.net \
                         3600000  IN   AAAA   2c0f:f530::6
   .                     3600000  IN   NS     \
                           3e23e8160039594a33894f6564e1b1.yeti-dns.net
   3e23e8160039594a33894f6564e1b1.yeti-dns.net \
                         3600000  IN   AAAA   2803:80:1004:63::1
   .                     3600000  IN   NS     \
                           3f79bb7b435b05321651daefd374cd.yeti-dns.net
   3f79bb7b435b05321651daefd374cd.yeti-dns.net \
                         3600000  IN   AAAA   2401:c900:1401:3b:c::6
   .                     3600000  IN   NS     \
                           xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c
   xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c \
                         3600000  IN   AAAA   2001:e30:1c1e:10::333
   .                     3600000  IN   NS     yeti1.ipv6.ernet.in
   yeti1.ipv6.ernet.in   3600000  IN   AAAA   2001:e30:187d::333
   .                     3600000  IN   NS     yeti-dns02.dnsworkshop.org
   yeti-dns02.dnsworkshop.org \
                         3600000  IN   AAAA   2001:19f0:0:1133::53
   .                     3600000  IN   NS     yeti.mind-dns.nl
   yeti.mind-dns.nl      3600000  IN   AAAA   2a02:990:100:b01::53:0


Appendix B.  Yeti-Root Server Priming Response

   Here is the reply of a Yeti root name server to a priming request.
   The authoritative server runs NSD.


   ...
   ;; Got answer:
   ;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 62391
   ;; flags: qr aa rd; QUERY: 1, ANSWER: 26, AUTHORITY: 0, ADDITIONAL: 7
   ;; WARNING: recursion requested but not available




Song, et al.              Expires May 18, 2018                 [Page 29]

Internet-Draft              Yeti DNS Testbed               November 2017


   ;; OPT PSEUDOSECTION:
   ; EDNS: version: 0, flags: do; udp: 1460
   ;; QUESTION SECTION:
   ;.                      IN NS

   ;; ANSWER SECTION:
   .            86400 IN NS bii.dns-lab.net.
   .            86400 IN NS yeti.bofh.priv.at.
   .            86400 IN NS yeti.ipv6.ernet.in.
   .            86400 IN NS yeti.aquaray.com.
   .            86400 IN NS yeti.jhcloos.net.
   .            86400 IN NS yeti.mind-dns.nl.
   .            86400 IN NS dahu1.yeti.eu.org.
   .            86400 IN NS dahu2.yeti.eu.org.
   .            86400 IN NS yeti1.ipv6.ernet.in.
   .            86400 IN NS ns-yeti.bondis.org.
   .            86400 IN NS yeti-ns.ix.ru.
   .            86400 IN NS yeti-ns.lab.nic.cl.
   .            86400 IN NS yeti-ns.tisf.net.
   .            86400 IN NS yeti-ns.wide.ad.jp.
   .            86400 IN NS yeti-ns.datev.net.
   .            86400 IN NS yeti-ns.switch.ch.
   .            86400 IN NS yeti-ns.as59715.net.
   .            86400 IN NS yeti-ns1.dns-lab.net.
   .            86400 IN NS yeti-ns2.dns-lab.net.
   .            86400 IN NS yeti-ns3.dns-lab.net.
   .            86400 IN NS xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c.
   .            86400 IN NS yeti-dns01.dnsworkshop.org.
   .            86400 IN NS yeti-dns02.dnsworkshop.org.
   .            86400 IN NS 3f79bb7b435b05321651daefd374cd.yeti-dns.net.
   .            86400 IN NS ca978112ca1bbdcafac231b39a23dc.yeti-dns.net.
   .            86400 IN RRSIG NS 8 0 86400 (
                            20171121050105 20171114050105 26253 .
                            FUvezvZgKtlLzQx2WKyg+D6dw/pITcbuZhzStZfg+LNa
                            DjLJ9oGIBTU1BuqTujKHdxQn0DcdFh9QE68EPs+93bZr
                            VlplkmObj8f0B7zTQgGWBkI/K4Tn6bZ1I7QJ0Zwnk1mS
                            BmEPkWmvo0kkaTQbcID+tMTodL6wPAgW1AdwQUInfy21
                            p+31GGm3+SU6SJsgeHOzPUQW+dUVWmdj6uvWCnUkzW9p
                            +5en4+85jBfEOf+qiyvaQwUUe98xZ1TOiSwYvk5s/qiv
                            AMjG6nY+xndwJUwhcJAXBVmGgrtbiR8GiGZfGqt748VX
                            4esLNtD8vdypucffem6n0T0eV1c+7j/eIA== )

   ;; ADDITIONAL SECTION:
   bii.dns-lab.net.        86400 IN AAAA 240c:f:1:22::6
   yeti.bofh.priv.at.      86400 IN AAAA 2a01:4f8:161:6106:1::10
   yeti.ipv6.ernet.in.     86400 IN AAAA 2001:e30:1c1e:1::333
   yeti.aquaray.com.       86400 IN AAAA 2a02:ec0:200::1
   yeti.jhcloos.net.       86400 IN AAAA 2001:19f0:5401:1c3::53



Song, et al.              Expires May 18, 2018                 [Page 30]

Internet-Draft              Yeti DNS Testbed               November 2017


   yeti.mind-dns.nl.       86400 IN AAAA 2a02:990:100:b01::53:0

   ;; Query time: 163 msec
   ;; SERVER: 2001:4b98:dc2:45:216:3eff:fe4b:8c5b#53
   ;; WHEN: Tue Nov 14 16:45:37 +08 2017
   ;; MSG SIZE  rcvd: 1222


Appendix C.  Tools developed for Yeti DNS testbed

   As part of Yeti DNS testbed, there are some tools developed
   especially to support the live testbed, including YmmV, IPv6
   PcapParser, DNS fragment and DNS ATR.  There are some brief
   introduction and GitHub links via which the audiences who are
   interested can follow our work.

   YmmV stands for Yeti Many Mirror Verifier.  It is designed to make it
   easy and safe for a DNS administrator to copy their resolver IANA
   root DNS traffic to the Yeti root.  For each query and answer that
   the resolver has made to the IANA root servers, a version will be
   sent to the some Yeti root servers.The Yeti answer that is returned
   will be compared to the IANA answer, and if there is a difference
   this will be logged.  It is a open source in Github
   (https://github.com/BII-Lab/ymmv)

   PcapParser is a IPv6 defrager which is a module used by YmmV.  As we
   described in section xxx, we need a DNS capture tool to handle large
   response.  IPv6 defrager is a program aiming at defragment ip
   fragments and resembling tcp of DNS packages in IPv6 environment.
   Now this program only reads PCAP file as input and output a PCAP
   file.  (https://github.com/RunxiaWan/PcapParser)

   DNS layer fragmentation is a code which implements proxies for DNS
   application-level fragmentation, based on a IETF draft DNS fragments
   [I-D.muks-dns-message-fragments].  The idea with these proxies is to
   explore splitting DNS messages in the protocol itself, so they will
   not by fragmented by the IP layer. (https://github.com/BII-Lab/DNS-
   layer-Fragmentation)

   In addition there is a test script of DNS ATR
   [I-D.song-atr-large-resp].  It works as a proxy demo of ATR which
   sits between resolver and authoritative servers.  It forwards the
   queries and responses as a silent and transparent listener.  If the
   response is surpass a certain size(1200 octets by default), it will
   reply additional truncated response just after the normal large
   response.  (https://github.com/songlinjian/DNS_ATR)





Song, et al.              Expires May 18, 2018                 [Page 31]

Internet-Draft              Yeti DNS Testbed               November 2017


Appendix D.  Controversy

   The Yeti DNS Project, its infrastructure and the various experiments
   that have been carried out using that infrastructure, have been
   described by people involved in the project in many public meetings
   at technical venues since its inception.  The mailing lists using
   which the operation of the infrastructure has been coordinated are
   open to join, and their archives are public.  The project as a whole
   has been the subject of robust public discussion.

   Some commentators have expressed concern that the Yeti DNS Project
   is, in effect, operating an "alternate root," challenging the IAB's
   comments published in [RFC2826].  Other such alternate roots are
   considered to have caused end-user confusion and instability in the
   namespace of the DNS by the introduction of new top-level labels or
   the different use of top-level labels present in the Root Server
   System.  The coordinators of the Yeti DNS Project do not consider the
   Yeti DNS Project to be an alternate root in this sense, since by
   design the namespace enabled by the Yeti-Root Zone is identical to
   that of the Root Zone.

   Some commentators have expressed concern that the Yeti DNS Project
   seeks to influence or subvert administrative policy relating to the
   Root Server System, in particular in the use of DNSSEC trust anchors
   not published by the IANA and the use of Yeti-Root Servers in regions
   where governments or other organisations have expressed interest in
   operating a Root Server.  The coordinators of the Yeti-Root project
   observe that their mandate is entirely technical and has no ambition
   to influence policy directly; they do hope, however, that technical
   findings from the Yeti DNS Project might act as a useful resource for
   the wider technical community.

   Finally, some concern has been expressed about the possible
   applications of the Yeti DNS Project to the governments of countries
   where access to the Internet is subject to substantial centralised
   control, in contrast to most other jurisdictions where such controls
   are either lighter or not present.  The coordinators of the Yeti DNS
   Project have taken care to steer all discussions and related
   decisions about the technical work of the project to public venues in
   the interests of full transparency, and encourage anybody concerned
   about the decision-making process to participate in those venues and
   review their archives directly.

Appendix E.  About This Document

   This section (and sub-sections) has been included as an aid to
   reviewers of this document, and should be removed prior to
   publication.



Song, et al.              Expires May 18, 2018                 [Page 32]

Internet-Draft              Yeti DNS Testbed               November 2017


E.1.  Venue

   The authors propose that this document proceed as an Independent
   Submission, since it documents work that, although relevant to the
   IETF, has been carried out externally to any IETF working group.
   However, a suitable venue for discussion of this document is the
   dnsop working group.

   Information about the Yeti DNS project and discussion relating to
   particular experiments described in this document can be found at
   <https://yeti-dns.org/>.

   This document is maintained in GitHub at <https://github.com/BII-Lab/
   yeti-testbed-experience>.

E.2.  Revision History

E.2.1.  draft-song-yeti-testbed-experience-00 through -03

   Change history is available in the public GitHub repository where
   this document is maintained: <https://github.com/BII-Lab/yeti-
   testbed-experience>.

E.2.2.  draft-song-yeti-testbed-experience-04

   Substantial editorial review and rearrangement of text by Joe Abley
   at request of BII.

   Added what is intended to be a balanced assessment of the controversy
   that has arisen around the Yeti DNS Project, at the request of the
   Independent Submissions Editorial Board.

   Changed the focus of the document from the description of individual
   experiments on a Root-like testbed to the construction and
   motivations of the testbed itself, since that better describes the
   output of the Yeti DNS Project to date.  In the considered opinion of
   this reviewer, the novel approaches taken in the construction of the
   testbed infrastructure and the technical challenges met in doing so
   are useful to record, and the RFC series is a reasonable place to
   record operational experiences related to core Internet
   infrastructure.

   Note that due to draft cut-off deadlines some of the technical
   details described in this revision of the document may not exactly
   match operational reality; however, this revision provides an
   indicative level of detail, focus and flow which it is hoped will be
   helpful to reviewers.




Song, et al.              Expires May 18, 2018                 [Page 33]

Internet-Draft              Yeti DNS Testbed               November 2017


Authors' Addresses

   Linjian Song (editor)
   Beijing Internet Institute
   2508 Room, 25th Floor, Tower A, Time Fortune
   Beijing  100028
   P. R. China

   Email: songlinjian@gmail.com
   URI:   http://www.biigroup.com/


   Dong Liu (editor)
   Beijing Internet Institute
   2508 Room, 25th Floor, Tower A, Time Fortune
   Beijing  100028
   P. R. China

   Email: dliu@biigroup.com
   URI:   http://www.biigroup.com/


   Paul Vixie (editor)
   TISF
   11400 La Honda Road
   Woodside, California  94062
   US

   Email: vixie@tisf.net
   URI:   http://www.redbarn.org/


   Akira Kato (editor)
   Keio University/WIDE Project
   Graduate School of Media Design, 4-1-1 Hiyoshi, Kohoku
   Yokohama  223-8526
   JAPAN

   Email: kato@wide.ad.jp
   URI:   http://www.kmd.keio.ac.jp/


   Shane Kerr
   Antoon Coolenlaan 41
   Uithoorn  1422 GN
   NL

   Email: shane@time-travellers.org



Song, et al.              Expires May 18, 2018                 [Page 34]
